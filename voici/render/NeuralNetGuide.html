<!DOCTYPE html>
<html lang="en">
<head><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>NeuralNetGuide</title>
  <!-- voila log js -->
  <script>
    const _debug = console.debug;
    const _info = console.info;
    const _warn = console.warn;
    const _error = console.error;

    function post(payload) {
      try {
        window.top.postMessage(payload);
      } catch(err) {
        window.top.postMessage({ level: "debug", msg: ["[Voilà]:",
                                                       "Issue cloning object when posting log message, JSON stringify version is:",
                                                       JSON.stringify(payload)
                                                       ] });
      }
    }
    console.debug = (...args) => {
        post({ level: "debug", msg: ["[Voilà]:", ...args] });
        _debug(...args);
    };

    console.info = console.info = (...args) => {
        post({ level: "info", msg: ["[Voilà]:", ...args] });
        _info(...args);
    };

    console.warn = (...args) => {
        post({ level: "warn", msg: ["[Voilà]:", ...args] });
        _warn(...args);
    };

    console.error = (...args) => {
        post({ level: "error", msg: ["[Voilà]:", ...args] });
        _error(...args);
    };
  </script>






<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>




  
  <!-- voila spinner -->
  <style>
    #loading {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        height: 75vh;
        color: var(--jp-content-font-color1);
        font-family: sans-serif;
        z-index: 100;
        position: relative;
    }

    .spinner {
      animation: rotation 2s infinite linear;
      transform-origin: 50% 50%;
    }

    .spinner-container {
      width: 10%;
    }

    @keyframes rotation {
      from {transform: rotate(0deg);}
      to   {transform: rotate(359deg);}
    }

    .voila-spinner-color1{
      fill:#268380;
    }

    .voila-spinner-color2{
      fill:#f8e14b;
    }

    .voila-skeleton-container {
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
    }

    .voila-skeleton-post {
      width: 220px;
      height: 80px;
    }
    .voila-skeleton-post .voila-skeleton-avatar {
      float: left;
      width: 52px;
      height: 52px;
      background-color: var(--jp-layout-color1);
      border-radius: 25%;
      margin: 8px;
      background-image: linear-gradient(90deg, var(--jp-layout-color2) 0px, var(--jp-layout-color3) 40px, var(--jp-layout-color2) 80px);
      background-size: 600px;
      animation: shine-avatar 1.6s infinite linear;
    }
    .voila-skeleton-post .voila-skeleton-line {
      float: left;
      width: 140px;
      height: 16px;
      margin-top: 12px;
      border-radius: 7px;
      background-image: linear-gradient(90deg, var(--jp-layout-color2) 0px, var(--jp-layout-color3) 40px, var(--jp-layout-color2) 80px);
      background-size: 600px;
      animation: shine-lines 1.6s infinite linear;
    }
    .voila-skeleton-post .voila-skeleton-avatar + .voila-skeleton-line {
      margin-top: 11px;
      width: 100px;
    }
    .voila-skeleton-post .voila-skeleton-line ~ .voila-skeleton-line {
      background-color: var(--jp-layout-color2);
    }

    @keyframes shine-lines {
      0% {
        background-position: -100px;
      }
      40%, 100% {
        background-position: 140px;
      }
    }
    @keyframes shine-avatar {
      0% {
        background-position: -32px;
      }
      40%, 100% {
        background-position: 208px;
      }
    }
  </style>


  <style>
    /*Hide empty cells*/
    .jp-mod-noOutputs.jp-mod-noInput {
      display: none;
    }
  </style>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@^5/css/all.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@^5/css/v4-shims.min.css" type="text/css" />

  
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KIDxnIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzQxNDE0MSI+CiAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiA8L2c+CiA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSguNDMgLjA0MDEpIiBmaWxsPSIjZmZmIj4KICA8cGF0aCBkPSJtNC4xNCA4Ljc2cTAuMDY4Mi0xLjg5IDIuNDItMS44OSAxLjE2IDAgMS42OCAwLjQyIDAuNTY3IDAuNDEgMC41NjcgMS4xNnYzLjQ3cTAgMC40NjIgMC41MTQgMC40NjIgMC4xMDMgMCAwLjItMC4wMjMxdjAuNzE0cS0wLjM5OSAwLjEwMy0wLjY1MSAwLjEwMy0wLjQ1MiAwLTAuNjkzLTAuMjItMC4yMzEtMC4yLTAuMjg0LTAuNjYyLTAuOTU2IDAuODcyLTIgMC44NzItMC45MDMgMC0xLjQ3LTAuNDcyLTAuNTI1LTAuNDcyLTAuNTI1LTEuMjYgMC0wLjI2MiAwLjA0NTItMC40NzIgMC4wNTY3LTAuMjIgMC4xMTYtMC4zNzggMC4wNjgyLTAuMTY4IDAuMjMxLTAuMzA0IDAuMTU4LTAuMTQ3IDAuMjYyLTAuMjQyIDAuMTE2LTAuMDkxNCAwLjM2OC0wLjE2OCAwLjI2Mi0wLjA5MTQgMC4zOTktMC4xMjYgMC4xMzYtMC4wNDUyIDAuNDcyLTAuMTAzIDAuMzM2LTAuMDU3OCAwLjUwNC0wLjA3OTggMC4xNTgtMC4wMjMxIDAuNTY3LTAuMDc5OCAwLjU1Ni0wLjA2ODIgMC43NzctMC4yMjEgMC4yMi0wLjE1MiAwLjIyLTAuNDQxdi0wLjI1MnEwLTAuNDMtMC4zNTctMC42NjItMC4zMzYtMC4yMzEtMC45NzYtMC4yMzEtMC42NjIgMC0wLjk5OCAwLjI2Mi0wLjMzNiAwLjI1Mi0wLjM5OSAwLjc5OHptMS44OSAzLjY4cTAuNzg4IDAgMS4yNi0wLjQxIDAuNTA0LTAuNDIgMC41MDQtMC45MDN2LTEuMDVxLTAuMjg0IDAuMTM2LTAuODYxIDAuMjMxLTAuNTY3IDAuMDkxNC0wLjk4NyAwLjE1OC0wLjQyIDAuMDY4Mi0wLjc2NiAwLjMyNi0wLjMzNiAwLjI1Mi0wLjMzNiAwLjcwNHQwLjMwNCAwLjcwNCAwLjg2MSAwLjI1MnoiIHN0cm9rZS13aWR0aD0iMS4wNSIvPgogIDxwYXRoIGQ9Im0xMCA0LjU2aDAuOTQ1djMuMTVxMC42NTEtMC45NzYgMS44OS0wLjk3NiAxLjE2IDAgMS44OSAwLjg0IDAuNjgyIDAuODQgMC42ODIgMi4zMSAwIDEuNDctMC43MDQgMi40Mi0wLjcwNCAwLjg4Mi0xLjg5IDAuODgyLTEuMjYgMC0xLjg5LTEuMDJ2MC43NjZoLTAuODV6bTIuNjIgMy4wNHEtMC43NDYgMC0xLjE2IDAuNjQtMC40NTIgMC42My0wLjQ1MiAxLjY4IDAgMS4wNSAwLjQ1MiAxLjY4dDEuMTYgMC42M3EwLjc3NyAwIDEuMjYtMC42MyAwLjQ5NC0wLjY0IDAuNDk0LTEuNjggMC0xLjA1LTAuNDcyLTEuNjgtMC40NjItMC42NC0xLjI2LTAuNjR6IiBzdHJva2Utd2lkdGg9IjEuMDUiLz4KICA8cGF0aCBkPSJtMi43MyAxNS44IDEzLjYgMC4wMDgxYzAuMDA2OSAwIDAtMi42IDAtMi42IDAtMC4wMDc4LTEuMTUgMC0xLjE1IDAtMC4wMDY5IDAtMC4wMDgzIDEuNS0wLjAwODMgMS41LTJlLTMgLTAuMDAxNC0xMS4zLTAuMDAxNC0xMS4zLTAuMDAxNGwtMC4wMDU5Mi0xLjVjMC0wLjAwNzgtMS4xNyAwLjAwMTMtMS4xNyAwLjAwMTN6IiBzdHJva2Utd2lkdGg9Ii45NzUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>

    <style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>

<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>

<script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      maxEdges: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        let { svg } = await mermaid.render(id, raw, el);
        svg = cleanMermaidSvg(svg);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }


    /**
     * Post-process to ensure mermaid diagrams contain only valid SVG and XHTML.
     */
    function cleanMermaidSvg(svg) {
      return svg.replace(RE_VOID_ELEMENT, replaceVoidElement);
    }


    /**
     * A regular expression for all void elements, which may include attributes and
     * a slash.
     *
     * @see https://developer.mozilla.org/en-US/docs/Glossary/Void_element
     *
     * Of these, only `<br>` is generated by Mermaid in place of `\n`,
     * but _any_ "malformed" tag will break the SVG rendering entirely.
     */
    const RE_VOID_ELEMENT =
      /<\s*(area|base|br|col|embed|hr|img|input|link|meta|param|source|track|wbr)\s*([^>]*?)\s*>/gi;

    /**
     * Ensure a void element is closed with a slash, preserving any attributes.
     */
    function replaceVoidElement(match, tag, rest) {
      rest = rest.trim();
      if (!rest.endsWith('/')) {
        rest = `${rest} /`;
      }
      return `<${tag} ${rest}>`;
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head><link rel="icon" href="data:;base64,=">

<body class="jp-Notebook theme-light" data-base-url="../../voila/" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light" data-voila="voila">


  <div id="loading" style="display: flex;">
    <div class="spinner-container">
      <svg class="spinner" data-name="c1" version="1.1" viewBox="0 0 500 500" xmlns="http://www.w3.org/2000/svg" xmlns:cc="http://creativecommons.org/ns#" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"><metadata><rdf:RDF><cc:Work rdf:about=""><dc:format>image/svg+xml</dc:format><dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage"/><dc:title>voila</dc:title></cc:Work></rdf:RDF></metadata><title>spin</title><path class="voila-spinner-color1" d="m250 405c-85.47 0-155-69.53-155-155s69.53-155 155-155 155 69.53 155 155-69.53 155-155 155zm0-275.5a120.5 120.5 0 1 0 120.5 120.5 120.6 120.6 0 0 0-120.5-120.5z"/><path class="voila-spinner-color2" d="m250 405c-85.47 0-155-69.53-155-155a17.26 17.26 0 1 1 34.51 0 120.6 120.6 0 0 0 120.5 120.5 17.26 17.26 0 1 1 0 34.51z"/></svg>
    </div>
    <h2 id="loading_text">Running ...</h2>
  </div>

<script>
  window.voila_heartbeat = function() {
    console.log('Ok, voila is still executing...');
  }
  window.update_loading_text = function(cell_index, cell_count, text) {
    const spinner = document.getElementById("loading")
    if(spinner && spinner.style.display === "none"){
      spinner.style.display="flex";
    }
    var el = document.getElementById("loading_text");
    let defaultText = `Executing ${cell_index} of ${cell_count}`
    if("False" === "True"){
      defaultText = `Reading ${cell_index} of ${cell_count}`
    }
    innterText = text ?? defaultText
    if(el){
      el.innerHTML = innterText;
    }
  }
  window.display_cells = function() {
    // TODO Apply the same logic to Voici
    if(!window.themeLoaded){
      window.cellLoaded = true;
      return;
    }
    // remove the loading element
    var el = document.getElementById("loading");
    if(el){
      el.parentNode.removeChild(el);
    }
    // show the cell output
    el = document.getElementById("rendered_cells");
    if(el){
      el.style.display = '';
    }
  }

  window.voila_process = (cell_index, cell_count) => {};
  window.voila_finish = () => {};
</script>
<div id="rendered_cells" style="display: none">
  <div>

  
  
    

    <script id="jupyter-config-data" type="application/json">
      {"appName": "JupyterLite", "appUrl": "./voici", "appVersion": "0.5.1", "baseUrl": "../../", "defaultKernelName": "python", "extensionConfig": {}, "faviconUrl": "./voici/favicon.ico", "federated_extensions": [{"extension": "./extension", "liteExtension": true, "load": "static/remoteEntry.e2b9c75217019fc6ff81.js", "name": "@jupyterlite/xeus-extension"}, {"extension": "./extension", "liteExtension": false, "load": "static/remoteEntry.5cbb9d2323598fbda535.js", "name": "jupyterlab_pygments", "style": "./style"}], "fileTypes": {"css": {"extensions": [".css"], "fileFormat": "text", "mimeTypes": ["text/css"], "name": "css"}, "csv": {"extensions": [".csv"], "fileFormat": "text", "mimeTypes": ["text/csv"], "name": "csv"}, "fasta": {"extensions": [".fasta"], "fileFormat": "text", "mimeTypes": ["text/plain"], "name": "fasta"}, "geojson": {"extensions": [".geojson"], "fileFormat": "json", "mimeTypes": ["application/geo+json"], "name": "geojson"}, "gzip": {"extensions": [".tgz", ".gz", ".gzip"], "fileFormat": "base64", "mimeTypes": ["application/gzip"], "name": "gzip"}, "html": {"extensions": [".html"], "fileFormat": "text", "mimeTypes": ["text/html"], "name": "html"}, "ical": {"extensions": [".ical", ".ics", ".ifb", ".icalendar"], "fileFormat": "text", "mimeTypes": ["text/calendar"], "name": "ical"}, "ico": {"extensions": [".ico"], "fileFormat": "base64", "mimeTypes": ["image/x-icon"], "name": "ico"}, "ipynb": {"extensions": [".ipynb"], "fileFormat": "json", "mimeTypes": ["application/x-ipynb+json"], "name": "ipynb"}, "jpeg": {"extensions": [".jpeg", ".jpg"], "fileFormat": "base64", "mimeTypes": ["image/jpeg"], "name": "jpeg"}, "js": {"extensions": [".js", ".mjs"], "fileFormat": "text", "mimeTypes": ["application/javascript"], "name": "js"}, "jsmap": {"extensions": [".map"], "fileFormat": "json", "mimeTypes": ["application/json"], "name": "jsmap"}, "json": {"extensions": [".json"], "fileFormat": "json", "mimeTypes": ["application/json"], "name": "json"}, "manifest": {"extensions": [".manifest"], "fileFormat": "text", "mimeTypes": ["text/cache-manifest"], "name": "manifest"}, "md": {"extensions": [".md", ".markdown"], "fileFormat": "text", "mimeTypes": ["text/markdown"], "name": "md"}, "pdf": {"extensions": [".pdf"], "fileFormat": "base64", "mimeTypes": ["application/pdf"], "name": "pdf"}, "plain": {"extensions": [".txt"], "fileFormat": "text", "mimeTypes": ["text/plain"], "name": "plain"}, "png": {"extensions": [".png"], "fileFormat": "base64", "mimeTypes": ["image/png"], "name": "png"}, "py": {"extensions": [".py"], "fileFormat": "text", "mimeTypes": ["text/x-python", "application/x-python-code"], "name": "py"}, "svg": {"extensions": [".svg"], "fileFormat": "text", "mimeTypes": ["image/svg+xml"], "name": "svg"}, "toml": {"extensions": [".toml"], "fileFormat": "text", "mimeTypes": ["application/toml"], "name": "toml"}, "vue": {"extensions": [".vue"], "fileFormat": "text", "mimeTypes": ["text/plain"], "name": "vue"}, "wasm": {"extensions": [".wasm"], "fileFormat": "base64", "mimeTypes": ["application/wasm"], "name": "wasm"}, "wheel": {"extensions": [".whl"], "fileFormat": "base64", "mimeTypes": ["octet/stream", "application/x-wheel+zip"], "name": "wheel"}, "xml": {"extensions": [".xml"], "fileFormat": "text", "mimeTypes": ["application/xml"], "name": "xml"}, "yaml": {"extensions": [".yaml", ".yml"], "fileFormat": "text", "mimeTypes": ["application/x-yaml"], "name": "yaml"}}, "fullLabextensionsUrl": "../../extensions", "fullStaticUrl": "../build", "include_output": true, "include_output_prompt": false, "jpThemeName": "JupyterLab Light", "kernelId": "", "licensesUrl": "./lab/api/licenses", "notebookSrc": {"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from Visualizers.labellednn import NeuralNetworkPlot\nfrom IPython.display import IFrame, Image, display\nfrom Visualizers.threeDConstructor import NeuralNetworkVisualization\nfrom Visualizers.singleMinima import GradientDescentSimulator\nfrom Visualizers.multiMinima import MultiMinimaSimulator\nfrom Visualizers.OptimizerComparisons import MultiMinimaSimulatorAdaptive\nfrom Visualizers.spinningPerceptron import PerceptronPlotlyDemo\nfrom Visualizers.derivativeGraph import PlotlySurfaceWithGradient\nimport numpy as np\nimport pandas as pd\n"}, {"cell_type": "markdown", "metadata": {}, "source": "# **A Comprehensive Breakdown of Neural Networks with Reasoning Behind My Own Design Choices for my Neural Network**\n\nAuthor: Patrick Erickson\n\n---\n\n## Abstract\n---\n\nNeural Networks have been gaining a lot of traction in the world as of recent, with the rise of giant models such as Chat-GPT and other LLMs. While the complexity of these giant AI models can not be explained by neural networks alone, they make up a core part of what makes them function. Furthermore, there are thousands of other types of models that use neural networks in their architecture to help them make predictions. In this document, I will be detailing the reason and intuition behind neural networks, the math behind them, including how to forward pass, activation functions, regularization techniques, and backpropagation (if you don\u0027t know what these mean, don\u0027t worry. We\u0027ll cover it!), and more complex stuff such as optimization functions. Lastly, I will go over an in-depth analysis on my own Neural Network\u0027s architecture so you can see my own intuitions behind the construction of one, from scratch without the use of popular libraries such as TensorFlow, PyTorch, and JAX. This article does assume, however, that you have a basic understanding of classification and regression, derivatives, and the derivative chain rule. I also expect that you understand how matrix multiplication, addition, and subtraction works. For the scope of this document, we will not be delving into Convolutional Heads or Time Series. I will also not be delving into Maximum Likelihood Estimators, but you should have a basic grasp of at least some probability distributions, such as gaussian and binomial.\n\n### A Brief History of Neural Networks: The Perceptron\nThe birth of the Neural Network had much humbler beginnings than the giant models we see today. [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt), at the Cornell Aeronautical Laboratory, had constructed something called a single layer perceptron simulated on an IBM 704 in 1957. This \"perceptron\" was made to simulate an individual neuron within the human brain, which either fires or doesn\u0027t fire based on the input it\u0027s given. \n\nFor example, let\u0027s say we know the following about someone:\n  - Income\n  - Properties Owned\n\nand we we want to predict if that person is rich or not rich (we will use 0 for not rich and 1 for rich). Our perceptron will look as such:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "perceptronPlot = NeuralNetworkPlot(\n    title=\"Perceptron\",\n    nodes_per_layer=[2,1,1],\n    label_arrows=True,\n    arrow_label_overrides={(0,0,0):\"Income\",(0,1,0):\"Properties Owned\",(1,0,0):\"Are they Rich? (0/1)\"}\n)\nperceptronPlot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Then the intuition behind this is that there is some correlation between the sizes of the features and the final output of being rich or not. Fer example, the income and the number of properties owned aren\u0027t over a certain threshold, we would print 0, for being not rich. Likewise, if income and and property value aren over this threshold, we then predict 1 (rich). More specifically, the algorithm looks at all of the points it misclassifies and then performs an operation to move the hyperplane to fix that misclassification. This algorithm continues to do this until all points are satisfied. We can visualize how the perceptron works with the following simulation:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "perfectSep = PerceptronPlotlyDemo(\n    title=\"Perceptron Simulation\",\n    n_points=100,\n    max_updates=50,\n    seed=42,\n    linearlySeperable=0\n)\nperfectSep()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Limitations of the Perceptron\nAs you can see, we slowly \"fixed\" the classifying line until all the \"Rich\" points are on one side of the line while all of the \"Not Rich\" points are on the other side of the line. We can therefore \"linearly seperate\" the data to make our predictions, which is the whole basis on how perceptrons work. However, what if the data isn\u0027t linearly seperable? For example, there could be some people who are considered \"rich\" who may only have one property, but that one property is a skyscraper in Manhattan worth billions. Our perceptron doesn\u0027t have this information, and we may not necessarily know this information to be able to put it into the perceptron. This \"unexplained data\" may cause our dataset to look more like this simulation, and is more analagous to the real world:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "perfectSep = PerceptronPlotlyDemo(\n    title=\"Perceptron Simulation For non Linearly-Seperable Data\",\n    n_points=100,\n    max_updates=200,\n    seed=42,\n    linearlySeperable=1\n)\nperfectSep()"}, {"cell_type": "markdown", "metadata": {}, "source": "The perceptron will have trouble classifying it, and the algorithm will never stop (converge), since there will always be some extra \"movement\" that can be made to try and fix the errors in the loss function. This issue only gets worse for datasets where linear seperability is borderline impossible. However, we can see that the data is still approximately linearly seperable, which allows us to be able to use methods such as logistic regression and other machine learning techniques will do a pretty good job of predicting the overall overall dataset, especially in the case we just described. But what about for data that isn\u0027t even remotely seperable? Let\u0027s look at the following example:\n\n### Further Limitations of Perceptron with XOR\n\nAssume we want to predict whether a customer would buy a product based on whether or not they visit the online and physical store of a specific company to review a product. It can be said that if a customer does not visit the online or real store, then the customer will not buy the product, because they are not interested. Likewise, a customer will most likely not walk out with anything if they visit both the online and real store, because they are currently assessing their options and they will not make a hasty decision on buying the product. However, if a customer visits the online store but not the real store, the customer may be more likely to buy it because they may think it\u0027s cheap and convenient. Likewise, a customer might be more enticed to buy a product if they physically saw it at the real store, but did not think to compare online, since the enticing prospect of buying and immediately having it is really high. We can model this problem with the following table:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "XOR = np.empty((4,2), dtype=\u0027U10\u0027)\nXOR[0,0] = \"No\"\nXOR[0,1] = \"No\"\nXOR[1,0] = \"No\"\nXOR[1,1] = \"Yes\"\nXOR[2,0] = \"Yes\"\nXOR[2,1] = \"No\"\nXOR[3,0] = \"Yes\"\nXOR[3,1] = \"Yes\"\n\nlabels = np.array([\"No\",\"Yes\",\"Yes\",\"No\"])\nXOR = np.c_[XOR,labels]\ncolumns = [\"Visted Store\", \"Visted Online\", \"Bought Item\"]\ndf = pd.DataFrame(XOR,columns=columns)\ndf.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "This is an example of a classic XOR problem, and even logistic regression and other classification methods perform poorly on this kind of data without any explicit feature engineering. Our perceptron will also suffer the same fate. The following is a simulation of different datapoints based on the company predictions:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "xorProblem = PerceptronPlotlyDemo(\n    title=\"Perceptron Simulation For non Linearly-Seperable Data\",\n    n_points=100,\n    max_updates=200,\n    redName=\"Didn\u0027t Buy Item\",\n    blueName=\"Bought Item\",\n    seed=42,\n    linearlySeperable=2\n)\nxorProblem()"}, {"cell_type": "markdown", "metadata": {}, "source": "As we expected, the perceptron performs extremely poorly. This phenomenon itself led to one of the biggest AI winters ever since the conception of the perceptron, slowing AI research to a near standstill for 20 years. However, this obviously does not pertain to today, as we have figured out a clever workaround for this issue. This, along with stochastic nature of the perceptron algorithm, did not specify loss efficiently, and would therefore oscillate with no real solution. In Layman\u0027s terms, the perceptron had no \"confidence\" in its decisions, based on the distance a point was from the classifying boundary.\n\n## Introduction to Neural Networks\n---\n\n\nThe Universal Approximation Theorem ultimately states that combining multiple neurons with some nonlinear activation can approximate any function, given enough of these neurons. This groundbreaking revelation in 1989 spurred the idea of stacking and aggregating inputs together into a single \"layer\" to effectively classify non-linear decision boundaries, such as the XOR problem described earlier. The introduction of the \"confidence method\" as an output mechanism further addressed the issue of perceptrons oscillating without convergence. This new approach, which focused on minimizing misclassification, required gradient descent optimization. Around the same period, a crucial algorithm called backpropagation was developed. Backpropagation efficiently computed chained derivatives through the network\u0027s hidden layers, enabling gradient descent to effectively tune perceptron weights, thereby minimizing confidence-based misclassifications, or in other words, reducing loss. Finally, breakthroughs in differentiable functions\u2014known as activation functions\u2014replaced the non-differentiable, step-based activation previously used by perceptrons (-1 for incorrect classification, +1 for correct). This shift to differentiable activations was the final essential component that allowed neural networks to be trained via gradient descent. Together, these advancements established the foundational framework of modern Artificial Neural Networks.\n\n### How Neural Networks Learn\nNeural Networks learn by creating predictions, then using that prediction and seeing how far it is away from the actualy associated value, often called the true label. These losses are aggregated, and then used to update the weights until the model can predict everything extremely well! Therefore, we break down neural network training into 3 problems:\n - Forward pass\n - Minimizing an objective function through backpropagation\n - evaluating its effectiveness with new data\n\n### Types of activation functions\n\n Some of the activation functions mentioned are: \n   - ReLU (Rectified Linear Unit), a function x that is 0 when $x\\leq0$\n   - $\\tanh$, used to create some value in between -1 and 1\n   - sigmoid, usually used to model some probability, giving a value of 0 to 1.\n\n They can be defined as the following, with their derivatives:\n\n$$\n\\operatorname{ReLU}(x) = \\begin{cases}\nx, \u0026 \\text{if } x \\geq 0, \\\\\n0, \u0026 \\text{if } x \u003c 0.\n\\end{cases}\n$$\n$$\n\\operatorname{ReLU\u0027}(x) = \\begin{cases}\n1, \u0026 \\text{if } x \\geq 0, \\\\\n0, \u0026 \\text{if } x \u003c 0.\n\\end{cases}\n$$\n$$\n\\tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}} \n$$\n$$\n\\tanh\u0027(x) = 1 - (\\frac{e^x-e^{-x}}{e^x+e^{-x}})^2 = 1 - \\tanh^2(x)\n$$\n$$\n\\operatorname{sigmoid}(x) = \\frac{1}{1+e^{-x}} \\text{. Note that this is commonly denoted as } \\sigma(x)\n$$\n$$\n\\operatorname{sigmoid}\u0027(x) = 1 - \\frac{e^{-x}}{(1+e^{-x})^2} = (\\frac{1}{1+e^{-x}})(1-\\frac{1}{1+e^{-x}}) = \\sigma(x)(1-\\sigma(x))\n$$\n  - **Note:** For sigmoid, due to the nature of the loss function it is usually good practice to ensure that values never go past ($1*10^{-8},1-1*10^{-8}$). This is called clipping, and ensures your super confident features don\u0027t diverge to infinity.\n\nAll of these can be used to replace the perceptron loss. **However, do note that for stable convergence it is highly recommended that you only use one type of activation and it\u0027s respective derivative for your entire network. This guarantees better convergence, and I tested this. Mixing activations just suck, you can try it yourself with my own [custom-implemented neural network](https://github.com/PatrickErickson4/FullyModularNumpyArtificialNeuralNetwork).** There are also many other types of activations, but for the sake of this demonstration, we will be working with these activations.\n\n  - **Fun Fact:** Something interesting of note is that ReLU is piecewise, so you might be wondering how this is differentiable for backpropagation. Well, the secret lies in being able to see which features were activated on unactivated by relu. Since we define that if $x=0$, then the derivative will also be zero, we fix the nondifferentiability issue. Secondly, x will just be 1, which preserves information. Meanwhile the step function is either -1 or 1, with nothing with respect to x. This gives us 0 for the entire function, giving us no meaningful information for backpropagation."}, {"cell_type": "markdown", "metadata": {}, "source": "### Loss Functions\n\nThere are also loss functions that assign some sort of non-discrete confidence of each prediction, in order to continue with the theme of differentiability for backpropagation. These are found at the end of the Neural Networks, commonly referred to as the \"head\". It is common to use softmax for classification problems. Softmax assignes a probability of confidence to each of the different categories for our problem, which we can then pick the highest in order to classify our problem. For example, if we have 3 categories, those being good, neutral, or bad, the number 1 will be split across all 3 of these categories based on the confidence for each prediction. There is also Mean Squared Error (MSE) for regression, which is the exact same formula used in that of linear regression. The following loss functions have the respective formulas, derivatives, and a reported loss, generally shown to the user when training:\n\n**Classification:**\n$$softmax \\text{ for a single sample } p_i = \\frac{e^{x_i}}{\\sum_{j=1, i\\in K}^Ke^{x_j}}. $$\nIn other words, the probability of every category for 1 sample in the dataset, and $p_i$ is analagous to the softmax function.\n$$ \\text{categorical cross entropy loss }= -\\frac{1}{n}\\sum_{i=1}^n \\hat{y_i}\\log(p_i)\\text{. }$$ \nIn other words, the amount the prediction deviated from the actual for the entire dataset.\n$$softmax\u0027 \\text{ for a single sample } = (p_i-\\hat{y_i})\\text{. }$$\nIn other words, how much we need to change our prediction by to fix the error.\n\n\n**Regression:**\n$$MSE \\text{ prediction for a single sample }= \\hat{y_i}\\text{. }$$\nIn other words, the square of how much one variable is on the y axis away from our predicted best fit line.\n$$MSE \\text{ loss }= -\\frac{1}{n}\\sum_{i=1}^n (\\hat{y_i}-\\bar{y_i})^2\\text{. }$$ \nIn other words, the amount the prediction deviated from the actual for the entire dataset.\n$$MSE\u0027 \\text{ for a single sample }= \\hat{y_i}-\\bar{y_i}\\text{. }$$\nIn other words, how much we need to change our prediction by to fix the error.\n\nthe following is a key for all of the variables:\n  - $p_i$ is the softmax function\n  - $\\hat{y_i}$ is the true value that we are trying to predict for the label $i$ in our dataset.\n  - $\\bar{y_i}$ is the value we predicted with our model\n  - $x_i$ are the values we get from the last layer.\n  - $\\frac{1}{n}\\sum_{i=1}^n$ means the \"mean\" of the values."}, {"cell_type": "markdown", "metadata": {}, "source": "For calculating the loss, it is much easier to think of it for every sample. We can calculate these then average the losses across all samples to get the middle loss value, which is generally shown to the client training a neural network. These equations are all based in fundamental statistics and the log likelihood to be able to find this total versus expected loss. Like the activation functions, there are also many different kinds of loss functions, especially for regression, but for the time being we will be sticking with these, as they are the most common. While you have most likely heard of mean squared error, you may not have heard of softmax. Let me explain it briefly:\n\n#### What is softmax?\n**Softmax turns all of your categories into probabilities of being picked.** Suppose you want to predict 10 different numbers, 0 through 9, from a dataset of images like the [MNIST Handwritten Digits Dataset](http://yann.lecun.com/exdb/mnist/). Let\u0027s say we want to predict the number 3. The idea is that the number produced for our model for the category of 3 will be a lot higher than the rest of the categories. What softmax does is it takes into account the values going into that specific category and divides it by the values of all the other categories to get a probability distribution for each category. If we look at the example, we can calculate the probability of us correctly picking 3 as such:\n$$\\frac{e^{x_3}}{e^{x_0}+e^{x_1}+e^{x_2}+e^{x_3}+e^{x_4}+e^{x_5}+e^{x_6}+e^{x_7}+e^{x_8}+e^{x_9}}$$\nnotice that this is the same as softmax we described as above, but expanded for the scope of our problem:\n$$p_3 = \\frac{e^{x_3}}{\\sum_{i=0, 3\\in \\text{ the set of integers from 0-9 }}^9e^{x_i}}$$\nIn other words, $K$ represents all categories, and each x will be the corresponding specific value of the category, where the bigger number for a specific category yields a higher probability. If our model were correct, our probability distribution would look something like this."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(100)\nbase_logits = np.array([8, 8, 8, 10, 3, 7, 8, 2, 9, 8])\nnoise = np.random.uniform(-0.5, 0.5, size=base_logits.shape)\nnoisy_logits = base_logits + noise\nif np.max(np.delete(noisy_logits, 3)) \u003e= noisy_logits[3]:\n    noisy_logits[3] = np.max(np.delete(noisy_logits, 3)) + 0.1\nexp_logits = np.exp(noisy_logits)\nprobs = exp_logits / np.sum(exp_logits)\ndigits = np.arange(10)\ncolors = [\u0027red\u0027 if i == 3 else \u0027skyblue\u0027 for i in range(10)]\nfig, ax = plt.subplots(figsize=(8, 6))\nbars = ax.barh(digits, probs, color=colors)\nfor bar, prob in zip(bars, probs):\n    width = bar.get_width()\n    ax.text(width + 0.005, bar.get_y() + bar.get_height()/2, f\u0027{prob:.3f}\u0027, va=\u0027center\u0027)\n\nax.set_yticks(digits)\nax.set_yticklabels(digits)\nax.set_title(\u0027Probability distribution for 10 Categories\u0027)\nplt.figtext(0.5, 0.0, \u00273 has the highest probability, so we pick 3.\u0027, color=\u0027red\u0027, ha=\u0027center\u0027, fontsize=12)\n\nplt.tight_layout()\nplt.show()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "**Notice how if we sum up the probabilities of all of the categories, we get 1.**\n\nWe can see that the model is \"this confident\" in predicting a 3. If certain probabilities are closer, that means our model is less confident in our prediction. Using this, we can classify many different categories. Modern architectures sometimes have thousands of these categories for image recognition, in order to accomplish things like object detection.\n\nIf instead all of these equations may look complicated, I promise it is as easy as just putting the function in the correct part of the formula. The derivations are all here for reference."}, {"cell_type": "markdown", "metadata": {}, "source": "## Application of Neural Networks\n---\n\nGoing back to our [**original shopping problem**](###Further-Limitations-of-Perceptron-with-XOR),  assume the following features are going to the shop ($x_1$) in store or online or not ($x_2$). We are going to simulate the following neural network, in order to see how a perceptron with a softmax head will perform."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 1, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={}  \n)\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We can see that this is almost exactly like the perceptron, but now we have a softmax output. **For the purpose of our example, note that there is an extra node that does not have any weights tied to it: it simply performs softmax**. When you are ready, press play on the following simulation, and see how the decision boundary changes on the graph. Stop it when the graph no longer seems to change."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "IFrame(src=\"https://playground.tensorflow.org/#activation=relu\u0026batchSize=10\u0026dataset=xor\u0026regDataset=reg-plane\u0026learningRate=0.1\u0026regularizationRate=0\u0026noise=0\u0026networkShape=1\u0026seed=0.58130\u0026showTestData=false\u0026discretize=false\u0026percTrainData=50\u0026x=true\u0026y=true\u0026xTimesY=false\u0026xSquared=false\u0026ySquared=false\u0026cosX=false\u0026sinX=false\u0026cosY=false\u0026sinY=false\u0026collectStats=false\u0026problem=classification\u0026initZero=false\u0026hideText=false\",width=800,height=600)"}, {"cell_type": "markdown", "metadata": {}, "source": " Based on the tensorflow playground, it is still impossible to linearly seperate the data. Let\u0027s try a different approach.  Using the Universal Approximation Theorem, we will construct the following neural network with 4 nodes in the hidden layer:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={}  # provide any overrides as \n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Let\u0027s see what happens when we try our simulation:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "IFrame(src=\u0027https://playground.tensorflow.org/#activation=relu\u0026batchSize=10\u0026dataset=xor\u0026regDataset=reg-plane\u0026learningRate=0.1\u0026regularizationRate=0\u0026noise=0\u0026networkShape=4\u0026seed=0.58130\u0026showTestData=false\u0026discretize=false\u0026percTrainData=50\u0026x=true\u0026y=true\u0026xTimesY=false\u0026xSquared=false\u0026ySquared=false\u0026cosX=false\u0026sinX=false\u0026cosY=false\u0026sinY=false\u0026collectStats=false\u0026problem=classification\u0026initZero=false\u0026hideText=false\u0027,width=800,height=600)"}, {"cell_type": "markdown", "metadata": {}, "source": "Notice how with four different linear combinators, we can create four different \"segments\" and finally solve the issue that has plagued AI researchers during the 70\u0027s and 80\u0027s.\n\n\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### The need for layers\n\nWhile the Universal Approximation theorem states that you can approximate any function $f(x)$ given the appropriate amount of nodes, this does not necessarily mean we should make  a single layer with this sufficient amount of nodes. By breaking each number of nodes up into seperate layers, we are able to compound on the features learned by each layer to arrived at a better gerneralization. We can see this motivation with the following architecture:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 8, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={}\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We can see this represented in Tensorflow. We will try to classify a spiral dataset:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "IFrame(src=\u0027https://playground.tensorflow.org/#activation=relu\u0026regularization=L2\u0026batchSize=10\u0026dataset=spiral\u0026regDataset=reg-plane\u0026learningRate=0.1\u0026regularizationRate=0.003\u0026noise=0\u0026networkShape=8\u0026seed=0.30700\u0026showTestData=false\u0026discretize=false\u0026percTrainData=50\u0026x=true\u0026y=true\u0026xTimesY=false\u0026xSquared=false\u0026ySquared=false\u0026cosX=false\u0026sinX=false\u0026cosY=false\u0026sinY=false\u0026collectStats=false\u0026problem=classification\u0026initZero=false\u0026hideText=false\u0027,width=800,height=600)"}, {"cell_type": "markdown", "metadata": {}, "source": "By the example, you can see the model struggle to correctly seperate the feature space. However, let\u0027s define a different architecture:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 6,6, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={}\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Let\u0027s see how this performs in the playground on the same dataset."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "IFrame(src=\u0027https://playground.tensorflow.org/#activation=relu\u0026regularization=L2\u0026batchSize=10\u0026dataset=spiral\u0026regDataset=reg-plane\u0026learningRate=0.1\u0026regularizationRate=0.003\u0026noise=0\u0026networkShape=6,6\u0026seed=0.30700\u0026showTestData=false\u0026discretize=false\u0026percTrainData=50\u0026x=true\u0026y=true\u0026xTimesY=false\u0026xSquared=false\u0026ySquared=false\u0026cosX=false\u0026sinX=false\u0026cosY=false\u0026sinY=false\u0026collectStats=false\u0026problem=classification\u0026initZero=false\u0026hideText=false\u0027,width=800,height=600)"}, {"cell_type": "markdown", "metadata": {}, "source": "You can see that the first layer learns bigger, more simple features, which are then fed into the second layer, where features are broken down and refined. As a result, we get a much better generalization than the 8 node example we had shown previously. This is the core idea behind deep learning: we can approximate almost any function, given enough layers and nodes. We have since learned how to numerically represent words and pictures, and we can therefore generalize even more complex ideas such as this. LLM\u0027s like ChatGPT and image generators such as DALL-E build off of this.\n\n  - **Fun Fact:** There is a theory in statistics that states that the best models are the simplest, referred to as Ockham\u0027s Razor. Neural Networks are thought to have been unscalable because of this. However, do you see how some of these nodes you see how some the nodes aren\u0027t used at all? The idea behind sparsification: that a neural network only trains the nodes it needs to, and you can effectively prune these other nodes. The idea behind it therefore is that the more nodes you add, the more likely you are to find a solution that is really good at approximating something, because there are more routes that weights can travel through. Despite this, your model can still overfit if you have too many nodes, so keep that in mind."}, {"cell_type": "markdown", "metadata": {}, "source": "## Thinking in Matrices: The Math Behind These Networks\n---\n\nKnowing the intuition behind why these neural networks are used, we can finally delve into the math that makes them work. Based on all of the connections for the neural network, we can see that each weight, being fully connected, can form a matrix of values. Let\u0027s look at out previous example, focusing on the blue arrow highlighting the area of interest:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=1\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We can see that $x_1$ connects to all four nodes, as does $x_2$. This means that there will be 8 total arrows, which matches with our diagram. Each one of these white arrows for represent some weight to multiply our input from the previous layer by. The corresponding weight matrix can be represented as such:\n\n$$\n\\left( \\begin{array}{cccc}\nw_{1,1}^{(1)} \u0026 w_{1,2}^{(1)} \\\\[0.5em]\nw_{2,1}^{(1)} \u0026 w_{2,2}^{(1)} \\\\[0.5em]\nw_{3,1}^{(1)} \u0026 w_{3,2}^{(1)} \\\\[0.5em]\nw_{4,1}^{(1)} \u0026 w_{4,2}^{(1)} \\\\[0.5em]\n\\end{array} \\right)\n$$\n\nWhere every weight has the following properties:\n$$w_{i\\text{ , }j}^{(\\ell)} $$\n\nThink of this term as any single arrow in your neural network, where the subscripts and superscripts tell us which arrow it is. Each arrow will have some weight, which will be some number.\n  - $w$ simply means weight number (the value we multiply our previous inputs by\n  - $i$ specifies which node in the layer it is going to \n  - $j$ specifies which node from the previous layer the weight is coming from\n  - $\\ell$ specifies the layer we are trying to calculate the weights for. For example, $\\ell=1$ for this particular weight matrix.\n  - if we drop the subscript $j$, that means we are talking about a particular node in the layer\n  - if we drop both subscripts and just have a capital $W^{(\\ell)}$, this means we are talking about all arrows pointing into the LAYER, which means the **entire weight matrix**.\n  - Any Weight Matrix feeding into a layer will have the dimensions (nodes of the current layer) x (nodes/features of the previous layer). In other words, $dim(W^{(\\ell)}) = \\ell\\text{ x }(\\ell-1)$\n\nUp until now, we have also been simplifying the neural network for visualization purposes. In reality, Neural Networks have a bias for every layer that looks something like this:\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=1,\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "The reason for this bias term is to ensure that some weights do not get too big to overcompensate for a poor intercept, when weights might have learned the shape already. You can imagine this with our spiral example. Imagine our weights learned a really good boundary for classifying the weight, but the whole boundary is shifted to the right. The bias accounts for this and shifts it back to its correct place. This bias term can be represented as\n$$b^{(\\ell)}_i$$\nwhere \n  - i is the node the bias value corresponds to\n  - $\\ell$ is the layer the bias is being used to calculate.\n  - $b^{(\\ell)}$ is the bias value for that entire layer\n  - $b_i^{(\\ell)}$ is the bias value for the $i^{th}$ node of the $\\ell^{th}$ layer\nso an individual bias for the layer we are talking about for node can be represented as\n$$b_1^{(1)}$$\nwhich will correspond to some single scalar number. Likewise, the bias for the entire layer we are pointing at can be represented as:\n$$\nb^{(1)} = \n\\left( \\begin{array}{cccc}\nb_{1}^{(1)} \\\\[0.5em]\nb_{2}^{(1)} \\\\[0.5em]\nb_{3}^{(1)} \\\\[0.5em]\nb_{4}^{(1)} \\\\[0.5em]\n\\end{array} \\right)\n$$\nwhere each subscript represents the node the bias corresponds to."}, {"cell_type": "markdown", "metadata": {}, "source": "## Predicting a Number: Forward Pass\n---\n\nBy computing forward pass, we get a predicted value, given all of the features we are predicting with in our dataset. The final output will give what the neural networks \"thinks\" the value should be. \n\n\nOur forward pass can be though of in 4 steps:\n  - Step 1: Multiply the Weights by the inputs\n  - Step 2: add the biases\n  - step 3: apply an activation function of your choice (ReLU, tanh, sigmoid, etc.)\nThese will give you the activated values for each of the next nodes.\n\n### Iteration 1: The First Layer of Weights\n\nLet\u0027s use the neural network we have described from the previous layer:\n\n\n#### Step 1\nWe multiply our previous layer\u0027s input by the corresponding row gives us the next node. When we do this, we are getting some feature representation of the previous layer and using that arrows as a means to put them into the node. For example, for node one, we would have ($w_{1,1}^{(1)} \\text{  } w_{1,2}^{(1)}$), which represents all the weights (white arrows) going to a specific node. Multiplying this by the inputs $(x_1 \\text{  } x_2)$ we can compute $w_1^{(1)}x$ to get the value for node one before being activated:\n\n\n**For node 1 in layer one (the top node):**\n\n$$\n\\left( \\begin{array}{cccc}\nw_{1,1}^{(1)} \u0026 w_{1,2}^{(1)}\n\\end{array} \\right) \n\\left( \\begin{array}{cccc}\nx_1  \\\\[0.5em]\nx_2\n\\end{array} \\right)\n=\nw_{1,1}^{(1)}x_1 + w_{1,2}^{(1)}x_2 = w_1^{(1)}x\n$$\n\nWe can do the math for the entire layer by representing the multiplications in matrix form, with each subsequent node represented by its row position in the weight matrix:\n$$\n\\left( \\begin{array}{cccc}\nw_{1,1}^{(1)} \u0026 w_{1,2}^{(1)} \\\\[0.5em]\nw_{2,1}^{(1)} \u0026 w_{2,2}^{(1)} \\\\[0.5em]\nw_{3,1}^{(1)} \u0026 w_{3,2}^{(1)} \\\\[0.5em]\nw_{4,1}^{(1)} \u0026 w_{4,2}^{(1)} \\\\[0.5em]\n\\end{array} \\right) \n\\left( \\begin{array}{cccc}\nx_1  \\\\[0.5em]\nx_2\n\\end{array} \\right) = \n\\left( \\begin{array}{cccc}\nw_{1,1}^{(1)}x_1 + w_{1,2}^{(1)}x_2 \\\\[0.5em]\nw_{2,1}^{(1)}x_1 + w_{2,2}^{(1)}x_2 \\\\[0.5em]\nw_{3,1}^{(1)}x_1 + w_{3,2}^{(1)}x_2 \\\\[0.5em]\nw_{4,1}^{(1)}x_1 + w_{4,2}^{(1)}x_2 \\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\nw_1^{(1)}x \\\\[0.5em]\nw_2^{(1)}x \\\\[0.5em]\nw_3^{(1)}x \\\\[0.5em]\nw_4^{(1)}x \\\\[0.5em]\n\\end{array} \\right) \n$$\n\n#### Step 2\nLet\u0027s ADD bias 1 to node 1 as such, for example:\n\n$$w_1^{(1)}x + b_1^{(1)}$$\n\n**It is important that you do not multiply the bias. Only add. This is a common mistake many people make.** As you can see, adding bias correlates to the **red arrows** coming from the diagram\u0027s input bias into the next layer. We can represent adding bias to all nodes in matrix notation: \n\n$$\n\\left( \\begin{array}{cccc}\nw_1^{(1)}x \\\\[0.5em]\nw_2^{(1)}x \\\\[0.5em]\nw_3^{(1)}x \\\\[0.5em]\nw_4^{(1)}x \\\\[0.5em]\n\\end{array} \\right) \n+ \\left( \\begin{array}{cccc}\nb_{1}^{(1)} \\\\[0.5em]\nb_{2}^{(1)} \\\\[0.5em]\nb_{3}^{(1)} \\\\[0.5em]\nb_{4}^{(1)} \\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\nw_1^{(1)}x + b_{1}^{(1)} \\\\[0.5em]\nw_2^{(1)}x + b_{2}^{(1)} \\\\[0.5em]\nw_3^{(1)}x + b_{3}^{(1)}\\\\[0.5em]\nw_4^{(1)}x + b_{4}^{(1)}\\\\[0.5em]\n\\end{array} \\right) \n$$\nWe will denote this matrix as $z^{(1)}$:\n$$\nz^{(1)} = \n\\left( \\begin{array}{cccc}\nz_{1}^{(1)} \\\\[0.5em]\nz_{2}^{(1)} \\\\[0.5em]\nz_{3}^{(1)} \\\\[0.5em]\nz_{4}^{(1)} \\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\nw_1^{(1)}x + b_{1}^{(1)} \\\\[0.5em]\nw_2^{(1)}x + b_{2}^{(1)} \\\\[0.5em]\nw_3^{(1)}x + b_{3}^{(1)}\\\\[0.5em]\nw_4^{(1)}x + b_{4}^{(1)}\\\\[0.5em]\n\\end{array} \\right) \n$$"}, {"cell_type": "markdown", "metadata": {}, "source": "We are now at this location of the graph, without activation. We need to activate it before we send our \"nodes\" to the next layer."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=1,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 3\nFinally, we can apply an activation, that we mentioned a previously. For the sake of ease, we let $f(x)$ be any non-linear [activation function](####Types-of-activation-functions). Then we have:\n$$\nf(z^{(1)}) = \nf\\left( \\begin{array}{cccc}\nz_{1}^{(1)} \\\\[0.5em]\nz_{2}^{(1)} \\\\[0.5em]\nz_{3}^{(1)} \\\\[0.5em]\nz_{4}^{(1)} \\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\n\\text{Node 1}^{(1)} \\\\[0.5em]\n\\text{Node 2}^{(1)} \\\\[0.5em]\n\\text{Node 3}^{(1)} \\\\[0.5em]\n\\text{Node 4}^{(1)} \\\\[0.5em]\n\\end{array} \\right)=\n\n\\left( \\begin{array}{cccc}\na_1^{(1)} \\\\[0.5em]\na_2^{(1)} \\\\[0.5em]\na_3^{(1)} \\\\[0.5em]\na_4^{(1)} \\\\[0.5em]\n\\end{array} \\right) = a^{(1)}\n$$\n\nwhere $a$ stands for activated.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "You would repeat this process over and over again until you reached the end of the network. Let\u0027s do the next layer, just to see what this would look like.\n### Iteration 2: The Second Layer of Weights\nIf we followed the equations correctly, you will now have the values for the first hidden layer. Let\u0027s focus on this layer, as denoted by the blue arrow:\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=2,\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 1\nWe multiply our nodes by the new weight matrices. Notice how this matrix multiplication forces the numbers to be in the same dimensions as the next node layer. A plus!\n\n$$\n\\left( \\begin{array}{cccc}\nw_{1,1}^{(2)} \u0026 w_{1,2}^{(2)} \u0026 w_{1,3}^{(2)} \u0026 w_{1,4}^{(2)}\\\\[0.5em]\nw_{2,1}^{(2)} \u0026 w_{2,2}^{(2)} \u0026 w_{2,3}^{(2)} \u0026 w_{2,4}^{(2)} \\\\[0.5em]\n\\end{array} \\right) \n\\left( \\begin{array}{cccc}\n\\text{Node 1}^{(1)} \\\\[0.5em]\n\\text{Node 2}^{(1)} \\\\[0.5em]\n\\text{Node 3}^{(1)} \\\\[0.5em]\n\\text{Node 4}^{(1)} \\\\[0.5em]\n\\end{array} \\right) = \n\\left( \\begin{array}{cccc}\nw_{1,1}^{(2)}a_1^{(1)} + w_{1,2}^{(2)}a_2^{(1)} + w_{1,3}^{(2)}a_3^{(1)} + w_{1,4}^{(2)}a_4^{(1)} \\\\[0.5em]\nw_{2,1}^{(2)}a_1^{(1)} + w_{2,2}^{(2)}a_2^{(1)} + w_{2,3}^{(2)}a_3^{(1)} + w_{2,4}^{(2)}a_4^{(1)}\\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\nw_1^{(2)}a^{(1)} \\\\[0.5em]\nw_2^{(2)}a^{(1)} \\\\[0.5em]\n\\end{array} \\right) \n$$\n**Notice the pattern here. Every row represents all of the \"arrows\" going into that node, and every \"arrow\" multiplies the previous layer\u0027s input, $a$, but some weight. When we perform the multiplication, all the the numbers correctly format to the number of nodes in the next layer!**\nAlso, notice that any node $i$ in layer $\\ell$ is the same thing as $a_i^{(\\ell)}$.\n\n#### Step 2\nWe now add our bias, as deonted by the red arrows on the diagram.\n$$\n\\left( \\begin{array}{cccc}\nw_1^{(2)}a^{(1)} \\\\[0.5em]\nw_2^{(2)}a^{(1)} \\\\[0.5em]\n\\end{array} \\right) \n+ \\left( \\begin{array}{cccc}\nb_{1}^{(2)} \\\\[0.5em]\nb_{2}^{(2)} \\\\[0.5em]\n\\end{array} \\right) = \n\\left( \\begin{array}{cccc}\nw_1^{(2)}a_1^{(1)} + b_{1}^{(2)} \\\\[0.5em]\nw_2^{(2)}a_1^{(1)} + b_{2}^{(2)} \\\\[0.5em]\n\\end{array} \\right) = \n\\left( \\begin{array}{cccc}\nz_1^{(2)} \\\\[0.5em]\nz_2^{(2)} \\\\[0.5em]\n\\end{array} \\right)  = z^{(2)}\n$$"}, {"cell_type": "markdown", "metadata": {}, "source": "This puts us at this current location of the graph:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=2,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 3\nWe can see that this is actually the last weight layer, and given that this is a classification head, we have a softmax activation. This means that\n$$f(x) = \\frac{e^{z_i}}{e^{z_1}+e^{z_2}}, \\text{ where i is whatever node we are coming from.} $$\n\nWe can apply it as such:\n$$\nf(z^{(2)}) =\nf\\left( \\begin{array}{cccc}\nz_1^{(2)} \\\\[0.5em]\nz_2^{(2)} \\\\[0.5em]\n\\end{array} \\right)  = \n\\Large\\left( \\begin{array}{cccc}\n\\frac{e^{z_1^{(2)}}}{e^{z_1^{(2)}} + e^{z_2^{(2)}}} \\\\[0.5cm]\n\\frac{e^{z_2^{(2)}}}{e^{z_1^{(2)}} + e^{z_2^{(2)}}}  \\\\[0.5em]\n\\end{array} \\right) =\n\\normalsize\\left( \\begin{array}{cccc}\na_1^{(2)} \\\\[0.5em]\na_2^{(2)}  \\\\[0.5em]\n\\end{array} \\right) = \na^{(2)} \n$$"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 2, 2],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=3,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We have now finished forward pass!"}, {"cell_type": "markdown", "metadata": {}, "source": "### Alternative Forward pass: What if we were doing a regression problem?\n\nIf we were doing regression, our neural network will look more like this, as we are predicting some numerical value."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 1, 1],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=1,\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Iteration 1: \nThis would be exactly the same as the previous problem:\n#### Step 1:\n$$\n\\left( \\begin{array}{cccc}\nw_{1,1}^{(1)} \u0026 w_{1,2}^{(1)} \\\\[0.5em]\nw_{2,1}^{(1)} \u0026 w_{2,2}^{(1)} \\\\[0.5em]\nw_{3,1}^{(1)} \u0026 w_{3,2}^{(1)} \\\\[0.5em]\nw_{4,1}^{(1)} \u0026 w_{4,2}^{(1)} \\\\[0.5em]\n\\end{array} \\right) \n\\left( \\begin{array}{cccc}\nx_1  \\\\[0.5em]\nx_2\n\\end{array} \\right) = \n\\left( \\begin{array}{cccc}\nw_{1,1}^{(1)}x_1 + w_{1,2}^{(1)}x_2 \\\\[0.5em]\nw_{2,1}^{(1)}x_1 + w_{2,2}^{(1)}x_2 \\\\[0.5em]\nw_{3,1}^{(1)}x_1 + w_{3,2}^{(1)}x_2 \\\\[0.5em]\nw_{4,1}^{(1)}x_1 + w_{4,2}^{(1)}x_2 \\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\nw_1^{(1)}x \\\\[0.5em]\nw_2^{(1)}x \\\\[0.5em]\nw_3^{(1)}x \\\\[0.5em]\nw_4^{(1)}x \\\\[0.5em]\n\\end{array} \\right) \n$$\n**Note:** $W^{(1)}$ has dimensions $4 \\text{ x } 2$ because the next layer has 4 nodes and the previous layer had 2 nodes/features.\n\n#### Step 2:\n$$\n\\left( \\begin{array}{cccc}\nw_1^{(1)}x \\\\[0.5em]\nw_2^{(1)}x \\\\[0.5em]\nw_3^{(1)}x \\\\[0.5em]\nw_4^{(1)}x \\\\[0.5em]\n\\end{array} \\right) \n+ \\left( \\begin{array}{cccc}\nb_{1}^{(1)} \\\\[0.5em]\nb_{2}^{(1)} \\\\[0.5em]\nb_{3}^{(1)} \\\\[0.5em]\nb_{4}^{(1)} \\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\nw_1^{(1)}x + b_{1}^{(1)} \\\\[0.5em]\nw_2^{(1)}x + b_{2}^{(1)} \\\\[0.5em]\nw_3^{(1)}x + b_{3}^{(1)}\\\\[0.5em]\nw_4^{(1)}x + b_{4}^{(1)}\\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\nz_{1}^{(1)} \\\\[0.5em]\nz_{2}^{(1)} \\\\[0.5em]\nz_{3}^{(1)} \\\\[0.5em]\nz_{4}^{(1)} \\\\[0.5em]\n\\end{array} \\right) \n =z^{(1)}\n$$"}, {"cell_type": "markdown", "metadata": {}, "source": "We are now in this portion of our network:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 1, 1],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=1,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 3:\nWe now activate our nodes, just like we did previously.\n$$\nf(z^{(1)}) = \nf\\left( \\begin{array}{cccc}\nz_{1}^{(1)} \\\\[0.5em]\nz_{2}^{(1)} \\\\[0.5em]\nz_{3}^{(1)} \\\\[0.5em]\nz_{4}^{(1)} \\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\n\\text{Node 1}^{(1)} \\\\[0.5em]\n\\text{Node 2}^{(1)} \\\\[0.5em]\n\\text{Node 3}^{(1)} \\\\[0.5em]\n\\text{Node 4}^{(1)} \\\\[0.5em]\n\\end{array} \\right)=\n\n\\left( \\begin{array}{cccc}\na_1^{(1)} \\\\[0.5em]\na_2^{(1)} \\\\[0.5em]\na_3^{(1)} \\\\[0.5em]\na_4^{(1)} \\\\[0.5em]\n\\end{array} \\right) = a^{(1)}\n$$"}, {"cell_type": "markdown", "metadata": {}, "source": "We are now here in our neural network:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 1, 1],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=2,\n    special_inter_layer_arrow_offset=\"layer\",\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Iteration 2:\n\nThis is where things get to be a little different. Since we are only predicting one numerical value, we only have one output node. \n\n  - **Note:** It is possible to have multiple nodes for regression. What this would mean is we are predicting multiple values at onces. For example, if we have house size, lot size, and the numnber of bathroom as our input (x1,x2,x3), we  could have 2 regression heads:\n    - One predicts house price (y1)\n    - One predicts the number of bedrooms (y2)\n  - This effectively predicting 2 values at once."}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 1:\nMultiply our weights by the nodes:\n\n$$\n\\left( \\begin{array}{cccc}\nw_{1,1}^{(2)} \u0026 w_{1,2}^{(2)} \u0026 w_{1,3}^{(2)} \u0026 w_{1,4}^{(2)}\\\\[0.5em]\n\\end{array} \\right) \n\\left( \\begin{array}{cccc}\n\\text{Node 1}^{(1)} \\\\[0.5em]\n\\text{Node 2}^{(1)} \\\\[0.5em]\n\\text{Node 3}^{(1)} \\\\[0.5em]\n\\text{Node 4}^{(1)} \\\\[0.5em]\n\\end{array} \\right) = \n\\left( \\begin{array}{cccc}\nw_{1,1}^{(2)}a_1^{(1)} + w_{1,2}^{(2)}a_2^{(1)} + w_{1,3}^{(2)}a_3^{(1)} + w_{1,4}^{(2)}a_4^{(1)} \\\\[0.5em]\n\\end{array} \\right) =\n\\left( \\begin{array}{cccc}\nw_1^{(2)}a^{(1)} \\\\[0.5em]\n\\end{array} \\right) \n$$\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Notice how we only have 1 node in the next layer, our node can be represented in a 1x1 matrix."}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 2:\nAdd our bias:\n\n$$\n\\left( \\begin{array}{cccc}\nw_1^{(2)}a^{(1)} \\\\[0.5em]\n\\end{array} \\right) \n+ \\left( \\begin{array}{cccc}\nb_{1}^{(2)} \\\\[0.5em]\n\\end{array} \\right) = \n\\left( \\begin{array}{cccc}\nw_1^{(2)}a_1^{(1)} + b_{1}^{(2)} \\\\[0.5em]\n\\end{array} \\right) = \n\\left( \\begin{array}{cccc}\nz_1^{(2)} \\\\[0.5em]\n\\end{array} \\right)  = z^{(2)}\n$$"}, {"cell_type": "markdown", "metadata": {}, "source": "We are now in the following portion of out network:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 1, 1],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=2,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 3: \n\nWe finish our forward pass using the regression activation. This means the layer is just linear, so we dont need to apply anything, we already have a numeric number! This means that the activation for this layer is just\n$$\nf(z^{(2)}) =\nf\\left( \\begin{array}{cccc}\nz^{(2)} \\\\[0.5em]\n\\end{array} \\right)  =\n\\left( \\begin{array}{cccc}\nz^{(2)} \\\\[0.5em]\n\\end{array} \\right) =\n\\normalsize\\left( \\begin{array}{cccc}\na^{(2)} \\\\[0.5em]\n\\end{array} \\right) = \na^{(2)} \n$$\nThis puts us here:\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 4, 1, 1],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=3,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n)\n\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Since we are at our output layer, this is actually just the output itself. This concludes forward pass through the entire network. If you wish to have more practice, I recommend asking ChatGPT for small neural networks to feed forward through. Then ask if you get it right.\n\n### Final Forward Pass Algorithm\nIn conclusion, each forward pass for the nodes in layer l can be can be summed up to the following Equation:\n\n$$\\Large a^{(\\ell)} = f(W^{(\\ell)}\\cdot a^{(\\ell-1)} + b^{\\ell})$$\n\nwhere\n  - $f(x)$ is the [activation function](##Introduction-to-Neural-Networks)\n  - $a^{(\\ell)}$ is a layer\u0027s output from its nodes\n  - $a^{(\\ell-1)}$ is the previous layer nodes, which I have shown to be the matrix $(\\text{Node 1}^{(1)}, \\text{Node 2}^{(1)},\\cdots)$ when we were computing the final output, for example (Iteration 2)\n  - $W^{(\\ell)}$ is the weight matrix for that layer\n  - $b^{\\ell}$ is the bias for that layer\n  - $\\cdot$ is a matrix multiplication\n  - $a^{(1)}$ is always one sample in your dataset (a single row of the dataset). It has d \"nodes\" (number of columns (features), without the classification labels)\n  - The following are equivalent: $z^{(\\ell)} = W^{(\\ell)}\\cdot a^{(\\ell-1)} + b^{\\ell}$ and $a^{(\\ell)} = f(z^{(\\ell)})$\n  - $z^{(\\ell)}$ is the node before being activated by its respective activation function\n\n\nUsing this, you should effectively be able to calculate the \"predictions\" a neural network makes, depending on the input feature given for **one sample in a dataset**."}, {"cell_type": "markdown", "metadata": {}, "source": "## Making our Models Learn: Backpropagation\n\n---\n\nWe have covered how to predict a number with our neural network. However, if our model doesn\u0027t learn, how will it ever predict anything correctly? This is why we have to slowly \"tune\" our model so that it\u0027s weights will correctly predict what we want it to. We do this by using gradients to move in the direction that reduces this loss as much as possible.\n\n### What are Gradients?\n\nIf you have every taken a calculus class before, you know what a derivative is: the change in a function at any given point f(x). The difference between derivatives and gradients lies in the fact that a gradient can be taken with respect to multiple variables, with their partial derivatives. This gradient is a **vector** of derivatives that not only tell you the magnitude of change, but also the **direction** of greatest increase. This is really useful, because most problems in machine learning are never just a single variable. Let\u0027s see an example:\n\nAssume we have the function\n$$f(x,y) = x^2+y^2$$"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "plot_obj = PlotlySurfaceWithGradient(title=\"f(x,y)=x^2+y^2\")\nplot_obj()"}, {"cell_type": "markdown", "metadata": {}, "source": "Let\u0027s say we take the gradient at point (0.2,0.2):\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "plot_obj = PlotlySurfaceWithGradient(title=\"Gradient at point (.2,.2)\")\nplot_obj((-.2,.2))"}, {"cell_type": "markdown", "metadata": {}, "source": "and the gradient at (.5,.5):"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "plot_obj = PlotlySurfaceWithGradient(title=\"Gradient at point (.5,.5)\")\nplot_obj((-.5,.5))"}, {"cell_type": "markdown", "metadata": {}, "source": "Notice how much bigger the arrow is. If we flip this around this can correspond to how \"steep\" the function is.\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "plot_obj = PlotlySurfaceWithGradient(title=\"Negative Gradient at point (.5,.5)\",flip=True)\nplot_obj((-.5,.5))"}, {"cell_type": "markdown", "metadata": {}, "source": " But how does this relate to training our model?\n \n### Minimizing Loss with Gradients: Gradient Descent\n\nWe had shown earlier the different loss fucntions we use for a neural network. If we were to review them again, we see that that we have the following loss functions:\n\n**Classification:**\n$$\\text{ categorical cross entropy loss }= -\\frac{1}{n}\\sum_{i=1}^n \\hat{y_i}\\log(p_i)\\text{. }$$\n**Regression:**\n$$MSE \\text{ loss }= -\\frac{1}{n}\\sum_{i=1}^n (\\hat{y_i}-\\bar{y_i})^2\\text{. }$$ \n  - **Reminder:** These aren\u0027t the only loss functions, just the most common.\n\nWhat this means is that the loss might be like some function like we see above, where the smallest loss is at the lowest point (minima). However, since every single sample in our dataset will generate a different function, we dont ever really know what the loss functions look like or even is for that matter. We just randomly \"spawn\" on some part of this function, and try to guess where we should go. This means we don\u0027t know what our \"true\" loss function looks like. We are blind, which means we want to take a \"step\" in the direction in which we decrease loss. Since the point with the smallest loss means we get the best prediction, the intuition is that **we can change the weights with respect to the change in loss in the negative direction to take a step towards this minima,** until we reach the minima. This is the idea behind backpropagation for gradient descent.\n\nIf we take the gradient of the loss functions, we get the following values:\n\n**Classification:**\n$$softmax\u0027 \\text{ for a single sample } = (p_i-\\hat{y_i})\\text{. }$$\n**Regression:**\n$$MSE\u0027 \\text{ for a single sample }= \\hat{y_i}-\\bar{y_i}\\text{. }$$\n\n\nthe following is a key for all of the variables:\n  - $p_i$ is the softmax function\n  - $\\hat{y_i}$ is the true value that we are trying to predict for the label $i$ in our dataset.\n  - $\\bar{y_i}$ is the value we predicted with our model\n  - $x_i$ are the values we get from the last layer.\n  - $\\frac{1}{n}\\sum_{i=1}^n$ means the \"mean\" of the values.\n\nIf you look closely, the gradients of the loss quantify how \"far\" our prediction was from the actual values. By changing all the weights in our matrix, subtracting the gradients of the weights with respect to the loss for every single weight, we do a single step of backpropagation. If we do this backpropagation step until we reach the loss minimum, we will successfully perform gradient descent, our model will eventually \"converge\" to the local minima, and be able to predict the values we want!"}, {"cell_type": "markdown", "metadata": {}, "source": "### Backpropagation: The Math Behind Each Backwards Step\n\nWhile in theory backpropagation might make sense, this is generally one of the hardest concepts to grasp the neural network. This is because backpropagation compounds on the fact that in order to calulate the gradient of the weights, (which is a weight matrix, as we had shown in forward pass), we have to chain the derivatives of the loss with the activations and pre-activations to be able to get the corresponding weight gradient. This means we have to calculate the gradient of the weights and biases for every layer, and tune them individually. For example, let\u0027s look at the following example:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    bias=True\n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "As a disclaimer, I have no idea how this was derived. I only know the formulas. If we want to calculate the gradients of all of the weights with respect to the loss for our example network, it would look something like the following:\n\n$$\\nabla_{W^{(3)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(3)}}{\\partial W^{(3)}}$$\n$$\\nabla_{b^{(3)}} = \\frac{\\partial L}{\\partial z^{(3)}}$$\n$$\\nabla_{W^{(2)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\cdot\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\cdot\\frac{\\partial z^{(2)}}{\\partial W^{(2)}}$$\n$$\\nabla_{b^{(2)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\cdot\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}$$\n$$\\nabla_{W^{(1)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\cdot\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\cdot\\frac{\\partial z^{(2)}}{\\partial a^{(1)}}\\cdot\\frac{\\partial a^{(1)}}{\\partial z^{(1)}}\\cdot\\frac{\\partial z^{(1)}}{\\partial W^{(1)}}$$\n$$\\nabla_{b^{(1)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\cdot\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\cdot\\frac{\\partial z^{(2)}}{\\partial a^{(1)}}\\cdot\\frac{\\partial a^{(1)}}{\\partial z^{(1)}}$$\nThis essentially means you are calulating the loss of the current layer by looking at the loss from the next layer and multiplying it by the derivatives of the previous layer\u0027s activated values and unactivated values.\n\n**Tip:** If you are coding this yourself, it\u0027s a good idea to save the pre-activated ($z$) and activated ($a$) values seperately. \n\nWhere\n  - $\\large\\frac{\\partial L}{\\partial z^{(i)}}$ is the direction of change needed to adjust the third layer\u0027s pre-activated z for the propagation of the loss\n  - $\\large\\frac{\\partial z^{(i)}}{\\partial a^{(i-1)}}$  is the change needed to adjust the previous layer\u0027s node (activated value) for the propogation of the loss\n  - $\\large\\frac{\\partial a^{(i)}}{\\partial z^{(i)}}$ is the change needed to adjust the current layer\u0027s value based on the activation function. Since the activation function is a function itself, that\u0027s how the chain rule ends up growing in length the more you go through layers.\n\n\nIt\u0027s a little hard to visualize these derivative changes, so let\u0027s try and work through the math. I know this is pretty complex, but notice this pattern here:\n  - We only need to compute the derivative with respect to the weight we care about\n  - Otherwise, its just some combination of $a$ (the node output for that layer), and $z$ (the preactivated node output for that layer-think before we apply our activation function) propagated from layers previous. This means that once we compute all of the other previous values, we just have to compute the next one in succession!\n  - You also already compute the gradient for the bias of each layer before finding the gradient of the weights\n\nKnowing all of this, **it\u0027s possible to set up a recursive formula to simplify it, like you would a loop in a coding program!** This part of the program will be pretty math-intensive, so feel free to go back and reread it. I will also have examples of me using backpropagation in examples, so you can get a grasp of it. IF you want even more practice, I personally learned backpropagation by having ChatGPT generate me small feed forward and backpropagation problems for both categorical and regression tasks that I could do by hand, with about 1-2 hidden layers each. Practice makes perfect.\n\n\n#### What the first $\\delta$ is:\nFor our purposes, $\\delta$ just means calculated loss, and likewise $\\delta^{(\\ell)}$ is the loss for that specific layer. The final layer, $\\delta^{(\\ell)}$ is just our derivative loss values, or $\\large\\frac{\\partial L}{\\partial z^{(3)}}$. Do you remember the $(p_i-\\hat{y_i})$ from above? For a hypothetical example, we have 3 categorical variables. Let\u0027s say we\u0027re trying to predict a number being either 1, 2, or 3, just to give meaning to the classification head, and let\u0027s say for the particular example we give it the true value is a 2. This means our $\\hat{y_i}=$\n$$\n\\left( \\begin{array}{cccc}\n0\\\\[0.5em]\n1\\\\[0.5em]\n0\\\\[0.5em]\n\\end{array}\\right)\n$$\nthen let\u0027s assume our model predicts the following values for each category with the softmax:\n\n$$\n\\left( \\begin{array}{cccc}\n.39\\\\[0.5em]\n.33\\\\[0.5em]\n.28\\\\[0.5em]\n\\end{array}\\right)\n$$\n\nthen the loss with respect to the last layer for our hypthetical values will be\n\n$$\n\\delta^{(\\ell)} = \n\\left( \\begin{array}{cccc}\n.39\\\\[0.5em]\n.33\\\\[0.5em]\n.28\\\\[0.5em]\n\\end{array}\\right) -\n\\left( \\begin{array}{cccc}\n0\\\\[0.5em]\n1\\\\[0.5em]\n0\\\\[0.5em]\n\\end{array}\\right)  \n = \n\\left( \\begin{array}{cccc}\n-.61\\\\[0.5em]\n.67\\\\[0.5em]\n-.28\\\\[0.5em]\n\\end{array}\\right)\n$$\nYou can think of it like we are trying to lower the values that are wrong and increase the values that are right! I will give an example of regression for this at the bottom of the document.\n\n\n#### Deriving the update rule\n\nThe most important thing to deriving the update rule is noticing patterns. We can see that any loss for the next layer is simply the loss for the previous layer, with the following values tacked on: $$\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\cdot\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}$$\nThen that means we can let $\\delta^{(\\ell)}$ be the loss of the function per layer. If we look at the pattern I showed you we can build a recursive rule to build a loop with: $\\large\\delta^{(\\ell)}\\cdot\\frac{\\partial z^{(\\ell)}}{\\partial a^{(\\ell-1)}}\\cdot\\frac{\\partial a^{(\\ell-1)}}{\\partial z^{(\\ell-1)}}$.\n\n\nTo calculate the loss $\\delta^{(\\ell-1)}$. We know that $z^{(\\ell)} = W^{(\\ell)}\\cdot a^{(\\ell-1)} + b^{(\\ell)}$, from our forward pass, before activation. We can start from here, because we already computed the derivative of the final layer doing ($p_i-y_i$). This means that taking the derivative of this function via the chain rule would therefore look something like this:\n$$\\frac{\\partial z^{(\\ell)}}{\\partial a^{(\\ell-1)}}[z^{(\\ell)} = W^{(\\ell)}\\cdot a^{(\\ell-1)} + b^{(\\ell)}] = W^{(\\ell)}$$\nsince the bias term just goes away. However for our second derivative $\\large\\frac{\\partial a^{(\\ell-1)}}{\\partial z^{(\\ell-1)}}$, we use the function $a^{(\\ell-1)} = f(W^{(\\ell-1)}\\cdot a^{(\\ell-2)} + b^{(\\ell-1)})$. Notice how this is functionally equivalent to $a^{(\\ell-1)} = f(z^{(\\ell-1)})$. Then\n$$\\frac{\\partial a^{(\\ell-1)}}{\\partial z^{(\\ell-1)}}[f(z^{(\\ell-1)})] = f\u0027(z^{(\\ell-1)})\\cdot 1 = f\u0027(z^{(\\ell-1)})$$\nwhere $f\u0027(x)$ is the derivative of the activation function for the layer $\\ell-1$.\n\nNotice that this z vector is a vector, not a matrix corresponding to the size of the nodes. This means when we finally construct our update rule, we need to do element-wise multiplication, via the hadamard product. \n\n#### Mini-lesson on Hadamard Product\nYou can only multiply these 2 matrices/vectors if they are the same dimensions, and every element in one matrix A $a_{i\\text{ , } j}$ will be multiplied by its corresponding coordinates in matrix B $b_{i\\text{ , } j}$ to get a matrix AB = $a_{i\\text{ , } j}*b_{i\\text{ , } j}$. For example, let\u0027s say I have 2 matrices\n\n$$\nA = \\left( \\begin{array}{cccc}\n1 \u0026 2 \u0026 3\\\\[0.5em]\n4 \u0026 5 \u0026 6\\\\[0.5em]\n7 \u0026 8 \u0026 9\\\\[0.5em]\n\\end{array}\\right) \\text{ and }\nB = \\left( \\begin{array}{cccc}\n1 \u0026 2 \u0026 3\\\\[0.5em]\n4 \u0026 5 \u0026 6\\\\[0.5em]\n7 \u0026 8 \u0026 9\\\\[0.5em]\n\\end{array}\\right)\n$$\n\nTheir hadamard product (element-wise matrix multiplication) would be \n\n$$\n\\left( \\begin{array}{cccc}\n1 \u0026 2 \u0026 3\\\\[0.5em]\n4 \u0026 5 \u0026 6\\\\[0.5em]\n7 \u0026 8 \u0026 9\\\\[0.5em]\n\\end{array}\\right)\\odot\n\\left( \\begin{array}{cccc}\n1 \u0026 2 \u0026 3\\\\[0.5em]\n4 \u0026 5 \u0026 6\\\\[0.5em]\n7 \u0026 8 \u0026 9\\\\[0.5em]\n\\end{array}\\right) = \n\\left( \\begin{array}{cccc}\n1 \u0026 4 \u0026 9\\\\[0.5em]\n16 \u0026 25 \u0026 36\\\\[0.5em]\n49 \u0026 64 \u0026 81\\\\[0.5em]\n\\end{array}\\right) = AB\n$$\n\nnormal matrix multiplication on the other hand would yield\n$$\n\\left( \\begin{array}{cccc}\n1 \u0026 2 \u0026 3\\\\[0.5em]\n4 \u0026 5 \u0026 6\\\\[0.5em]\n7 \u0026 8 \u0026 9\\\\[0.5em]\n\\end{array}\\right) \\text{X}\n\\left( \\begin{array}{cccc}\n1 \u0026 2 \u0026 3\\\\[0.5em]\n4 \u0026 5 \u0026 6\\\\[0.5em]\n7 \u0026 8 \u0026 9\\\\[0.5em]\n\\end{array}\\right) = \n\\left( \\begin{array}{cccc}\n30 \u0026 36 \u0026 42\\\\[0.5em]\n66 \u0026 81 \u0026 96\\\\[0.5em]\n102 \u0026 126 \u0026 150\\\\[0.5em]\n\\end{array}\\right) = AB\n$$\n\n#### Back to Propagation\nwith this now in mind, we put our derivations together to get our new update rule for each layer\u0027s loss:\n\n$$\\large\\delta^{(\\ell-1)} = \\delta^{(\\ell)}\\cdot\\frac{\\partial z^{(\\ell)}}{\\partial a^{(\\ell-1)}}\\cdot\\frac{\\partial a^{(\\ell-1)}}{\\partial z^{(\\ell-1)}} = W^{(\\ell)T}\\delta^{(\\ell)} \\odot f\u0027(z^{(\\ell-1)})\n$$\n\nor less confusingly, \n$$\\Large\\delta^{(\\ell-1)} = W^{(\\ell)T}\\delta^{(\\ell)} \\odot f\u0027(z^{(\\ell-1)}).$$\nThis is the loss for every layer! As a reminder:\n  - $(\\ell)$ is the layer\n  - $\\large \\delta^{(\\ell-1)}$ is the loss for the layer $\\ell-1$ \n  - $\\large W^{(\\ell)}$ is the weight matrix for layer $\\ell$\n  - $\\large \\delta^{(\\ell)}$ is the loss for the layer $\\ell$\n  - $\\large z^{(\\ell-1)}$ is the values calculated for the node BEFORE it is sent through the activation function for that layer\n  - $\\large f(z^{(\\ell)})$ is the activation function for the layer corresponding to the layer of it\u0027s z value\n  - $\\large f\u0027(x)$ is the derivative of the activation function\n  - $\\large f\u0027(z^{(\\ell-1)})$ are the preactivated nodes from the $\\ell-1$ layer, **put through the DERIVATIVE of their activation.** \u003c- This is common source of confusion.\n  - $\\odot$ is the hadamard product, aka element-wise multiplication (**DIFFERENT FROM MATRIX MULTIPLICATION**)\n  - $^T$ is transpose, where we switch the rows to be the columns of a matrix, and the columns to be the rows."}, {"cell_type": "markdown", "metadata": {}, "source": "#### Getting the Gradient of the bias and Weight Matrices\nThe most important part about making update rules is looking for patterns. Look at the following: What do you notice?\n\n##### Bias\nLet\u0027s look at our previous examples for bias derivations:\n$$\\nabla_{b^{(3)}} = \\frac{\\partial L}{\\partial z^{(3)}}$$\n$$\\nabla_{b^{(2)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\cdot\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}$$\n$$\\nabla_{b^{(1)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\cdot\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\cdot\\frac{\\partial z^{(2)}}{\\partial a^{(1)}}\\cdot\\frac{\\partial a^{(1)}}{\\partial z^{(1)}}$$\n\nDid you notice that this is exactly the loss of every single layer? This means that for our update rule, once we find our the loss we find our bias! in other words\n$$\\Large\\delta^{(\\ell)}=\\nabla_{b^{(\\ell)}}$$\n\n##### Weights\nfor $\\large\\nabla_{W^{(\\ell)}}$, we notice that the only difference is now we tack on $\\large\\frac{\\partial z^{(3)}}{\\partial W^{(3)}}$ or \n$$\\nabla_{W^{(3)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(\\ell)}}{\\partial W^{(\\ell)}}$$\n$$\\nabla_{W^{(2)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\cdot\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\cdot\\frac{\\partial z^{(2)}}{\\partial W^{(2)}}$$\n$$\\nabla_{W^{(1)}} = \\frac{\\partial L}{\\partial z^{(3)}}\\cdot\\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\cdot\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\cdot\\frac{\\partial z^{(2)}}{\\partial a^{(1)}}\\cdot\\frac{\\partial a^{(1)}}{\\partial z^{(1)}}\\cdot\\frac{\\partial z^{(1)}}{\\partial W^{(1)}}$$\n\nall of these are essentially loss multiplied by the derivative of the weight matrix (gradient). So then we can build the following rule, by looking at the patterns:\n$$\\large\\nabla_{W^{(\\ell)}} = \\delta^{(\\ell)}\\cdot\\frac{\\partial z^{(\\ell)}}{\\partial W^{(\\ell)}}$$\nif we deconstruct this derivative, the top is the function $z^{(\\ell)}$. We then can grab the function corresponding: $z^{(\\ell)} = W^{(\\ell)}\\cdot a^{(\\ell-1)} + b^{(\\ell)}$. Then \n$$\\frac{\\partial z^{(\\ell)}}{\\partial W^{(\\ell)}}[W^{(\\ell)}\\cdot a^{(\\ell-1)} + b^{(\\ell)}] =  a^{(\\ell-1)}$$\nNotice that $\\delta{(\\ell)}$ and $a^{(\\ell-1)}$ do not match dimension, nor will they create a matrix of the same dimensions of the weight matrix. They are both dimensions $\\large n^{(\\ell)}\\text{x}1$ and $\\large n^{(\\ell-1)}\\text{x}1$ respectively, where $n$ is the number of nodes in the layer. IF we want to make it the size of the weight matrix, we do $\\large a^{(\\ell-1)T}$ so we get dimensions $\\large n^{(\\ell)}\\text{x}1$ and $\\large 1\\text{x}n^{(\\ell-1)}$ for the matrix outer product, which matches the dimensions of the weight matrix. We can now find out what the update rule for the gradient of a weight matrix of any given layer!\n$$\\Large\\nabla_{W^{(\\ell)}}=\\delta^{(\\ell)}\\cdot a^{(\\ell-1)T}$$\n\n### Final Update Rules\nBased on the following update rules, we have deduced recursively the update rules for any given layer, and they are as follows:\n\n$$\\Large\\text{The first }\\delta^{(\\ell)} = (p_i-\\hat{y_i}) \\text{ for classification, or } (\\hat{y_i}-\\bar{y_i}) \\text{ for regression.}$$\n$$\\Large\\nabla_{W^{(\\ell)}}=\\delta^{(\\ell)} a^{(\\ell-1)T}$$\n$$\\Large\\nabla_{b^{(\\ell)}} = \\delta^{(\\ell)}$$\n$$\\Large\\delta^{(\\ell-1)} = W^{(\\ell)T}\\delta^{(\\ell)} \\odot f\u0027(z^{(\\ell-1)}).$$\n\nWhere:\n  - $\\hat{y_i}$ is the predicted value generated by your network\u0027s forward pass for any given category, for a certain sample.\n  - $\\bar{y_i}$ is the actual value/label given to you for any given category, for a certain sample. This will be 0/1 for classification and some real number for regression.\n  - $p_i$ is the softmax function.\n  - $(\\ell)$ is the layer\n  - $\\large W^{(\\ell)}$ is the weight matrix for layer $\\ell$\n  - $\\large \\delta^{(\\ell)}$ is the loss for the layer $\\ell$\n  - Likewise, $\\large \\delta^{(\\ell-1)}$ is the loss for the layer $\\ell-1$ \n  - $\\large z^{(\\ell-1)}$ is the values calculated for the node BEFORE it is sent through the activation function for that layer\n  - $\\large f(z^{(\\ell)})$ is the activation function for the layer corresponding to the layer of it\u0027s preactivated nodes\n  - $\\large f\u0027(x)$ is the derivative of the activation function\n  - $\\large f\u0027(z^{(\\ell-1)})$ are the preactivated nodes from the $\\ell-1$ layer, **put through the DERIVATIVE of their activation.** \u003c- This is common source of confusion.\n  - $\\odot$ is the hadamard product, aka element-wise multiplication (**DIFFERENT FROM MATRIX MULTIPLICATION**)\n  - $^T$ is transpose, where we switch the rows to be the columns of a matrix, and the columns to be the rows.\n  - $\\large\\delta^{(\\ell)}\\cdot a^{(\\ell-1)T}$ is an outer product\n  - $\\large a^{(\\ell)} = f(W^{(\\ell)}\\cdot a^{(\\ell-1)} + b^{(\\ell)}) = f(z^{(\\ell)})$\n  - $\\large z^{(\\ell)} = W^{(\\ell)}\\cdot a^{(\\ell-1)} + b^{(\\ell)}$\n\nThis is definitely a lot of information, so make sure to come back to this a couple of times to be able to get it."}, {"cell_type": "markdown", "metadata": {}, "source": "### Weight updates (motivations of gradient descent)\n\nNow that we are able to derive the gradients of the weights, we can take a step in the direction of these gradients. Naively, we can do this by subtracting the gradients from the current weights, and likewise with the biases:\n$$W^{(\\ell)}_{new} = W^{(\\ell)} - \\nabla W^{(\\ell)}$$\n$$b^{(\\ell)}_{new} = b^{(\\ell)} - \\nabla b^{(\\ell)}$$\n\nThis basic idea is how gradient descent works, however there is an important caveat: We might take too big of a step. Let\u0027s visualize how a parameter space for a loss function MIGHT look. Run the following simulation:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sim = GradientDescentSimulator(learning_rate=50, title=\"Gradient Descent With no Learning Rate\", max_iter=100)\nsim.fig.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Do you see what happens? The gradient tells us to move quickly where it\u0027s steep, and as a result, we \"overshoot\" our minimum point of loss. We keep moving back and forth, and we will never reach the best minima.\n\nHere\u0027s an analogy: Think you are holding a basketball in a dense fog, so dense that you can\u0027t see anything. This is basketball your neural network. Your goal is to throw the basketball into the hoop, but you obviously don\u0027t know where it is because you can\u0027t see You have some information telling you which direction to throw the basketball and it tells you how hard to throw the ball. Every time you throw the ball, you\u0027re automatically teleported to the ball, so you don\u0027t get to walk over the landscape to see if you can guess where to shoot. For some reason, the information telling you to throw it harder and harder when you\u0027re close to the goal. As a result, you\u0027re never able to get the ball into the hoop. You don\u0027t have all day!\n\nWe can see that in our simulation, the curve gets steeper the closer we get to the local minimum. As a result, the gradient tells us to throw our ball REALLY hard, because the negative gradient is steep, like we saw in the example above. So how do we fix this?\n\n### Our first Hyperparameter: $\\eta$ (learning rate) and the Gradient Descent Algorithm\n\nWe can add some small constant $\\eta$ to the negative gradients to \"dampen\" their effects on the weights. This will effectively make us \"toss\" the basketball a bunch of times, so that we inch towards the goal. This kind of makes more sense, because we can\u0027t see! The classic gradient descent algorithm can therefore be written as such: \n\n$$\\large W^{(\\ell)}_{new} = W^{(\\ell)} - \\eta\\nabla W^{(\\ell)}$$\n$$\\large b^{(\\ell)}_{new} = b^{(\\ell)} - \\eta\\nabla b^{(\\ell)}$$\n\nwhere\n - $\\eta$ is the learning rate. This is usually some small number, custom to every neural network problem! (You need to figure out the best one yourself)\n - $\\nabla b^{(\\ell)}$ is the gradient of the biases for that layer $\\ell$\n - $\\nabla W^{(\\ell)}$ is the gradient of the Weight Matrix for that layer $\\ell$\n - $W^{(\\ell)}$ is the weight matrix for layer $\\ell$\n - $b^{(\\ell)}$ is the biases for layer $\\ell$\n - $W^{(\\ell)}_{new}$ is the updated weight matrix for layer $\\ell$\n - $b^{(\\ell)}_{new}$ is the updated biases matrix for layer $\\ell$\n\nLet\u0027s see what happens when we add this constant to our simulation:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sim = GradientDescentSimulator(learning_rate=10, title=\"Gradient Descent With Good Learning Rate\",max_iter=100)\nsim.fig.show()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "As you can see, we converge perfectly! Conversely it\u0027s important not to make the learning rate too low, or we have the opposite problem that we saw in the first simulation:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sim = GradientDescentSimulator(learning_rate=1, title=\"Gradient Descent With Low Learning Rate\",max_iter=100)\nsim.fig.show()"}, {"cell_type": "markdown", "metadata": {}, "source": " As you can see the ball moves so slowly that it will never reach its local minima in the time that we need it to. As a result we strain our computer, while not getting a good model.\n \n Now that we know what learning rate ($\\eta$) is, we now have all the tools to go through a full backpropagation example."}, {"cell_type": "markdown", "metadata": {}, "source": "### Backpropagation Example\n\n In the meantime, let\u0027s use our new update rules to apply backpropagation to our synthetic model we had defined previously:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=3,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n    \n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Layer 3\n\nLet\u0027s recall our toolbox of update rules:\n$$\\text{The first }\\delta^{(\\ell)} = (p_i-\\hat{y_i}) \\text{ for classification, or } (\\hat{y_i}-\\bar{y_i}) \\text{ for regression.}$$\n$$\\nabla_{W^{(\\ell)}}=\\delta^{(\\ell)} a^{(\\ell-1)T}$$\n$$\\nabla_{b^{(\\ell)}} = \\delta^{(\\ell)}$$\n$$\\delta^{(\\ell-1)} = W^{(\\ell)T}\\delta^{(\\ell)} \\odot f\u0027(z^{(\\ell-1)})$$\n\n#### Step 1: Calculate initial loss\nBecause we have specified that this is a classification problem, we know:\n$$\\delta^{(3)} = \n\\left( \\begin{array}{cccc}\np_1 - y_1 \\\\[0.5em]\np_2 - y_2 \\\\[0.5em]\np_3 - y_3\\\\[0.5em]\n\\end{array}\\right)$$\n\n#### Step 2: Calculate the weight and bias gradients\n\nWe are now at this point on the graph:\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=3,\n    special_inter_layer_arrow_offset=\"layer\",\n    bias=True\n    \n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We can calculate our weights with the update rule $\\nabla_{W^{(\\ell)}}=\\delta^{(\\ell)} a^{(\\ell-1)T}$.\n\n$$\\Large\\nabla_{W^{(3)}} = \\delta^{(3)} a^{(2)T}\\normalsize = \n\\left( \\begin{array}{cccc}\np_1 - y_1 \\\\[0.5em]\np_2 - y_2 \\\\[0.5em]\np_3 - y_3\\\\[0.5em]\n\\end{array}\\right)\\cdot\n\\left( \\begin{array}{cccc}\n\\text{Node 1}^{(2)} \\\\[0.5em]\n\\text{Node 2}^{(2)} \\\\[0.5em]\n\\text{Node 3}^{(2)}\\\\[0.5em]\n\\text{Node 4}^{(2)}\\\\[0.5em]\n\\end{array}\\right)^T =\n$$\nwhere Node $i$ is the activated value coming out of that node, or $a_i^{(2)}$.\n$$\\left( \\begin{array}{cccc}\np_1 - y_1 \\\\[0.5em]\np_2 - y_2 \\\\[0.5em]\np_3 - y_3\\\\[0.5em]\n\\end{array}\\right)\\cdot\n\\left( \\begin{array}{cccc}\n\\text{Node 1}^{(2)} \u0026 \\text{Node 2}^{(2)}\u0026 \\text{Node 3}^{(2)}\u0026 \\text{Node 4}^{(2)}\\\\[0.5em]\n\\end{array}\\right) =\n$$\n$$\\left( \\begin{array}{cccc}\n(p_1 - y_1)\\cdot\\text{Node 1}^{(2)} \u0026 (p_1 - y_1)\\cdot\\text{Node 2}^{(2)} \u0026 (p_1 - y_1)\\cdot\\text{Node 3}^{(2)} \u0026 (p_1 - y_1)\\cdot\\text{Node 4}^{(2)} \\\\[0.5em]\n(p_2 - y_2)\\cdot\\text{Node 1}^{(2)} \u0026 (p_2 - y_2)\\cdot\\text{Node 2}^{(2)} \u0026 (p_2 - y_2)\\cdot\\text{Node 3}^{(2)} \u0026 (p_2 - y_2)\\cdot\\text{Node 4}^{(2)} \\\\[0.5em]\n(p_3 - y_3)\\cdot\\text{Node 1}^{(2)} \u0026 (p_3 - y_3)\\cdot\\text{Node 2}^{(2)} \u0026 (p_3 - y_3)\\cdot\\text{Node 3}^{(2)} \u0026 (p_3 - y_3)\\cdot\\text{Node 4}^{(2)}\n\\end{array}\\right) \n$$\n\nWe calculate our bias to be via the update rule:\n\n$$\n\\Large\\nabla_{b^{(3)}} = \\delta^{(3)} \\normalsize= \n\\left( \\begin{array}{cccc}\np_1 - y_1 \\\\[0.5em]\np_2 - y_2 \\\\[0.5em]\np_3 - y_3\\\\[0.5em]\n\\end{array}\\right)\n$$\n\n#### Step 3: Perform Gradient descent:\n$$\\large W^{(3)}_{new} = W^{(3)} - \\eta\\nabla W^{(3)}$$\n$$\\Large W^{(3)} \\normalsize= \\left( \\begin{array}{cccc}\nw_{1,1}^{(3)}\u0026 w_{1,2}^{(3)} \u0026 w_{1,3}^{(3)} \u0026 w_{1,4}^{(3)}\\\\[0.5em]\nw_{2,1}^{(3)}\u0026 w_{2,2}^{(3)} \u0026 w_{2,3}^{(3)} \u0026 w_{2,4}^{(3)}\\\\[0.5em]\nw_{3,1}^{(3)}\u0026 w_{3,2}^{(3)} \u0026 w_{3,3}^{(3)} \u0026 w_{3,4}^{(3)}\\\\[0.5em]\n\\end{array}\\right) $$\n$$\\large\\nabla_{W^{(3)}} \\normalsize = \n\\left( \\begin{array}{cccc}\n(p_1 - y_1)\\cdot\\text{Node 1}^{(2)} \u0026 (p_1 - y_1)\\cdot\\text{Node 2}^{(2)} \u0026 (p_1 - y_1)\\cdot\\text{Node 3}^{(2)} \u0026 (p_1 - y_1)\\cdot\\text{Node 4}^{(2)} \\\\[0.5em]\n(p_2 - y_2)\\cdot\\text{Node 1}^{(2)} \u0026 (p_2 - y_2)\\cdot\\text{Node 2}^{(2)} \u0026 (p_2 - y_2)\\cdot\\text{Node 3}^{(2)} \u0026 (p_2 - y_2)\\cdot\\text{Node 4}^{(2)} \\\\[0.5em]\n(p_3 - y_3)\\cdot\\text{Node 1}^{(2)} \u0026 (p_3 - y_3)\\cdot\\text{Node 2}^{(2)} \u0026 (p_3 - y_3)\\cdot\\text{Node 3}^{(2)} \u0026 (p_3 - y_3)\\cdot\\text{Node 4}^{(2)}\n\\end{array}\\right)\n$$\n\n$$\\large b^{(3)}_{new} = b^{(3)} - \\eta\\nabla b^{(3)}$$\n$$\\large b^{(3)} \\normalsize = \n\\left( \\begin{array}{cccc}\nb_{1}^{(3)} \\\\[0.5em]\nb_{2}^{(3)} \\\\[0.5em]\nb_{3}^{(3)} \\\\[0.5em]\n\\end{array}\\right) \n$$\n$$\\large\\nabla b^{(3)}=\\delta^{(3)} \\normalsize\n\\left( \\begin{array}{cccc}\np_1 - y_1 \\\\[0.5em]\np_2 - y_2 \\\\[0.5em]\np_3 - y_3\\\\[0.5em]\n\\end{array}\\right)$$\n\n$$\\large W^{(3)}_{new} = \\normalsize\\left( \\begin{array}{cccc}\nw_{1,1}^{(3)}\u0026 w_{1,2}^{(3)} \u0026 w_{1,3}^{(3)} \u0026 w_{1,4}^{(3)}\\\\[0.5em]\nw_{2,1}^{(3)}\u0026 w_{2,2}^{(3)} \u0026 w_{2,3}^{(3)} \u0026 w_{2,4}^{(3)}\\\\[0.5em]\nw_{3,1}^{(3)}\u0026 w_{3,2}^{(3)} \u0026 w_{3,3}^{(3)} \u0026 w_{3,4}^{(3)}\\\\[0.5em]\n\\end{array}\\right) - \\eta\\left( \\begin{array}{cccc}\n(p_1 - y_1)\\cdot\\text{Node 1}^{(2)} \u0026 (p_1 - y_1)\\cdot\\text{Node 2}^{(2)} \u0026 (p_1 - y_1)\\cdot\\text{Node 3}^{(2)} \u0026 (p_1 - y_1)\\cdot\\text{Node 4}^{(2)} \\\\[0.5em]\n(p_2 - y_2)\\cdot\\text{Node 1}^{(2)} \u0026 (p_2 - y_2)\\cdot\\text{Node 2}^{(2)} \u0026 (p_2 - y_2)\\cdot\\text{Node 3}^{(2)} \u0026 (p_2 - y_2)\\cdot\\text{Node 4}^{(2)} \\\\[0.5em]\n(p_3 - y_3)\\cdot\\text{Node 1}^{(2)} \u0026 (p_3 - y_3)\\cdot\\text{Node 2}^{(2)} \u0026 (p_3 - y_3)\\cdot\\text{Node 3}^{(2)} \u0026 (p_3 - y_3)\\cdot\\text{Node 4}^{(2)}\n\\end{array}\\right)$$\n\n$$\\large b^{(3)}_{new}\\normalsize = \n\\left( \\begin{array}{cccc}\nb_{1}^{(3)} \\\\[0.5em]\nb_{2}^{(3)} \\\\[0.5em]\nb_{3}^{(3)} \\\\[0.5em]\n\\end{array}\\right) \n-\\eta\n\\left( \\begin{array}{cccc}\np_1 - y_1 \\\\[0.5em]\np_2 - y_2 \\\\[0.5em]\np_3 - y_3\\\\[0.5em]\n\\end{array}\\right)$$\n"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Layer 2: Calculate Loss for the New Layer\nWe are now here in our neural network:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=2,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n    \n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 1: Calculate loss\nThis part is a little tricky, but just remember our update rules. You can find activation functions and their derivatives above. Remember $a^{(\\ell)}$ does NOT refer to the activation function, but rather the number we get from it. I use Node i and  $a_i^{(\\ell)}$ interchangeably. Our update rule is as follows: $\\delta^{(2)} = W^{(3)T}\\delta^{(3)} \\odot f\u0027(z^{(2)})$\n\n$$\\large \\delta^{(2)} \\normalsize=  \\left( \\begin{array}{cccc}\nw_{1,1}^{(3)}\u0026 w_{1,2}^{(3)} \u0026 w_{1,3}^{(3)} \u0026 w_{1,4}^{(3)}\\\\[0.5em]\nw_{2,1}^{(3)}\u0026 w_{2,2}^{(3)} \u0026 w_{2,3}^{(3)} \u0026 w_{2,4}^{(3)}\\\\[0.5em]\nw_{3,1}^{(3)}\u0026 w_{3,2}^{(3)} \u0026 w_{3,3}^{(3)} \u0026 w_{3,4}^{(3)}\\\\[0.5em]\n\\end{array}\\right)^T\n\\left( \\begin{array}{cccc}\np_1 - y_1 \\\\[0.5em]\np_2 - y_2 \\\\[0.5em]\np_3 - y_3\\\\[0.5em]\n\\end{array}\\right)\\odot\nf\u0027\\left( \\begin{array}{cccc}\nz_{1}^{(2)} \\\\[0.5em]\nz_{2}^{(2)} \\\\[0.5em]\nz_{3}^{(2)} \\\\[0.5em]\nz_{4}^{(2)} \\\\[0.5em]\n\\end{array}\\right)\n$$\n\n$$\\large \\delta^{(2)} =  \\left( \\begin{array}{ccc}\nw_{1,1}^{(3)} \u0026 w_{2,1}^{(3)} \u0026 w_{3,1}^{(3)} \\\\[0.5em]\nw_{1,2}^{(3)} \u0026 w_{2,2}^{(3)} \u0026 w_{3,2}^{(3)} \\\\[0.5em]\nw_{1,3}^{(3)} \u0026 w_{2,3}^{(3)} \u0026 w_{3,3}^{(3)} \\\\[0.5em]\nw_{1,4}^{(3)} \u0026 w_{2,4}^{(3)} \u0026 w_{3,4}^{(3)}\n\\end{array}\\right)\n\\left( \\begin{array}{cccc}\np_1 - y_1 \\\\[0.5em]\np_2 - y_2 \\\\[0.5em]\np_3 - y_3\\\\[0.5em]\n\\end{array}\\right)\\odot\nf\u0027\\left( \\begin{array}{cccc}\nz_{1}^{(2)}\\\\[0.5em]\nz_{2}^{(2)}\\\\[0.5em]\nz_{3}^{(2)}\\\\[0.5em]\nz_{4}^{(2)}\\\\[0.5em]\n\\end{array}\\right)\n$$\n\nto simplify this section, we will simplify each matrix knowing their constituent parts. We know for our $\\delta^{(3)}, p_i - y_i$ is just some positive or negative in $-1\\leq0\\leq1$. This number can be represented as such: $\\delta_i^{(3)}$. So we let $\\delta^{(3)}$ = \n$$\n\\left( \\begin{array}{cccc}\n\\delta_1^{(3)}\\\\[0.5em]\n\\delta_2^{(3)} \\\\[0.5em]\n\\delta_3^{(3)}\\\\[0.5em]\n\\end{array}\\right)\n$$\n\nthen \n$$W^{(3)T}\\delta^{(3)} = \\left( \\begin{array}{ccc}\nw_{1,1}^{(3)} \u0026 w_{2,1}^{(3)} \u0026 w_{3,1}^{(3)} \\\\[0.5em]\nw_{1,2}^{(3)} \u0026 w_{2,2}^{(3)} \u0026 w_{3,2}^{(3)} \\\\[0.5em]\nw_{1,3}^{(3)} \u0026 w_{2,3}^{(3)} \u0026 w_{3,3}^{(3)} \\\\[0.5em]\nw_{1,4}^{(3)} \u0026 w_{2,4}^{(3)} \u0026 w_{3,4}^{(3)}\n\\end{array}\\right)\\left( \\begin{array}{cccc}\n\\delta_1^{(3)}\\\\[0.5em]\n\\delta_2^{(3)} \\\\[0.5em]\n\\delta_3^{(3)}\\\\[0.5em]\n\\end{array}\\right)\\odot\nf\u0027\\left( \\begin{array}{cccc}\nz_{1}^{(2)} \\\\[0.5em]\nz_{2}^{(2)} \\\\[0.5em]\nz_{3}^{(2)} \\\\[0.5em]\nz_{4}^{(2)} \\\\[0.5em]\n\\end{array}\\right) = \n$$\n$$\n\\left(\\begin{array}{cccc}\n w_{1,1}^{(3)}\\delta_1^{(3)} + w_{2,1}^{(3)}\\delta_2^{(3)} + w_{3,1}^{(3)}\\delta_3^{(3)} \\\\[0.5em]\nw_{1,2}^{(3)}\\delta_1^{(3)} + w_{2,2}^{(3)}\\delta_2^{(3)} + w_{3,2}^{(3)}\\delta_3^{(3)} \\\\[0.5em]\nw_{1,3}^{(3)}\\delta_1^{(3)} + w_{2,3}^{(3)}\\delta_2^{(3)} + w_{3,3}^{(3)}\\delta_3^{(3)} \\\\[0.5em]\nw_{1,4}^{(3)}\\delta_1^{(3)} + w_{2,4}^{(3)}\\delta_2^{(3)} + w_{3,4}^{(3)}\\delta_3^{(3)} \\\\[0.5em]\n\\end{array}\\right)\\odot\nf\u0027\\left( \\begin{array}{cccc}\nz_{1}^{(2)} \\\\[0.5em]\nz_{2}^{(2)} \\\\[0.5em]\nz_{3}^{(2)} \\\\[0.5em]\nz_{4}^{(2)} \\\\[0.5em]\n\\end{array}\\right)\n$$\n\nWhere f\u0027(x) is the DERIVATIVE of some [activation function](#types-of-activation-functions). We then get that \n$$\\Large \\delta^{(2)} = \\normalsize\n\\left(\\begin{array}{cccc}\n(w_{1,1}^{(3)}\\delta_1^{(3)} + w_{2,1}^{(3)}\\delta_2^{(3)} + w_{3,1}^{(3)}\\delta_3^{(3)})*f\u0027(z_{1}^{(2)})\\\\[0.5em]\n(w_{1,2}^{(3)}\\delta_1^{(3)} + w_{2,2}^{(3)}\\delta_2^{(3)} + w_{3,2}^{(3)}\\delta_3^{(3)})*f\u0027(z_{2}^{(2)}) \\\\[0.5em]\n(w_{1,3}^{(3)}\\delta_1^{(3)} + w_{2,3}^{(3)}\\delta_2^{(3)} + w_{3,3}^{(3)}\\delta_3^{(3)})*f\u0027(z_{3}^{(2)})\\\\[0.5em]\n(w_{1,4}^{(3)}\\delta_1^{(3)} + w_{2,4}^{(3)}\\delta_2^{(3)} + w_{3,4}^{(3)}\\delta_3^{(3)})*f\u0027(z_{4}^{(2)}) \\\\[0.5em]\n\\end{array}\\right)\n$$\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "notice how each row is a single number. We can simplify this as\n$$\\Large \\delta^{(2)}\\normalsize =\n\\left(\\begin{array}{cccc}\n(w_{1,1}^{(3)}\\delta_1^{(3)} + w_{2,1}^{(3)}\\delta_2^{(3)} + w_{3,1}^{(3)}\\delta_3^{(3)})*f\u0027(z_{1}^{(2)})\\\\[0.5em]\n(w_{1,2}^{(3)}\\delta_1^{(3)} + w_{2,2}^{(3)}\\delta_2^{(3)} + w_{3,2}^{(3)}\\delta_3^{(3)})*f\u0027(z_{2}^{(2)}) \\\\[0.5em]\n(w_{1,3}^{(3)}\\delta_1^{(3)} + w_{2,3}^{(3)}\\delta_2^{(3)} + w_{3,3}^{(3)}\\delta_3^{(3)})*f\u0027(z_{3}^{(2)})\\\\[0.5em]\n(w_{1,4}^{(3)}\\delta_1^{(3)} + w_{2,4}^{(3)}\\delta_2^{(3)} + w_{3,4}^{(3)}\\delta_3^{(3)})*f\u0027(z_{4}^{(2)}) \\\\[0.5em]\n\\end{array}\\right)=\n\\left( \\begin{array}{cccc}\n\\delta_1^{(2)}\\\\[0.5em]\n\\delta_2^{(2)} \\\\[0.5em]\n\\delta_3^{(2)}\\\\[0.5em]\n\\delta_4^{(2)}\\\\[0.5em]\n\\end{array}\\right)\n$$\nWe will use this matrix for our further calculations to avoid confusion."}, {"cell_type": "markdown", "metadata": {}, "source": "We are now here in our backpropagation:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=2,\n    special_inter_layer_arrow_offset=\"layer\",\n    bias=True\n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We will now go a little faster, now that we\u0027ve seen the process.\n#### Step 2: Calculate $\\nabla_{W^{(2)}}$ and $\\nabla_{b^{(2)}}$\nOur update rule for this layer is $\\nabla_{W^{(2)}}=\\delta^{(2)} a^{(1)T}$ and for bias $\\nabla_{b^{(2)}}=\\delta^{(2)}$. \n"}, {"cell_type": "markdown", "metadata": {}, "source": "Our weight gradient\n$$\\Large\\nabla_{W^{(2)}} = \\delta^{(2)} a^{(1)T}\\normalsize = \n\\left( \\begin{array}{cccc}\n\\delta_1^{(2)} \\\\[0.5em]\n\\delta_2^{(2)} \\\\[0.5em]\n\\delta_3^{(2)} \\\\[0.5em]\n\\delta_4^{(2)} \\\\[0.5em]\n\\end{array}\\right)\\cdot\n\\left( \\begin{array}{cccc}\n\\text{Node 1}^{(1)} \\\\[0.5em]\n\\text{Node 2}^{(1)} \\\\[0.5em]\n\\end{array}\\right)^T =\n$$\n$$\\left( \\begin{array}{cccc}\n\\delta_1^{(2)} \\\\[0.5em]\n\\delta_2^{(2)} \\\\[0.5em]\n\\delta_3^{(2)} \\\\[0.5em]\n\\delta_4^{(2)} \\\\[0.5em]\n\\end{array}\\right)\\cdot\n\\left( \\begin{array}{cccc}\n\\text{Node 1}^{(1)} \u0026 \\text{Node 2}^{(1)}\\\\[0.5em]\n\\end{array}\\right) =\n$$\n$$\\left( \\begin{array}{cc}\n\\delta_1^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_1^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\delta_2^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_2^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\delta_3^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_3^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\delta_4^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_4^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\end{array}\\right)\n$$\n\nCalculating Bias gradient:\n\n$$\\nabla_{b^{(2)}}=\\delta^{(2)} =\\left( \\begin{array}{cccc}\n\\delta_1^{(2)} \\\\[0.5em]\n\\delta_2^{(2)} \\\\[0.5em]\n\\delta_3^{(2)} \\\\[0.5em]\n\\delta_4^{(2)} \\\\[0.5em]\n\\end{array}\\right) $$\n"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 3: Update weights and biases for layer 2 with gradient descent\n\nfor weights:\n$$\\Large W^{(2)} \\normalsize = \\left( \\begin{array}{cc}\nw_{1,1}^{(2)} \u0026 w_{1,2}^{(2)} \\\\[0.5em]\nw_{2,1}^{(2)} \u0026 w_{2,2}^{(2)} \\\\[0.5em]\nw_{3,1}^{(2)} \u0026 w_{3,2}^{(2)} \\\\[0.5em]\nw_{4,1}^{(2)} \u0026 w_{4,2}^{(2)} \\\\[0.5em]\n\\end{array}\\right)\n$$\n$$\\Large\\nabla_{W^{(2)}} \\normalsize = \\left( \\begin{array}{cc}\n\\delta_1^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_1^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\delta_2^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_2^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\delta_3^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_3^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\delta_4^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_4^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\end{array}\\right)\n$$\n\nfor bias:\n$$b^{(2)} = \\left( \\begin{array}{cccc}\nb_1^{(2)} \\\\[0.5em]\nb_2^{(2)} \\\\[0.5em]\nb_3^{(2)} \\\\[0.5em]\nb_4^{(2)} \\\\[0.5em]\n\\end{array}\\right)$$\n$$\\nabla_{b^{(2)}}=\\delta^{(2)} =\\left( \\begin{array}{cccc}\n\\delta_1^{(2)} \\\\[0.5em]\n\\delta_2^{(2)} \\\\[0.5em]\n\\delta_3^{(2)} \\\\[0.5em]\n\\delta_4^{(2)} \\\\[0.5em]\n\\end{array}\\right) $$\n\nstep:\n$$\n\\Large W_{new}^{(2)} \\normalsize =\n\\left( \\begin{array}{cc}\nw_{1,1}^{(2)} \u0026 w_{1,2}^{(2)} \\\\[0.5em]\nw_{2,1}^{(2)} \u0026 w_{2,2}^{(2)} \\\\[0.5em]\nw_{3,1}^{(2)} \u0026 w_{3,2}^{(2)} \\\\[0.5em]\nw_{4,1}^{(2)} \u0026 w_{4,2}^{(2)} \\\\[0.5em]\n\\end{array}\\right) - \\eta\\left( \\begin{array}{cc}\n\\delta_1^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_1^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\delta_2^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_2^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\delta_3^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_3^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\delta_4^{(2)}\\cdot\\text{Node 1}^{(1)} \u0026 \\delta_4^{(2)}\\cdot\\text{Node 2}^{(1)} \\\\[0.5em]\n\\end{array}\\right)\n$$\n\n$$\n\\Large b_{new}^{(2)} \\normalsize =\nb^{(2)} = \\left( \\begin{array}{cccc}\nb_1^{(2)} \\\\[0.5em]\nb_2^{(2)} \\\\[0.5em]\nb_3^{(2)} \\\\[0.5em]\nb_4^{(2)} \\\\[0.5em]\n\\end{array}\\right) - \\eta\n\\left( \\begin{array}{cccc}\n\\delta_1^{(2)} \\\\[0.5em]\n\\delta_2^{(2)} \\\\[0.5em]\n\\delta_3^{(2)} \\\\[0.5em]\n\\delta_4^{(2)} \\\\[0.5em]\n\\end{array}\\right)\n$$"}, {"cell_type": "markdown", "metadata": {}, "source": "### Layer 1\nWe are now here in our diagram:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=1,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 1: Calculate loss for the new layer\nOur update rule for this layer\u0027s loss is $\\delta^{(1)} = W^{(2)T}\\delta^{(2)} \\odot f\u0027(z^{(1)})$\n$$\n\\Large \\delta^{(1)} = \\normalsize\\left( \\begin{array}{cc}\nw_{1,1}^{(2)} \u0026 w_{1,2}^{(2)} \\\\[0.5em]\nw_{2,1}^{(2)} \u0026 w_{2,2}^{(2)} \\\\[0.5em]\nw_{3,1}^{(2)} \u0026 w_{3,2}^{(2)} \\\\[0.5em]\nw_{4,1}^{(2)} \u0026 w_{4,2}^{(2)} \\\\[0.5em]\n\\end{array}\\right)^T\\left( \\begin{array}{cccc}\n\\delta_1^{(2)} \\\\[0.5em]\n\\delta_2^{(2)} \\\\[0.5em]\n\\delta_3^{(2)} \\\\[0.5em]\n\\delta_4^{(2)} \\\\[0.5em]\n\\end{array}\\right) \\odot\nf\u0027\\left( \\begin{array}{cccc}\nz_{1}^{(1)} \\\\[0.5em]\nz_{2}^{(1)} \\\\[0.5em]\n\\end{array}\\right) = \n$$\n$$\n\\left( \\begin{array}{cccc}\nw_{1,1}^{(2)} \u0026 w_{2,1}^{(2)} \u0026 w_{3,1}^{(2)} \u0026 w_{4,1}^{(2)} \\\\[0.5em]\nw_{1,2}^{(2)} \u0026 w_{2,2}^{(2)} \u0026 w_{3,2}^{(2)} \u0026 w_{4,2}^{(2)} \\\\[0.5em]\n\\end{array} \\right)\\left( \\begin{array}{cccc}\n\\delta_1^{(2)} \\\\[0.5em]\n\\delta_2^{(2)} \\\\[0.5em]\n\\delta_3^{(2)} \\\\[0.5em]\n\\delta_4^{(2)} \\\\[0.5em]\n\\end{array}\\right) \\odot\nf\u0027\\left( \\begin{array}{cccc}\nz_{1}^{(1)} \\\\[0.5em]\nz_{2}^{(1)} \\\\[0.5em]\n\\end{array}\\right) = \n$$\n$$\n\\left( \\begin{array}{c}\nw_{1,1}^{(2)}\\cdot\\delta_1^{(2)} + w_{2,1}^{(2)}\\cdot\\delta_2^{(2)} + w_{3,1}^{(2)}\\cdot\\delta_3^{(2)} + w_{4,1}^{(2)}\\cdot\\delta_4^{(2)} \\\\[0.5em]\nw_{1,2}^{(2)}\\cdot\\delta_1^{(2)} + w_{2,2}^{(2)}\\cdot\\delta_2^{(2)} + w_{3,2}^{(2)}\\cdot\\delta_3^{(2)} + w_{4,2}^{(2)}\\cdot\\delta_4^{(2)} \\\\[0.5em]\n\\end{array} \\right) \\odot\nf\u0027\\left( \\begin{array}{cccc}\nz_{1}^{(1)} \\\\[0.5em]\nz_{2}^{(1)} \\\\[0.5em]\n\\end{array}\\right) = \n$$\n$$\n\\left( \\begin{array}{c}\n(w_{1,1}^{(2)}\\cdot\\delta_1^{(2)} + w_{2,1}^{(2)}\\cdot\\delta_2^{(2)} + w_{3,1}^{(2)}\\cdot\\delta_3^{(2)} + w_{4,1}^{(2)}\\cdot\\delta_4^{(2)})*f\u0027(z_{1}^{(1)}) \\\\[0.5em]\n(w_{1,2}^{(2)}\\cdot\\delta_1^{(2)} + w_{2,2}^{(2)}\\cdot\\delta_2^{(2)} + w_{3,2}^{(2)}\\cdot\\delta_3^{(2)} + w_{4,2}^{(2)}\\cdot\\delta_4^{(2)})*f\u0027(z_{2}^{(1)}) \\\\[0.5em]\n\\end{array} \\right) = \n\\left( \\begin{array}{cccc}\n\\delta_{1}^{(1)} \\\\[0.5em]\n\\delta_{2}^{(1)} \\\\[0.5em]\n\\end{array}\\right)\n$$\n\nWe are now here in our neural network:\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=1,\n    special_inter_layer_arrow_offset=\"layer\",\n    bias=True\n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "This means that these are the last set of weights we have to compute!"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Step 2: Compute $\\nabla_{W^{(1)}}$ and $\\nabla_{b^{(1)}}$\nOur update rule for this layer is $\\nabla_{W^{(1)}}=\\delta^{(1)} a^{(0)T}$ and for bias $\\nabla_{b^{(1)}}=\\delta^{(1)}$. Notice that $a^{(0)}$ corresponds to the input layer. Because they\u0027re just the inputs, and neural networks do not apply activations on inputs before going through the network. This means that\n$$a^{(0)} = x = \n\\left( \\begin{array}{cccc}\nx_{1} \\\\[0.5em]\nx_{2} \\\\[0.5em]\n\\end{array}\\right)$$\n\nthen our new weight gradient\n\n$$\n\\Large\\nabla_{W^{(1)}}\\normalsize = \n\\left( \\begin{array}{cccc}\n\\delta_1^{(1)}\\\\[0.5em]\n\\delta_2^{(1)}\\\\[0.5em]\n\\end{array}\\right)\n\\left( \\begin{array}{cccc}\nx_{1} \\\\[0.5em]\nx_{2} \\\\[0.5em]\n\\end{array}\\right)^T = \n$$\n$$\n\\left( \\begin{array}{cccc}\n\\delta_1^{(1)}\\\\[0.5em]\n\\delta_2^{(1)}\\\\[0.5em]\n\\end{array}\\right)\n\\left( \\begin{array}{cccc}\nx_{1} \u0026 x_{2}\\\\[0.5em]\n\\end{array}\\right) = \n\\left( \\begin{array}{cccc}\n\\delta_1^{(1)}x_1 \u0026\\delta_1^{(1)}x_2\\\\[0.5em]\n\\delta_2^{(1)}x_1 \u0026\\delta_2^{(1)}x_2 \\\\[0.5em]\n\\end{array}\\right)\n$$\n\nour new bias gradient\n$$\\Large \\nabla_{b^{(1)}}=\\delta^{(1)}\\normalsize = \\left( \\begin{array}{cccc}\n\\delta_1^{(1)}\\\\[0.5em]\n\\delta_2^{(1)}\\\\[0.5em]\n\\end{array}\\right)$$ \n\n#### Step 3: We perform our final gradient for the sample\n\nfor weights:\n$$\n\\Large W^{(1)} \\normalsize  =\\left( \\begin{array}{cc}\nw_{1,1}^{(1)} \u0026 w_{1,2}^{(1)} \\\\[0.5em]\nw_{2,1}^{(1)} \u0026 w_{2,2}^{(1)} \\\\[0.5em]\n\\end{array}\\right)\n$$\n$$\n\\Large \\nabla{W^{(1)}} \\normalsize = \\left( \\begin{array}{cccc}\n\\delta_1^{(1)}x_1 \u0026\\delta_1^{(1)}x_2\\\\[0.5em]\n\\delta_2^{(1)}x_1 \u0026\\delta_2^{(1)}x_2 \\\\[0.5em]\n\\end{array}\\right)\n$$\nfor biases\n$$\n\\Large b^{(1)} \\normalsize  = \\left( \\begin{array}{cc}\nb_{1}^{(1)}\\\\[0.5em]\nb_{2}^{(1)}\\\\[0.5em]\n\\end{array}\\right)\n$$\n$$\n\\Large \\nabla_{b^{(1)}}=\\delta^{(1)}\\normalsize = \\left( \\begin{array}{cccc}\n\\delta_1^{(1)}\\\\[0.5em]\n\\delta_2^{(1)}\\\\[0.5em]\n\\end{array}\\right)\n$$\n\nGradient Descent:\n\n$$\nW_{new}^{(1)} = \\left( \\begin{array}{cc}\nw_{1,1}^{(1)} \u0026 w_{1,2}^{(1)} \\\\[0.5em]\nw_{2,1}^{(1)} \u0026 w_{2,2}^{(1)} \\\\[0.5em]\n\\end{array}\\right) - \\eta\\left( \\begin{array}{cccc}\n\\delta_1^{(1)}x_1 \u0026\\delta_1^{(1)}x_2\\\\[0.5em]\n\\delta_2^{(1)}x_1 \u0026\\delta_2^{(1)}x_2 \\\\[0.5em]\n\\end{array}\\right)\n$$\n\n$$\nb_{new}^{(1)} = \n\\left( \\begin{array}{cc}\nb_{1}^{(1)}\\\\[0.5em]\nb_{2}^{(1)}\\\\[0.5em]\n\\end{array}\\right) - \\eta\\left( \\begin{array}{cccc}\n\\delta_1^{(1)}\\\\[0.5em]\n\\delta_2^{(1)}\\\\[0.5em]\n\\end{array}\\right)\n$$\n\nWe have now finished backpropagation:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=0,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Closing Remarks on Backpropagation: Tips and tricks\n\n#### For regression\nIf you want to do the same with regression, your last weight layer will simply be a 4x1 matrix instead, with no activation. This means that your loss $\\delta^{(3)}$ would simply be 1 value, $\\hat{y}-\\bar{y}$, which makes sense because you would only have 1 node, essentially a 1x1 matrix. Make sure you line up the matrices perfectly for this case. I highly recommend trying these problems out with concrete numbers, as I will only go over theory here. Feed problems into ChatGPT, and ask for small regression or classification networks with 1 or 2 hidden layers to feed forward and backpropagate through\n\n#### A caveat for ReLU\n\nReLU is a weird function in such that it is \"technically\" non-differentiable. While however, with coding neural networks, we defined ReLU and its derivative to be simply 0 at the cusp, so that we maintain some sort of differentiability. We can do this by exploiting if else statements, like that of ```np.where()``` in the numpy package.\n\n\n\n#### A common mistake\nPeople generally tend to mix up the activation function and its derivative. Remember that for forward pass, we use the activation function, and backwards pass, when computing the loss we use the derivative of the activation function. This might be easier to understand if we visualize it. Let\u0027s come up with some synthetic values for our previous nueral network for the following layer:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=2,\n    special_inter_layer_arrow_offset=\"layer\",\n    bias=True\n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Recall ReLU:\n$$\n\\operatorname{ReLU}(x) = \\begin{cases}\nx, \u0026 \\text{if } x \\geq 0, \\\\\n0, \u0026 \\text{if } x \u003c 0.\n\\end{cases}\n$$\n$$\n\\operatorname{ReLU\u0027}(x) = \\begin{cases}\n1, \u0026 \\text{if } x \\geq 0, \\\\\n0, \u0026 \\text{if } x \u003c 0.\n\\end{cases}\n$$\n\nlets assume we calculated $z^{(2)}$ to be:\n\n$$\nz^{(2)} = \n\\left( \\begin{array}{cc}\n1.35\\\\[0.5em]\n-4.30\\\\[0.5em]\n6.78\\\\[0.5em]\n-.32\\\\[0.5em]\n\\end{array}\\right)\n$$\nthen  $a^{(1)} =  f(z^{(2)}) = $\n$$\n\\left( \\begin{array}{cc}\n1.35\\\\[0.5em]\n0\\\\[0.5em]\n6.78\\\\[0.5em]\n0\\\\[0.5em]\n\\end{array}\\right)\n$$\n\n__**It is extremely important to understand that $a^{(1)} \\neq f\u0027(z^{(1)})$. Remember that $a^{(1)} = f(z^{(1)})$. You can not use the derivative of $z^{(1)}$ to compute $z^{(1)}$. This is another extremely common mistake**__ This is how they are different:\n\n$$\n\\text{ where } z^{(2)} = \n\\left( \\begin{array}{cc}\n1.35\\\\[0.5em]\n-4.30\\\\[0.5em]\n6.78\\\\[0.5em]\n-.32\\\\[0.5em]\n\\end{array}\\right)\n$$\n\n\nour forward pass (activated values for nodes 1, 2, 3, and 4 would be as such)\n$$\nf(z^{(2)}) = \\left( \\begin{array}{cc}\n1.35\\\\[0.5em]\n0\\\\[0.5em]\n6.78\\\\[0.5em]\n0\\\\[0.5em]\n\\end{array}\\right)\n$$\n\nNotice the difference:\n$$\nf\u0027(z^{(2)}) = \\left( \\begin{array}{cc}\n1\\\\[0.5em]\n0\\\\[0.5em]\n1\\\\[0.5em]\n0\\\\[0.5em]\n\\end{array}\\right)\n$$\n\nLet\u0027s look at our backpropagation example. with $z^{(2)}$ This means we would be located here in our network:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2, 4, 3,3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_target_layer=2,\n    special_inter_layer_arrow_offset=\"node\",\n    bias=True\n)\n\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Keep this in mind. You use $f(z^{(2)})$ in forward pass and $f\u0027(z^{(2)})$ in backpropagation. This goes for all of the other differentiable activations too. When we compute the hadamard product when doing our backpropagation example, you see how this becomes\n\n\n$$\\Large \\delta^{(2)}\\normalsize =\n\\left(\\begin{array}{cccc}\n(w_{1,1}^{(3)}\\delta_1^{(3)} + w_{2,1}^{(3)}\\delta_2^{(3)} + w_{3,1}^{(3)}\\delta_3^{(3)})*f\u0027(z_{1}^{(2)})\\\\[0.5em]\n(w_{1,2}^{(3)}\\delta_1^{(3)} + w_{2,2}^{(3)}\\delta_2^{(3)} + w_{3,2}^{(3)}\\delta_3^{(3)})*f\u0027(z_{2}^{(2)}) \\\\[0.5em]\n(w_{1,3}^{(3)}\\delta_1^{(3)} + w_{2,3}^{(3)}\\delta_2^{(3)} + w_{3,3}^{(3)}\\delta_3^{(3)})*f\u0027(z_{3}^{(2)})\\\\[0.5em]\n(w_{1,4}^{(3)}\\delta_1^{(3)} + w_{2,4}^{(3)}\\delta_2^{(3)} + w_{3,4}^{(3)}\\delta_3^{(3)})*f\u0027(z_{4}^{(2)}) \\\\[0.5em]\n\\end{array}\\right) = \n$$\n$$\n\\left(\\begin{array}{cccc}\n(w_{1,1}^{(3)}\\delta_1^{(3)} + w_{2,1}^{(3)}\\delta_2^{(3)} + w_{3,1}^{(3)}\\delta_3^{(3)})*1\\\\[0.5em]\n(w_{1,2}^{(3)}\\delta_1^{(3)} + w_{2,2}^{(3)}\\delta_2^{(3)} + w_{3,2}^{(3)}\\delta_3^{(3)})*0 \\\\[0.5em]\n(w_{1,3}^{(3)}\\delta_1^{(3)} + w_{2,3}^{(3)}\\delta_2^{(3)} + w_{3,3}^{(3)}\\delta_3^{(3)})*1\\\\[0.5em]\n(w_{1,4}^{(3)}\\delta_1^{(3)} + w_{2,4}^{(3)}\\delta_2^{(3)} + w_{3,4}^{(3)}\\delta_3^{(3)})*0 \\\\[0.5em]\n\\end{array}\\right) $$\n\nFeel free to refer back here, as you may make this mistake in the future."}, {"cell_type": "markdown", "metadata": {}, "source": "## From Matrices to Tensors: The Need for Batches:\n---\n\nThe example that we had just performed forward pass and backpropagation on was for but one sample in our dataset. For the following, we can model our neural network again, to show you what I mean:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_vis = NeuralNetworkVisualization(batchSize=1, nodes_per_layer=[2,2,4,3,3],bias=True)"}, {"cell_type": "markdown", "metadata": {}, "source": "**Tip:** For this model, drag with your cursor to see a 3d view."}, {"cell_type": "markdown", "metadata": {}, "source": "You can see we have 2 inputs, and then 3 possible categories to choose from. This is all for one sample in a dataset that may contain millions. We essentially performed **stochastic gradient descent.** The intuition is that every single sample individually, then use that to step in the direction of the local minima. This is called stochastic gradient descent. Meanwhile, normal gradient descent is when you compute the gradient with to EVERY SINGLE SAMPLE AT THE SAME TIME, then averaging them. For a dataset with a million datapoints, and neural networks with 100+ nodes per layer, you see how inefficient this becomes (you\u0027d brick your computer!). \n\nHowever, something to note is that every sample might deviate a little bit from its idealistic prediction. This introduces noise when performing gradient descent, and as a result we do not travel directly into the local minima. Let\u0027s look at the following example:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sim = GradientDescentSimulator(learning_rate=10, title=\"Stochastic Gradient Descent\",max_iter=20,noise_level=.05,random_seed=99)\nsim.fig.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "You see the issue here? There is a lot of noise when we are trying to compute the gradient, since one sample, while it can roughly approximate the gradient for every batch, will add noise to every step. We then \"deviate a little bit\", so that we can never reach the bottom of the local minima. So how do we solve this problem?\n### Solution: batched gradient descent (hyperparameter 2)\n\nLet\u0027s calculate maybe only 10 samples at the same time, average them, and then update the weights accordingly. We can build a neural network that looks like this:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_vis = NeuralNetworkVisualization(batchSize=10, nodes_per_layer=[2,2,4,3,3],bias=True)"}, {"cell_type": "markdown", "metadata": {}, "source": "You see how now, we can input 10 samples of our 2 feature dataset at a time? We then average them, and then compute the gradients with respect to the batch. We essentially have 10 neural networks running in parallel, which we will call a \"slice\" for the purpose of this demonstration.\n\nLet\u0027s let $W$ denote all of the weights and biases for every layer in a neural network \"slice\".\n\n **Here, the values for $W$ will be the same for every single slice**. However, when we compute the gradients $\\nabla W$, notice that all of the samples are different across slices, and so each \"slice\" will have a different gradient for their layers of weights and biases. So across all of the batches, **we average $\\nabla W$**, so that we only have one set of weights for our batched neural network. We use this as our new gradient to update the weights with respect to the first layer. Then, these new weights for the first layer are all used to \"update\" the weights for all the other layers, so that the weights and biases for every neural network slice match again. Look what happens when we do this:\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sim = GradientDescentSimulator(learning_rate=10, title=\"Batched Gradient Descent\",max_iter=100,noise_level=.01,tolerance=.1,random_seed=99)\nsim.fig.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We still have a little noise, but the convergence is much better, and we can process our dataset much quicker. In practice, these \"tensors\" are how computers actually operate neural networks. Whenever you talk about specifying a batch size, this is generally what you are doing. Please note that each dataset might need different batch sizes based on their size. For example a dataset of 100 samples may not need batches, while a dataset of 1 million samples definitely will. Play around with it and see what gives you the best convergence for your dataset."}, {"cell_type": "markdown", "metadata": {}, "source": "## Introduction to Different Optimizers (hyperparameter 4)\n---\n\nYou have seen how we can use gradient descent in order to take steps into the local minima. Gradient Descent is a type of **optimizer**.\n\nHowever, it turns out that neural networks spaces, with their multiple layers and their non-linear activation functions end up perverting the objective loss function such that it is no longer just a nice bowl, like we have seen in the previous examples. It ends up looking more like a ruggedy terrain, with lots of little dips and mountains. As a result what ends up happening is that our ball will end up getting \"stuck\" in a local minima, and never achieve better generalization. Let\u0027s look what happens in a scenario like this:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sim_multi = MultiMinimaSimulator(learning_rate=1, title=\"Learning Rate that is optimized for the parameter space\",camera_distance=1,azim=135,elev=25)\nsim_multi.fig.show()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "As you can see, our ball travels down into one of the two dips, but you can see that the other one is a lot deeper. Having our ball go down there, we would get a model that can effectively generalize better. So naively we can try turning up the learning rate and see if we can \"jump\" into the better minima:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sim_multi = MultiMinimaSimulator(learning_rate=5, title=\"Learning Rate that is too High for a Non-convex Space\",camera_distance=1,azim=35,elev=25)\nsim_multi.fig.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We can see that we do jump over the first minima, but the sides of the objective function are much steeper. As a result the gradient tells us to \"throw the ball really hard,\" pushing us back out of the local minima dip. We then just jump around the minimas and never converge. So how do we combat this?\n\n### Different optimizers to combat this issue\n\n#### An important Caveat \nBefore going on to cover these algorithms, all of these gradient and learning rate updates are with respect to the iterations of each individual layer\u0027s gradients. This means that each new hyperparameter we add that is not $\\eta$ or the gradient $\\nabla W$ will also have the superscript $(\\ell)$, denoting it\u0027s respect to its gradient. This is because all layers\u0027 gradients are seperate with respect to each other (note this is different from being independent). All of these optimizers are recursive in nature, and work on there previous individual iterations. This means we will update for as many batches that we have to divide out datset. The optimizer methods we are about to cover update with respect to these batches. The gradients are averaged per batch, and we use this averaged gradient as our update rule. **This is called layer-wise dependency**\n\n#### AdaGrad\n\nThere have been many algorithms to combat this issue, and help converge at a better solution. The first of these is AdaGrad. Do you remember how we had to tune the learning rate ourselves? Well what if we made an algorithm that also helped tune the learning rate by accumulating these gradients? This is what AdaGrad proposes, and it\u0027s following algorithm can be modeled as such:\n\n$$W_{new} = W - \\frac{\\eta}{\\sqrt{G_t}+\\epsilon}g_t$$\n\nwhere\n - $g_t$ is the current gradient at the current iteration $t$, equivalent to $\\nabla W$, representing the gradient for every individual layer. Note that this is not additive; it is more like a set.\n - $G_t$ is the sum of the sqare of all previous gradients, or $\\large \\sum_{i=1}^t g_i^2$.  For a loop, you can think of this as $G = G + (g_t)^2$\n - t is batch t out of how ever many batches it takes to subdivide the whole dataset\n - $\\epsilon$ is usually some super small constant (usually $1*10^{-8}$) used to ensure no divide by 0 error\n - $\\eta$ is the learning rate\n - $W_{new}$ is the new weights for all the layers of the matrix\n - $W$ is the weights for all the layers of the matrix\n\n**Note:** This means that for our update rule, for every iteration, you \"save\" $g_t^{(\\ell)}$\u0027s weights and accumulate the square of them for $G_t^{(\\ell)}$. You will therefore update $G_t^{(\\ell)}$ recursively, usually via a loop.\n\nWhat this essentially does is that for gradients (layers in our case) with high gradients and frequent big updates, we slow down in those directions, while maintaining speed for our sparser gradient updates, in the directions where the objective function isn\u0027t as steep. With this we solve the problem of the averages of the gradients not pushing us enough in some directions while pushin us too much in others, leading to poorer convergence in normal or stochastic gradient descent.\nBy keeping track of the square of the gradients for each layer, we can kind of \"adapt\" the learning rate depending on the slope. As an analogy, imagine you\u0027re in a car, driving on a city road with a lot of bumps and potholes. With AdaGrad, your car adjusts its speed by taking into account every pothole you\u2019ve encountered from the start of your trip.\n\nThis way, we slowly start to \"slow down\" as we encounter minima. We start with a fast initial learning rate, but then it slowly diminishes, coming to zero. Going back to our analogy, over time, because you remember all the rough spots, your speed gets dialed down too much; even in areas where the road is relatively smooth\u2014making your journey unnecessarily slow. This means you may just end up coming to a stop completely before reaching a good minimum. \n\n#### RMSProp\n\nRMSProp aims to solve this issue where the gradients diminish too quickly by taking the moving average of the adaptive learning rates. This means that our older gradients decay, and our newer gradients will contribute more to the moving average, ensuring that the update reflects more recent information. The following algorithm looks as such: \n\n$$v_t = \\beta v_{t-1} + (1-\\beta)g_t^2$$\n$$W_{new} = W - \\frac{\\eta}{\\sqrt{v_t}+\\epsilon}g_t$$\n$$v_0 = 0$$\n\nwhere\n - $\\beta$ is the decay rate. It is usually set at .9\n - $v_t$ is the moving average of squared gradients at batch t. \n - $g_t$ is the current gradient at the current iteration $t$, equivalent to $\\nabla W$, representing the gradient for every individual layer. Note that this is not additive; it is more like a set, so you would have seperate $v_t$\u0027s for each layer, or a $v_t^{(\\ell)}$\n - t is batch t out of how ever many batches it takes to subdivide the whole dataset\n - $\\epsilon$ is usually some super small constant (usually $1*10^{-8}$) used to ensure no divide by 0 error\n - $\\eta$ is the learning rate\n - $W_{new}$ is the new weights for all the layers of the matrix\n - $W$ is the weights for all the layers of the matrix\n\n**Note:** This means that for our update rule, for every iteration, you \"save\" $v_t^{(\\ell)}$ as it\u0027s own seperate entity per layer of weights. $v_t{(\\ell)}$ will have the same dimensions as your weight matrix. You use the update rule above to get $v_t{(\\ell)}$ for the next batch.\n\nThis solves the problem where the accumulated gradients were slowing down the optimization well before it reached it local minima. As for the continuation of our analogy:\n\nWith RMSProp, your car only pays attention to the potholes you\u2019ve seen in the recent part of your drive. This means your speed is adjusted based on the current road conditions rather than being weighed down by every past bump. You\u2019re still cautious, but not overly so, and you can keep a steadier pace overall.\n\nWhile RMSProp is extremely powerful, it still has the potential to get stuck in local minima, because the squared gradients peter out when we reach some local minima. This means we will converge in the local minima, while there may be a better minima over the horizon. ie. We can still get stuck in potholes before we get to our destination! Also notice that for $v_0$, we essentially get that our adaptive learning rate needs to \"adjust\" from 0 in order to get the best moving average for the scenario. This is equivalent to the need to start up your car. Time is precious, and you don\u0027t want to waste it!\n\n#### Adam\n\nAdam aims to solve RMSProp\u0027s convergence in smaller local minimums by adding first order momentum to the algorithm, and also a bias correction, so we dont need to \"start up our car,\" as I had put it previously. Adam updates can be defined with the following formulas:\n\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2$$\n$$\\text{Bias Correction: } \\hat{m_t} = \\frac{m_t}{(1-\\beta_1^t)} \\text {. The superscript t represents an exponent per iteration}$$\n$$\\text{Bias Correction: } \\hat{v_t} = \\frac{v_t}{(1-\\beta_2^t)} \\text {. The superscript t represents an exponent per iteration}$$\n$$W_{new} = W - \\eta\\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}}+\\epsilon}g_t$$\n$$m_0 = 0$$\n$$v_0 = 0$$\n\n**Note:** This means that for our update rule, for every iteration, you \"save\" $v_t^{(\\ell)}$ and $m_t^{(\\ell)}$ as it\u0027s own seperate entity per layer of weights. $v_t{(\\ell)}$ and $m_t^{(\\ell)}$ will have the same dimensions as your weight matrix. You use the update rule above to get $v_t{(\\ell)}$ and $m_t^{(\\ell)}$, apply the bias correction seperately, then use the uncorrected $v_t{(\\ell)}$ and $m_t^{(\\ell)}$ for the next batch. **This was a common source of confusion for me when coding adam myself. When you are updating the first and second moments, you should only bias correct the value right before performing a step of gradient descent. The m and v should stay seperate from this bias correction when you recursively update them.**\n\nwhere\n - $\\beta_1$ is the decay rate for momentum. It is usually set at .9\n - $\\beta_2$ is the decay rate for squared gradient moving average (2nd moment). It is usually set at .999\n - $v_t$ is the moving average of squared gradients at batch t. \n - $m_t$ is the momentum of our function.\n - $g_t$ is the current gradient at the current iteration $t$, equivalent to $\\nabla W$, representing the gradient for every individual layer. Note that this is not additive; it is more like a set, so you would have seperate $v_t$\u0027s and $m_t$\u0027s for each layer, or a $v_t^{(\\ell)}$ and $m_t^{(\\ell)}$\n - t is batch t out of how ever many batches it takes to subdivide the whole dataset\n - $\\epsilon$ is usually some super small constant (usually $1*10^{-8}$) used to ensure no divide by 0 error\n - $\\eta$ is the learning rate\n - $W_{new}$ is the new weights for all the layers of the matrix\n - $W$ is the weights for all the layers of the matrix\n\nWith this newly added \"momentum,\" we essentially press our accelerator in these \"potholes,\" and this gives us enough power to drive out of them, all the way up until we reach our desired destination! While thinking of these analagies, look at the simulation below:\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sim_adaptive = MultiMinimaSimulatorAdaptive(\n    learning_rate=0.7,\n    title=\"Optimizer Comparisons\",\n    azim=50,  \n    elev=35,   \n    r_cam=1  \n)\n\nsim_adaptive.fig.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "You can clearly see why I opted to use Adam in my neural network implementation. However, I do not want you to discourage you from using any of these, as all optimization problems will be different, and you should choose the best optimizer that suits your specific problem!"}, {"cell_type": "markdown", "metadata": {}, "source": "#### AdamW (hyperparameter 5)\nIf you have seen logistic regression, you are probably familiar with the term regularization. There are many different types of regularization, such as $\\ell_1$ (LASSO) regularization, which uses the absolute value of the weights to create a sparser solution, or more commonly $\\ell_2$ (Ridge) Regression, which is the square of the weights used for regularization. Notice there are many other types of regularization such as ElasticNet, or even regularizations with powers $p\u003c1$. The issue with these regularization techniques, however, is that the gradients are updated adaptively, meaning the steps we take will change over time, and will not directly correlate to the size of the weights in any given layer. This means that some layers can have growing gradients, some can have shrinking gradients, and some may have very sparse gradients (matrices with a lot of 0\u0027s), and as a result, we get inconsistent regularization. This is where AdamW comes in. AdamW can be defined as\n$$W_{new} = W - \\eta(\\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}}+\\epsilon}g_t + \\lambda W)$$\n\nwhere are new term\n - $\\lambda$ is the hyperparameter specified by the client for how strong they want their decay\n\nAs opposed to $\\ell_2$ regularization:\n$$W_{new} = W - \\eta\\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}}+\\epsilon}g_t$$\n$$g_t = \\nabla W + \\lambda W$$\nYou can see that $\\ell_2$ regularization is encapsulated inside the gradient, which can pose the issue of this inconistent scaling, since the adaptive learning will distort the $\\ell_2$ example. What adamW (the first example) does is it \"decouples\" the regularization, so that way the regularization scales independently of the adaptive learning rates and momentum calculations, and the regularization becomes consistent. **This is why if you are performing any sort of norm regularization, you should use AdamW to ensure consistent regularization.**\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## Other Regularization Techniques\n---\n\nWe have covered 1 regularization technique. However, it is important to have multiple different types of regularization techniques, because each technique may help with certain aspects of your model overfitting. For example, we don\u0027t want any couple of nodes predicting using only one feature for its predictions, so we apply weight regularization, so that each feature and weight has more opportunity to learn, and therefore increase generalizeability. Our second types of regularization comes in the form of dropout:\n\n### Dropout (hyperparameter 6):\nThe idea behind this is that we randomly drop some nodes in a layer per batch so that our neural network can not rely on any one given node, which should increase generalizability. While we are still not sure how it works, it still proves to be an extremely effective regularization technique. Here is an example picture: \n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "display(Image(filename=\u0027Visualizers/dropout.png\u0027))"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Applying dropout\nOur dropouts can be defined by \n$$X \\sim Binomial(n,(1-p))$$\n\nwhere \n - p is the probability specified by the client\n - n is the number of nodes in the layer\n\nWhat we do is we essentially create a mask, or some random probability distribution of 0\u0027s depending on the specified probability. Let\u0027s revisit an old example, where the arrow is pointing to a layer we want to apply dropout to:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2,4, 3, 3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_offset=\"node\",\n    special_inter_layer_arrow_target_layer=2\n)\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Then n = 4, and let\u0027s assume we have a dropout of .2. We generate a list of size n with 80$ of generating a 1 and 20% chance of generating a zero. We could hypothetically generate something like this:\n$$\n\\left( \\begin{array}{ccc}\n1\\\\[0.5em]\n0 \\\\[0.5em]\n1\\\\[0.5em]\n1\n\\end{array}\\right)\n$$\n\nNote that this could be different every single time.\n\nThen, in order to balance out our lost weights, we multiply our \"mask\" by $\\frac{1}{(1-p)}$:\n$$\n\\frac{1}{(1-.2)}\n\\left( \\begin{array}{ccc}\n1\\\\[0.5em]\n0 \\\\[0.5em]\n1\\\\[0.5em]\n1\n\\end{array}\\right) = \n\\left( \\begin{array}{ccc}\n1.25\\\\[0.5em]\n0 \\\\[0.5em]\n1.25\\\\[0.5em]\n1.25\n\\end{array}\\right)\n$$\nThen our layer with dropout would be:\n$$\\left( \\begin{array}{ccc}\n1.25*w_{1,1}^{(3)} \u0026 1.25*w_{2,1}^{(3)} \u0026 1.25*w_{3,1}^{(3)} \\\\[0.5em]\n0 \u0026 0\u0026 0\\\\[0.5em]\n1.25*w_{1,3}^{(3)} \u0026 1.25*w_{2,3}^{(3)} \u0026 1.25*w_{3,3}^{(3)} \\\\[0.5em]\n1.25*w_{1,4}^{(3)} \u0026 1.25*w_{2,4}^{(3)} \u00261.25* w_{3,4}^{(3)}\n\\end{array}\\right)$$\n\nThis would last up until we compute everything in the batch and backpropagate correctly. Then we would do the same thing for the next batch, but notice the placement of 0\u0027s and 1\u0027s might be different, because they are picked randomly from a binomial distribution.\n\n#### Caveat: What if the dropout drops all nodes?\nWhile you may be thinking of this, it can be a good edge case to handle. However, this is an extremely rare case, because you would eseentially have to multiply (1-dropout rate) * number of nodes, which for large layers would give next to 0 probability. However, in the rare event you get a mask with all 0\u0027s you can just flip the top one to 1, since this is so statistically improbabable it will most likely not hurt your generalization. You can pick a random 1 to flip, but note that any computational expense added in backpropagation, training, or anything with loops will blow up time complexity."}, {"cell_type": "markdown", "metadata": {}, "source": "### Input Dropout (Hyperparameter 7)\n\nLikewise, we can apply the same thing to the input layer, that way our neural network can\u0027t ever rely on any one such feature."}, {"cell_type": "markdown", "metadata": {}, "source": "### Closing Notes: Weight Initializations\n\nIf you are building a neural network yourself, I will save you a HUGE headache. This took me 9 hours to figure out when coding my weights and trying to figure out why my models were never converging.\n\nIn theory, weights in all of the weight matrices can be initialized randomly, but in practice, this is actually horrible. If we do not standardize our weights, then our model might contain some sort of symmetry, or have some sort of pattern in certain parts of the weight matrices that can make gradients explode or vanish. For example, if we have a weight and it\u0027s the first layer, that means we chain together the derivatives of all the other layers, all of these chained derivatives can make our value explode, or vanish, if our initial values isn\u0027t set up yet.\n\n#### Common Initialization tecnhiques\n\nThe method I used is called He initialization which utilizes a normal distribution to initialize the weights. It\u0027s most popular for ReLU, but works fairly well for sigmoid and tanh as well. Here is the following formula:\n\n$$N \\sim(0, \\frac{2}{\\text{nodes of the previous layer}} )$$\n\nThe formula for for Xavier/Gloat Initialization is very similar. This works well for tanh and sigmoid:\n$$N \\sim(0, \\frac{1}{\\text{nodes of the previous layer}} )$$\n\nFor biases, it is common to just set these to 0. This is done because the model should \"adjust the height\" with every backpropagation step.\nGiven the picture below:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "nn_plot = NeuralNetworkPlot(\n    nodes_per_layer=[2, 2,1, 3, 3],\n    label_arrows=False,\n    arrow_label_overrides={},\n    special_inter_layer_arrow_color=\"blue\",\n    special_inter_layer_arrow_offset=\"node\",\n    special_inter_layer_arrow_target_layer=2\n)\n# Display the plot.\nnn_plot.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "our nodes of the previous layer for every single node will be 2!\nThat means we would pick a value from that distribution that we described, and make that the weight of our node.\n\n**WARNING!!!!!!!!!!!!!!!!! DO NOT FORGET THIS. I ALMOST WENT INSANE TRYING TO FIX THIS.**\n\nUnless you want to spend 9 hours debugging :))))))))))))))))))))))))))))))))))))))))))"}, {"cell_type": "markdown", "metadata": {}, "source": "## Conclusion\n---\nThs is the end of my document. We went over the history and reasons behind using neural networks, the forward and backwards pass, optimizers, hyperparameters, gradient descent, batches, and regularization techniques. You should be ready with all of this information to build your own neural network from scratch! Feel free to use this document as a reference for building your own neural network! I will have the listed derivaations from earlier in the document below this section for anyone who is interested in the math and derivations behind the loss, and step-by-step breakdowns on the derivatives of the activation and loss functions, as well as any other fun facts. Thank you so much for reading!"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.9"}}, "nbformat": 4, "nbformat_minor": 2}, "settingsUrl": "../../build/schemas", "themesUrl": "./build/themes"}
    </script>

    

    <div cell-index="1">
          <script>
            voila_process(1, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="2">
          <script>
            voila_process(2, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h1 id="A-Comprehensive-Breakdown-of-Neural-Networks-with-Reasoning-Behind-My-Own-Design-Choices-for-my-Neural-Network"><strong>A Comprehensive Breakdown of Neural Networks with Reasoning Behind My Own Design Choices for my Neural Network</strong><a class="anchor-link" href="#A-Comprehensive-Breakdown-of-Neural-Networks-with-Reasoning-Behind-My-Own-Design-Choices-for-my-Neural-Network">&#182;</a></h1><p>Author: Patrick Erickson</p>
<hr />
<h2 id="Abstract">Abstract<a class="anchor-link" href="#Abstract">&#182;</a></h2><hr />
<p>Neural Networks have been gaining a lot of traction in the world as of recent, with the rise of giant models such as Chat-GPT and other LLMs. While the complexity of these giant AI models can not be explained by neural networks alone, they make up a core part of what makes them function. Furthermore, there are thousands of other types of models that use neural networks in their architecture to help them make predictions. In this document, I will be detailing the reason and intuition behind neural networks, the math behind them, including how to forward pass, activation functions, regularization techniques, and backpropagation (if you don't know what these mean, don't worry. We'll cover it!), and more complex stuff such as optimization functions. Lastly, I will go over an in-depth analysis on my own Neural Network's architecture so you can see my own intuitions behind the construction of one, from scratch without the use of popular libraries such as TensorFlow, PyTorch, and JAX. This article does assume, however, that you have a basic understanding of classification and regression, derivatives, and the derivative chain rule. I also expect that you understand how matrix multiplication, addition, and subtraction works. For the scope of this document, we will not be delving into Convolutional Heads or Time Series. I will also not be delving into Maximum Likelihood Estimators, but you should have a basic grasp of at least some probability distributions, such as gaussian and binomial.</p>
<h3 id="A-Brief-History-of-Neural-Networks:-The-Perceptron">A Brief History of Neural Networks: The Perceptron<a class="anchor-link" href="#A-Brief-History-of-Neural-Networks:-The-Perceptron">&#182;</a></h3><p>The birth of the Neural Network had much humbler beginnings than the giant models we see today. <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a>, at the Cornell Aeronautical Laboratory, had constructed something called a single layer perceptron simulated on an IBM 704 in 1957. This &quot;perceptron&quot; was made to simulate an individual neuron within the human brain, which either fires or doesn't fire based on the input it's given.</p>
<p>For example, let's say we know the following about someone:</p>
<ul>
<li>Income</li>
<li>Properties Owned</li>
</ul>
<p>and we we want to predict if that person is rich or not rich (we will use 0 for not rich and 1 for rich). Our perceptron will look as such:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="3">
          <script>
            voila_process(3, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="4">
          <script>
            voila_process(4, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Then the intuition behind this is that there is some correlation between the sizes of the features and the final output of being rich or not. Fer example, the income and the number of properties owned aren't over a certain threshold, we would print 0, for being not rich. Likewise, if income and and property value aren over this threshold, we then predict 1 (rich). More specifically, the algorithm looks at all of the points it misclassifies and then performs an operation to move the hyperplane to fix that misclassification. This algorithm continues to do this until all points are satisfied. We can visualize how the perceptron works with the following simulation:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="5">
          <script>
            voila_process(5, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="6">
          <script>
            voila_process(6, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Limitations-of-the-Perceptron">Limitations of the Perceptron<a class="anchor-link" href="#Limitations-of-the-Perceptron">&#182;</a></h3><p>As you can see, we slowly &quot;fixed&quot; the classifying line until all the &quot;Rich&quot; points are on one side of the line while all of the &quot;Not Rich&quot; points are on the other side of the line. We can therefore &quot;linearly seperate&quot; the data to make our predictions, which is the whole basis on how perceptrons work. However, what if the data isn't linearly seperable? For example, there could be some people who are considered &quot;rich&quot; who may only have one property, but that one property is a skyscraper in Manhattan worth billions. Our perceptron doesn't have this information, and we may not necessarily know this information to be able to put it into the perceptron. This &quot;unexplained data&quot; may cause our dataset to look more like this simulation, and is more analagous to the real world:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="7">
          <script>
            voila_process(7, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="8">
          <script>
            voila_process(8, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>The perceptron will have trouble classifying it, and the algorithm will never stop (converge), since there will always be some extra &quot;movement&quot; that can be made to try and fix the errors in the loss function. This issue only gets worse for datasets where linear seperability is borderline impossible. However, we can see that the data is still approximately linearly seperable, which allows us to be able to use methods such as logistic regression and other machine learning techniques will do a pretty good job of predicting the overall overall dataset, especially in the case we just described. But what about for data that isn't even remotely seperable? Let's look at the following example:</p>
<h3 id="Further-Limitations-of-Perceptron-with-XOR">Further Limitations of Perceptron with XOR<a class="anchor-link" href="#Further-Limitations-of-Perceptron-with-XOR">&#182;</a></h3><p>Assume we want to predict whether a customer would buy a product based on whether or not they visit the online and physical store of a specific company to review a product. It can be said that if a customer does not visit the online or real store, then the customer will not buy the product, because they are not interested. Likewise, a customer will most likely not walk out with anything if they visit both the online and real store, because they are currently assessing their options and they will not make a hasty decision on buying the product. However, if a customer visits the online store but not the real store, the customer may be more likely to buy it because they may think it's cheap and convenient. Likewise, a customer might be more enticed to buy a product if they physically saw it at the real store, but did not think to compare online, since the enticing prospect of buying and immediately having it is really high. We can model this problem with the following table:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="9">
          <script>
            voila_process(9, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="10">
          <script>
            voila_process(10, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>This is an example of a classic XOR problem, and even logistic regression and other classification methods perform poorly on this kind of data without any explicit feature engineering. Our perceptron will also suffer the same fate. The following is a simulation of different datapoints based on the company predictions:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="11">
          <script>
            voila_process(11, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="12">
          <script>
            voila_process(12, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>As we expected, the perceptron performs extremely poorly. This phenomenon itself led to one of the biggest AI winters ever since the conception of the perceptron, slowing AI research to a near standstill for 20 years. However, this obviously does not pertain to today, as we have figured out a clever workaround for this issue. This, along with stochastic nature of the perceptron algorithm, did not specify loss efficiently, and would therefore oscillate with no real solution. In Layman's terms, the perceptron had no &quot;confidence&quot; in its decisions, based on the distance a point was from the classifying boundary.</p>
<h2 id="Introduction-to-Neural-Networks">Introduction to Neural Networks<a class="anchor-link" href="#Introduction-to-Neural-Networks">&#182;</a></h2><hr />
<p>The Universal Approximation Theorem ultimately states that combining multiple neurons with some nonlinear activation can approximate any function, given enough of these neurons. This groundbreaking revelation in 1989 spurred the idea of stacking and aggregating inputs together into a single &quot;layer&quot; to effectively classify non-linear decision boundaries, such as the XOR problem described earlier. The introduction of the &quot;confidence method&quot; as an output mechanism further addressed the issue of perceptrons oscillating without convergence. This new approach, which focused on minimizing misclassification, required gradient descent optimization. Around the same period, a crucial algorithm called backpropagation was developed. Backpropagation efficiently computed chained derivatives through the network's hidden layers, enabling gradient descent to effectively tune perceptron weights, thereby minimizing confidence-based misclassifications, or in other words, reducing loss. Finally, breakthroughs in differentiable functions—known as activation functions—replaced the non-differentiable, step-based activation previously used by perceptrons (-1 for incorrect classification, +1 for correct). This shift to differentiable activations was the final essential component that allowed neural networks to be trained via gradient descent. Together, these advancements established the foundational framework of modern Artificial Neural Networks.</p>
<h3 id="How-Neural-Networks-Learn">How Neural Networks Learn<a class="anchor-link" href="#How-Neural-Networks-Learn">&#182;</a></h3><p>Neural Networks learn by creating predictions, then using that prediction and seeing how far it is away from the actualy associated value, often called the true label. These losses are aggregated, and then used to update the weights until the model can predict everything extremely well! Therefore, we break down neural network training into 3 problems:</p>
<ul>
<li>Forward pass</li>
<li>Minimizing an objective function through backpropagation</li>
<li>evaluating its effectiveness with new data</li>
</ul>
<h3 id="Types-of-activation-functions">Types of activation functions<a class="anchor-link" href="#Types-of-activation-functions">&#182;</a></h3><p>Some of the activation functions mentioned are:</p>
<ul>
<li>ReLU (Rectified Linear Unit), a function x that is 0 when $x\leq0$</li>
<li>$\tanh$, used to create some value in between -1 and 1</li>
<li>sigmoid, usually used to model some probability, giving a value of 0 to 1.</li>
</ul>
<p>They can be defined as the following, with their derivatives:</p>
<p>$$
\operatorname{ReLU}(x) = \begin{cases}
x, &amp; \text{if } x \geq 0, \\
0, &amp; \text{if } x &lt; 0.
\end{cases}
$$
$$
\operatorname{ReLU'}(x) = \begin{cases}
1, &amp; \text{if } x \geq 0, \\
0, &amp; \text{if } x &lt; 0.
\end{cases}
$$
$$
\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} 
$$
$$
\tanh'(x) = 1 - (\frac{e^x-e^{-x}}{e^x+e^{-x}})^2 = 1 - \tanh^2(x)
$$
$$
\operatorname{sigmoid}(x) = \frac{1}{1+e^{-x}} \text{. Note that this is commonly denoted as } \sigma(x)
$$
$$
\operatorname{sigmoid}'(x) = 1 - \frac{e^{-x}}{(1+e^{-x})^2} = (\frac{1}{1+e^{-x}})(1-\frac{1}{1+e^{-x}}) = \sigma(x)(1-\sigma(x))
$$</p>
<ul>
<li><strong>Note:</strong> For sigmoid, due to the nature of the loss function it is usually good practice to ensure that values never go past ($1*10^{-8},1-1*10^{-8}$). This is called clipping, and ensures your super confident features don't diverge to infinity.</li>
</ul>
<p>All of these can be used to replace the perceptron loss. <strong>However, do note that for stable convergence it is highly recommended that you only use one type of activation and it's respective derivative for your entire network. This guarantees better convergence, and I tested this. Mixing activations just suck, you can try it yourself with my own <a href="https://github.com/PatrickErickson4/FullyModularNumpyArtificialNeuralNetwork">custom-implemented neural network</a>.</strong> There are also many other types of activations, but for the sake of this demonstration, we will be working with these activations.</p>
<ul>
<li><strong>Fun Fact:</strong> Something interesting of note is that ReLU is piecewise, so you might be wondering how this is differentiable for backpropagation. Well, the secret lies in being able to see which features were activated on unactivated by relu. Since we define that if $x=0$, then the derivative will also be zero, we fix the nondifferentiability issue. Secondly, x will just be 1, which preserves information. Meanwhile the step function is either -1 or 1, with nothing with respect to x. This gives us 0 for the entire function, giving us no meaningful information for backpropagation.</li>
</ul>

</div>
</div>
</div>
</div>
        </div><div cell-index="13">
          <script>
            voila_process(13, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Loss-Functions">Loss Functions<a class="anchor-link" href="#Loss-Functions">&#182;</a></h3><p>There are also loss functions that assign some sort of non-discrete confidence of each prediction, in order to continue with the theme of differentiability for backpropagation. These are found at the end of the Neural Networks, commonly referred to as the &quot;head&quot;. It is common to use softmax for classification problems. Softmax assignes a probability of confidence to each of the different categories for our problem, which we can then pick the highest in order to classify our problem. For example, if we have 3 categories, those being good, neutral, or bad, the number 1 will be split across all 3 of these categories based on the confidence for each prediction. There is also Mean Squared Error (MSE) for regression, which is the exact same formula used in that of linear regression. The following loss functions have the respective formulas, derivatives, and a reported loss, generally shown to the user when training:</p>
<p><strong>Classification:</strong>
$$softmax \text{ for a single sample } p_i = \frac{e^{x_i}}{\sum_{j=1, i\in K}^Ke^{x_j}}. $$
In other words, the probability of every category for 1 sample in the dataset, and $p_i$ is analagous to the softmax function.
$$ \text{categorical cross entropy loss }= -\frac{1}{n}\sum_{i=1}^n \hat{y_i}\log(p_i)\text{. }$$
In other words, the amount the prediction deviated from the actual for the entire dataset.
$$softmax' \text{ for a single sample } = (p_i-\hat{y_i})\text{. }$$
In other words, how much we need to change our prediction by to fix the error.</p>
<p><strong>Regression:</strong>
$$MSE \text{ prediction for a single sample }= \hat{y_i}\text{. }$$
In other words, the square of how much one variable is on the y axis away from our predicted best fit line.
$$MSE \text{ loss }= -\frac{1}{n}\sum_{i=1}^n (\hat{y_i}-\bar{y_i})^2\text{. }$$
In other words, the amount the prediction deviated from the actual for the entire dataset.
$$MSE' \text{ for a single sample }= \hat{y_i}-\bar{y_i}\text{. }$$
In other words, how much we need to change our prediction by to fix the error.</p>
<p>the following is a key for all of the variables:</p>
<ul>
<li>$p_i$ is the softmax function</li>
<li>$\hat{y_i}$ is the true value that we are trying to predict for the label $i$ in our dataset.</li>
<li>$\bar{y_i}$ is the value we predicted with our model</li>
<li>$x_i$ are the values we get from the last layer.</li>
<li>$\frac{1}{n}\sum_{i=1}^n$ means the &quot;mean&quot; of the values.</li>
</ul>

</div>
</div>
</div>
</div>
        </div><div cell-index="14">
          <script>
            voila_process(14, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>For calculating the loss, it is much easier to think of it for every sample. We can calculate these then average the losses across all samples to get the middle loss value, which is generally shown to the client training a neural network. These equations are all based in fundamental statistics and the log likelihood to be able to find this total versus expected loss. Like the activation functions, there are also many different kinds of loss functions, especially for regression, but for the time being we will be sticking with these, as they are the most common. While you have most likely heard of mean squared error, you may not have heard of softmax. Let me explain it briefly:</p>
<h4 id="What-is-softmax?">What is softmax?<a class="anchor-link" href="#What-is-softmax?">&#182;</a></h4><p><strong>Softmax turns all of your categories into probabilities of being picked.</strong> Suppose you want to predict 10 different numbers, 0 through 9, from a dataset of images like the <a href="http://yann.lecun.com/exdb/mnist/">MNIST Handwritten Digits Dataset</a>. Let's say we want to predict the number 3. The idea is that the number produced for our model for the category of 3 will be a lot higher than the rest of the categories. What softmax does is it takes into account the values going into that specific category and divides it by the values of all the other categories to get a probability distribution for each category. If we look at the example, we can calculate the probability of us correctly picking 3 as such:
$$\frac{e^{x_3}}{e^{x_0}+e^{x_1}+e^{x_2}+e^{x_3}+e^{x_4}+e^{x_5}+e^{x_6}+e^{x_7}+e^{x_8}+e^{x_9}}$$
notice that this is the same as softmax we described as above, but expanded for the scope of our problem:
$$p_3 = \frac{e^{x_3}}{\sum_{i=0, 3\in \text{ the set of integers from 0-9 }}^9e^{x_i}}$$
In other words, $K$ represents all categories, and each x will be the corresponding specific value of the category, where the bigger number for a specific category yields a higher probability. If our model were correct, our probability distribution would look something like this.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="15">
          <script>
            voila_process(15, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="16">
          <script>
            voila_process(16, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p><strong>Notice how if we sum up the probabilities of all of the categories, we get 1.</strong></p>
<p>We can see that the model is &quot;this confident&quot; in predicting a 3. If certain probabilities are closer, that means our model is less confident in our prediction. Using this, we can classify many different categories. Modern architectures sometimes have thousands of these categories for image recognition, in order to accomplish things like object detection.</p>
<p>If instead all of these equations may look complicated, I promise it is as easy as just putting the function in the correct part of the formula. The derivations are all here for reference.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="17">
          <script>
            voila_process(17, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Application-of-Neural-Networks">Application of Neural Networks<a class="anchor-link" href="#Application-of-Neural-Networks">&#182;</a></h2><hr />
<p>Going back to our <a href="###Further-Limitations-of-Perceptron-with-XOR"><strong>original shopping problem</strong></a>,  assume the following features are going to the shop ($x_1$) in store or online or not ($x_2$). We are going to simulate the following neural network, in order to see how a perceptron with a softmax head will perform.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="18">
          <script>
            voila_process(18, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="19">
          <script>
            voila_process(19, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We can see that this is almost exactly like the perceptron, but now we have a softmax output. <strong>For the purpose of our example, note that there is an extra node that does not have any weights tied to it: it simply performs softmax</strong>. When you are ready, press play on the following simulation, and see how the decision boundary changes on the graph. Stop it when the graph no longer seems to change.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="20">
          <script>
            voila_process(20, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="21">
          <script>
            voila_process(21, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Based on the tensorflow playground, it is still impossible to linearly seperate the data. Let's try a different approach.  Using the Universal Approximation Theorem, we will construct the following neural network with 4 nodes in the hidden layer:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="22">
          <script>
            voila_process(22, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="23">
          <script>
            voila_process(23, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Let's see what happens when we try our simulation:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="24">
          <script>
            voila_process(24, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="25">
          <script>
            voila_process(25, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Notice how with four different linear combinators, we can create four different &quot;segments&quot; and finally solve the issue that has plagued AI researchers during the 70's and 80's.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="26">
          <script>
            voila_process(26, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="The-need-for-layers">The need for layers<a class="anchor-link" href="#The-need-for-layers">&#182;</a></h3><p>While the Universal Approximation theorem states that you can approximate any function $f(x)$ given the appropriate amount of nodes, this does not necessarily mean we should make  a single layer with this sufficient amount of nodes. By breaking each number of nodes up into seperate layers, we are able to compound on the features learned by each layer to arrived at a better gerneralization. We can see this motivation with the following architecture:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="27">
          <script>
            voila_process(27, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="28">
          <script>
            voila_process(28, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We can see this represented in Tensorflow. We will try to classify a spiral dataset:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="29">
          <script>
            voila_process(29, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="30">
          <script>
            voila_process(30, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>By the example, you can see the model struggle to correctly seperate the feature space. However, let's define a different architecture:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="31">
          <script>
            voila_process(31, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="32">
          <script>
            voila_process(32, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Let's see how this performs in the playground on the same dataset.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="33">
          <script>
            voila_process(33, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="34">
          <script>
            voila_process(34, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>You can see that the first layer learns bigger, more simple features, which are then fed into the second layer, where features are broken down and refined. As a result, we get a much better generalization than the 8 node example we had shown previously. This is the core idea behind deep learning: we can approximate almost any function, given enough layers and nodes. We have since learned how to numerically represent words and pictures, and we can therefore generalize even more complex ideas such as this. LLM's like ChatGPT and image generators such as DALL-E build off of this.</p>
<ul>
<li><strong>Fun Fact:</strong> There is a theory in statistics that states that the best models are the simplest, referred to as Ockham's Razor. Neural Networks are thought to have been unscalable because of this. However, do you see how some of these nodes you see how some the nodes aren't used at all? The idea behind sparsification: that a neural network only trains the nodes it needs to, and you can effectively prune these other nodes. The idea behind it therefore is that the more nodes you add, the more likely you are to find a solution that is really good at approximating something, because there are more routes that weights can travel through. Despite this, your model can still overfit if you have too many nodes, so keep that in mind.</li>
</ul>

</div>
</div>
</div>
</div>
        </div><div cell-index="35">
          <script>
            voila_process(35, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Thinking-in-Matrices:-The-Math-Behind-These-Networks">Thinking in Matrices: The Math Behind These Networks<a class="anchor-link" href="#Thinking-in-Matrices:-The-Math-Behind-These-Networks">&#182;</a></h2><hr />
<p>Knowing the intuition behind why these neural networks are used, we can finally delve into the math that makes them work. Based on all of the connections for the neural network, we can see that each weight, being fully connected, can form a matrix of values. Let's look at out previous example, focusing on the blue arrow highlighting the area of interest:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="36">
          <script>
            voila_process(36, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="37">
          <script>
            voila_process(37, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We can see that $x_1$ connects to all four nodes, as does $x_2$. This means that there will be 8 total arrows, which matches with our diagram. Each one of these white arrows for represent some weight to multiply our input from the previous layer by. The corresponding weight matrix can be represented as such:</p>
<p>$$
\left( \begin{array}{cccc}
w_{1,1}^{(1)} &amp; w_{1,2}^{(1)} \\[0.5em]
w_{2,1}^{(1)} &amp; w_{2,2}^{(1)} \\[0.5em]
w_{3,1}^{(1)} &amp; w_{3,2}^{(1)} \\[0.5em]
w_{4,1}^{(1)} &amp; w_{4,2}^{(1)} \\[0.5em]
\end{array} \right)
$$</p>
<p>Where every weight has the following properties:
$$w_{i\text{ , }j}^{(\ell)} $$</p>
<p>Think of this term as any single arrow in your neural network, where the subscripts and superscripts tell us which arrow it is. Each arrow will have some weight, which will be some number.</p>
<ul>
<li>$w$ simply means weight number (the value we multiply our previous inputs by</li>
<li>$i$ specifies which node in the layer it is going to</li>
<li>$j$ specifies which node from the previous layer the weight is coming from</li>
<li>$\ell$ specifies the layer we are trying to calculate the weights for. For example, $\ell=1$ for this particular weight matrix.</li>
<li>if we drop the subscript $j$, that means we are talking about a particular node in the layer</li>
<li>if we drop both subscripts and just have a capital $W^{(\ell)}$, this means we are talking about all arrows pointing into the LAYER, which means the <strong>entire weight matrix</strong>.</li>
<li>Any Weight Matrix feeding into a layer will have the dimensions (nodes of the current layer) x (nodes/features of the previous layer). In other words, $dim(W^{(\ell)}) = \ell\text{ x }(\ell-1)$</li>
</ul>
<p>Up until now, we have also been simplifying the neural network for visualization purposes. In reality, Neural Networks have a bias for every layer that looks something like this:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="38">
          <script>
            voila_process(38, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="39">
          <script>
            voila_process(39, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>The reason for this bias term is to ensure that some weights do not get too big to overcompensate for a poor intercept, when weights might have learned the shape already. You can imagine this with our spiral example. Imagine our weights learned a really good boundary for classifying the weight, but the whole boundary is shifted to the right. The bias accounts for this and shifts it back to its correct place. This bias term can be represented as
$$b^{(\ell)}_i$$
where</p>
<ul>
<li>i is the node the bias value corresponds to</li>
<li>$\ell$ is the layer the bias is being used to calculate.</li>
<li>$b^{(\ell)}$ is the bias value for that entire layer</li>
<li>$b_i^{(\ell)}$ is the bias value for the $i^{th}$ node of the $\ell^{th}$ layer
so an individual bias for the layer we are talking about for node can be represented as
$$b_1^{(1)}$$
which will correspond to some single scalar number. Likewise, the bias for the entire layer we are pointing at can be represented as:
$$
b^{(1)} = 
\left( \begin{array}{cccc}
b_{1}^{(1)} \\[0.5em]
b_{2}^{(1)} \\[0.5em]
b_{3}^{(1)} \\[0.5em]
b_{4}^{(1)} \\[0.5em]
\end{array} \right)
$$
where each subscript represents the node the bias corresponds to.</li>
</ul>

</div>
</div>
</div>
</div>
        </div><div cell-index="40">
          <script>
            voila_process(40, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Predicting-a-Number:-Forward-Pass">Predicting a Number: Forward Pass<a class="anchor-link" href="#Predicting-a-Number:-Forward-Pass">&#182;</a></h2><hr />
<p>By computing forward pass, we get a predicted value, given all of the features we are predicting with in our dataset. The final output will give what the neural networks &quot;thinks&quot; the value should be.</p>
<p>Our forward pass can be though of in 4 steps:</p>
<ul>
<li>Step 1: Multiply the Weights by the inputs</li>
<li>Step 2: add the biases</li>
<li>step 3: apply an activation function of your choice (ReLU, tanh, sigmoid, etc.)
These will give you the activated values for each of the next nodes.</li>
</ul>
<h3 id="Iteration-1:-The-First-Layer-of-Weights">Iteration 1: The First Layer of Weights<a class="anchor-link" href="#Iteration-1:-The-First-Layer-of-Weights">&#182;</a></h3><p>Let's use the neural network we have described from the previous layer:</p>
<h4 id="Step-1">Step 1<a class="anchor-link" href="#Step-1">&#182;</a></h4><p>We multiply our previous layer's input by the corresponding row gives us the next node. When we do this, we are getting some feature representation of the previous layer and using that arrows as a means to put them into the node. For example, for node one, we would have ($w_{1,1}^{(1)} \text{  } w_{1,2}^{(1)}$), which represents all the weights (white arrows) going to a specific node. Multiplying this by the inputs $(x_1 \text{  } x_2)$ we can compute $w_1^{(1)}x$ to get the value for node one before being activated:</p>
<p><strong>For node 1 in layer one (the top node):</strong></p>
<p>$$
\left( \begin{array}{cccc}
w_{1,1}^{(1)} &amp; w_{1,2}^{(1)}
\end{array} \right) 
\left( \begin{array}{cccc}
x_1  \\[0.5em]
x_2
\end{array} \right)
=
w_{1,1}^{(1)}x_1 + w_{1,2}^{(1)}x_2 = w_1^{(1)}x
$$</p>
<p>We can do the math for the entire layer by representing the multiplications in matrix form, with each subsequent node represented by its row position in the weight matrix:
$$
\left( \begin{array}{cccc}
w_{1,1}^{(1)} &amp; w_{1,2}^{(1)} \\[0.5em]
w_{2,1}^{(1)} &amp; w_{2,2}^{(1)} \\[0.5em]
w_{3,1}^{(1)} &amp; w_{3,2}^{(1)} \\[0.5em]
w_{4,1}^{(1)} &amp; w_{4,2}^{(1)} \\[0.5em]
\end{array} \right) 
\left( \begin{array}{cccc}
x_1  \\[0.5em]
x_2
\end{array} \right) = 
\left( \begin{array}{cccc}
w_{1,1}^{(1)}x_1 + w_{1,2}^{(1)}x_2 \\[0.5em]
w_{2,1}^{(1)}x_1 + w_{2,2}^{(1)}x_2 \\[0.5em]
w_{3,1}^{(1)}x_1 + w_{3,2}^{(1)}x_2 \\[0.5em]
w_{4,1}^{(1)}x_1 + w_{4,2}^{(1)}x_2 \\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
w_1^{(1)}x \\[0.5em]
w_2^{(1)}x \\[0.5em]
w_3^{(1)}x \\[0.5em]
w_4^{(1)}x \\[0.5em]
\end{array} \right) 
$$</p>
<h4 id="Step-2">Step 2<a class="anchor-link" href="#Step-2">&#182;</a></h4><p>Let's ADD bias 1 to node 1 as such, for example:</p>
<p>$$w_1^{(1)}x + b_1^{(1)}$$</p>
<p><strong>It is important that you do not multiply the bias. Only add. This is a common mistake many people make.</strong> As you can see, adding bias correlates to the <strong>red arrows</strong> coming from the diagram's input bias into the next layer. We can represent adding bias to all nodes in matrix notation:</p>
<p>$$
\left( \begin{array}{cccc}
w_1^{(1)}x \\[0.5em]
w_2^{(1)}x \\[0.5em]
w_3^{(1)}x \\[0.5em]
w_4^{(1)}x \\[0.5em]
\end{array} \right) 
+ \left( \begin{array}{cccc}
b_{1}^{(1)} \\[0.5em]
b_{2}^{(1)} \\[0.5em]
b_{3}^{(1)} \\[0.5em]
b_{4}^{(1)} \\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
w_1^{(1)}x + b_{1}^{(1)} \\[0.5em]
w_2^{(1)}x + b_{2}^{(1)} \\[0.5em]
w_3^{(1)}x + b_{3}^{(1)}\\[0.5em]
w_4^{(1)}x + b_{4}^{(1)}\\[0.5em]
\end{array} \right) 
$$
We will denote this matrix as $z^{(1)}$:
$$
z^{(1)} = 
\left( \begin{array}{cccc}
z_{1}^{(1)} \\[0.5em]
z_{2}^{(1)} \\[0.5em]
z_{3}^{(1)} \\[0.5em]
z_{4}^{(1)} \\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
w_1^{(1)}x + b_{1}^{(1)} \\[0.5em]
w_2^{(1)}x + b_{2}^{(1)} \\[0.5em]
w_3^{(1)}x + b_{3}^{(1)}\\[0.5em]
w_4^{(1)}x + b_{4}^{(1)}\\[0.5em]
\end{array} \right) 
$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="41">
          <script>
            voila_process(41, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We are now at this location of the graph, without activation. We need to activate it before we send our &quot;nodes&quot; to the next layer.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="42">
          <script>
            voila_process(42, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="43">
          <script>
            voila_process(43, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-3">Step 3<a class="anchor-link" href="#Step-3">&#182;</a></h4><p>Finally, we can apply an activation, that we mentioned a previously. For the sake of ease, we let $f(x)$ be any non-linear <a href="####Types-of-activation-functions">activation function</a>. Then we have:
$$
f(z^{(1)}) = 
f\left( \begin{array}{cccc}
z_{1}^{(1)} \\[0.5em]
z_{2}^{(1)} \\[0.5em]
z_{3}^{(1)} \\[0.5em]
z_{4}^{(1)} \\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
\text{Node 1}^{(1)} \\[0.5em]
\text{Node 2}^{(1)} \\[0.5em]
\text{Node 3}^{(1)} \\[0.5em]
\text{Node 4}^{(1)} \\[0.5em]
\end{array} \right)=

\left( \begin{array}{cccc}
a_1^{(1)} \\[0.5em]
a_2^{(1)} \\[0.5em]
a_3^{(1)} \\[0.5em]
a_4^{(1)} \\[0.5em]
\end{array} \right) = a^{(1)}
$$</p>
<p>where $a$ stands for activated.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="44">
          <script>
            voila_process(44, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>You would repeat this process over and over again until you reached the end of the network. Let's do the next layer, just to see what this would look like.</p>
<h3 id="Iteration-2:-The-Second-Layer-of-Weights">Iteration 2: The Second Layer of Weights<a class="anchor-link" href="#Iteration-2:-The-Second-Layer-of-Weights">&#182;</a></h3><p>If we followed the equations correctly, you will now have the values for the first hidden layer. Let's focus on this layer, as denoted by the blue arrow:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="45">
          <script>
            voila_process(45, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="46">
          <script>
            voila_process(46, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-1">Step 1<a class="anchor-link" href="#Step-1">&#182;</a></h4><p>We multiply our nodes by the new weight matrices. Notice how this matrix multiplication forces the numbers to be in the same dimensions as the next node layer. A plus!</p>
<p>$$
\left( \begin{array}{cccc}
w_{1,1}^{(2)} &amp; w_{1,2}^{(2)} &amp; w_{1,3}^{(2)} &amp; w_{1,4}^{(2)}\\[0.5em]
w_{2,1}^{(2)} &amp; w_{2,2}^{(2)} &amp; w_{2,3}^{(2)} &amp; w_{2,4}^{(2)} \\[0.5em]
\end{array} \right) 
\left( \begin{array}{cccc}
\text{Node 1}^{(1)} \\[0.5em]
\text{Node 2}^{(1)} \\[0.5em]
\text{Node 3}^{(1)} \\[0.5em]
\text{Node 4}^{(1)} \\[0.5em]
\end{array} \right) = 
\left( \begin{array}{cccc}
w_{1,1}^{(2)}a_1^{(1)} + w_{1,2}^{(2)}a_2^{(1)} + w_{1,3}^{(2)}a_3^{(1)} + w_{1,4}^{(2)}a_4^{(1)} \\[0.5em]
w_{2,1}^{(2)}a_1^{(1)} + w_{2,2}^{(2)}a_2^{(1)} + w_{2,3}^{(2)}a_3^{(1)} + w_{2,4}^{(2)}a_4^{(1)}\\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
w_1^{(2)}a^{(1)} \\[0.5em]
w_2^{(2)}a^{(1)} \\[0.5em]
\end{array} \right) 
$$
<strong>Notice the pattern here. Every row represents all of the &quot;arrows&quot; going into that node, and every &quot;arrow&quot; multiplies the previous layer's input, $a$, but some weight. When we perform the multiplication, all the the numbers correctly format to the number of nodes in the next layer!</strong>
Also, notice that any node $i$ in layer $\ell$ is the same thing as $a_i^{(\ell)}$.</p>
<h4 id="Step-2">Step 2<a class="anchor-link" href="#Step-2">&#182;</a></h4><p>We now add our bias, as deonted by the red arrows on the diagram.
$$
\left( \begin{array}{cccc}
w_1^{(2)}a^{(1)} \\[0.5em]
w_2^{(2)}a^{(1)} \\[0.5em]
\end{array} \right) 
+ \left( \begin{array}{cccc}
b_{1}^{(2)} \\[0.5em]
b_{2}^{(2)} \\[0.5em]
\end{array} \right) = 
\left( \begin{array}{cccc}
w_1^{(2)}a_1^{(1)} + b_{1}^{(2)} \\[0.5em]
w_2^{(2)}a_1^{(1)} + b_{2}^{(2)} \\[0.5em]
\end{array} \right) = 
\left( \begin{array}{cccc}
z_1^{(2)} \\[0.5em]
z_2^{(2)} \\[0.5em]
\end{array} \right)  = z^{(2)}
$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="47">
          <script>
            voila_process(47, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>This puts us at this current location of the graph:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="48">
          <script>
            voila_process(48, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="49">
          <script>
            voila_process(49, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-3">Step 3<a class="anchor-link" href="#Step-3">&#182;</a></h4><p>We can see that this is actually the last weight layer, and given that this is a classification head, we have a softmax activation. This means that
$$f(x) = \frac{e^{z_i}}{e^{z_1}+e^{z_2}}, \text{ where i is whatever node we are coming from.} $$</p>
<p>We can apply it as such:
$$
f(z^{(2)}) =
f\left( \begin{array}{cccc}
z_1^{(2)} \\[0.5em]
z_2^{(2)} \\[0.5em]
\end{array} \right)  = 
\Large\left( \begin{array}{cccc}
\frac{e^{z_1^{(2)}}}{e^{z_1^{(2)}} + e^{z_2^{(2)}}} \\[0.5cm]
\frac{e^{z_2^{(2)}}}{e^{z_1^{(2)}} + e^{z_2^{(2)}}}  \\[0.5em]
\end{array} \right) =
\normalsize\left( \begin{array}{cccc}
a_1^{(2)} \\[0.5em]
a_2^{(2)}  \\[0.5em]
\end{array} \right) = 
a^{(2)} 
$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="50">
          <script>
            voila_process(50, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="51">
          <script>
            voila_process(51, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We have now finished forward pass!</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="52">
          <script>
            voila_process(52, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Alternative-Forward-pass:-What-if-we-were-doing-a-regression-problem?">Alternative Forward pass: What if we were doing a regression problem?<a class="anchor-link" href="#Alternative-Forward-pass:-What-if-we-were-doing-a-regression-problem?">&#182;</a></h3><p>If we were doing regression, our neural network will look more like this, as we are predicting some numerical value.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="53">
          <script>
            voila_process(53, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="54">
          <script>
            voila_process(54, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Iteration-1:">Iteration 1:<a class="anchor-link" href="#Iteration-1:">&#182;</a></h3><p>This would be exactly the same as the previous problem:</p>
<h4 id="Step-1:">Step 1:<a class="anchor-link" href="#Step-1:">&#182;</a></h4><p>$$
\left( \begin{array}{cccc}
w_{1,1}^{(1)} &amp; w_{1,2}^{(1)} \\[0.5em]
w_{2,1}^{(1)} &amp; w_{2,2}^{(1)} \\[0.5em]
w_{3,1}^{(1)} &amp; w_{3,2}^{(1)} \\[0.5em]
w_{4,1}^{(1)} &amp; w_{4,2}^{(1)} \\[0.5em]
\end{array} \right) 
\left( \begin{array}{cccc}
x_1  \\[0.5em]
x_2
\end{array} \right) = 
\left( \begin{array}{cccc}
w_{1,1}^{(1)}x_1 + w_{1,2}^{(1)}x_2 \\[0.5em]
w_{2,1}^{(1)}x_1 + w_{2,2}^{(1)}x_2 \\[0.5em]
w_{3,1}^{(1)}x_1 + w_{3,2}^{(1)}x_2 \\[0.5em]
w_{4,1}^{(1)}x_1 + w_{4,2}^{(1)}x_2 \\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
w_1^{(1)}x \\[0.5em]
w_2^{(1)}x \\[0.5em]
w_3^{(1)}x \\[0.5em]
w_4^{(1)}x \\[0.5em]
\end{array} \right) 
$$
<strong>Note:</strong> $W^{(1)}$ has dimensions $4 \text{ x } 2$ because the next layer has 4 nodes and the previous layer had 2 nodes/features.</p>
<h4 id="Step-2:">Step 2:<a class="anchor-link" href="#Step-2:">&#182;</a></h4><p>$$
\left( \begin{array}{cccc}
w_1^{(1)}x \\[0.5em]
w_2^{(1)}x \\[0.5em]
w_3^{(1)}x \\[0.5em]
w_4^{(1)}x \\[0.5em]
\end{array} \right) 
+ \left( \begin{array}{cccc}
b_{1}^{(1)} \\[0.5em]
b_{2}^{(1)} \\[0.5em]
b_{3}^{(1)} \\[0.5em]
b_{4}^{(1)} \\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
w_1^{(1)}x + b_{1}^{(1)} \\[0.5em]
w_2^{(1)}x + b_{2}^{(1)} \\[0.5em]
w_3^{(1)}x + b_{3}^{(1)}\\[0.5em]
w_4^{(1)}x + b_{4}^{(1)}\\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
z_{1}^{(1)} \\[0.5em]
z_{2}^{(1)} \\[0.5em]
z_{3}^{(1)} \\[0.5em]
z_{4}^{(1)} \\[0.5em]
\end{array} \right) 
 =z^{(1)}
$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="55">
          <script>
            voila_process(55, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We are now in this portion of our network:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="56">
          <script>
            voila_process(56, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="57">
          <script>
            voila_process(57, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-3:">Step 3:<a class="anchor-link" href="#Step-3:">&#182;</a></h4><p>We now activate our nodes, just like we did previously.
$$
f(z^{(1)}) = 
f\left( \begin{array}{cccc}
z_{1}^{(1)} \\[0.5em]
z_{2}^{(1)} \\[0.5em]
z_{3}^{(1)} \\[0.5em]
z_{4}^{(1)} \\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
\text{Node 1}^{(1)} \\[0.5em]
\text{Node 2}^{(1)} \\[0.5em]
\text{Node 3}^{(1)} \\[0.5em]
\text{Node 4}^{(1)} \\[0.5em]
\end{array} \right)=

\left( \begin{array}{cccc}
a_1^{(1)} \\[0.5em]
a_2^{(1)} \\[0.5em]
a_3^{(1)} \\[0.5em]
a_4^{(1)} \\[0.5em]
\end{array} \right) = a^{(1)}
$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="58">
          <script>
            voila_process(58, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We are now here in our neural network:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="59">
          <script>
            voila_process(59, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="60">
          <script>
            voila_process(60, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Iteration-2:">Iteration 2:<a class="anchor-link" href="#Iteration-2:">&#182;</a></h3><p>This is where things get to be a little different. Since we are only predicting one numerical value, we only have one output node.</p>
<ul>
<li><strong>Note:</strong> It is possible to have multiple nodes for regression. What this would mean is we are predicting multiple values at onces. For example, if we have house size, lot size, and the numnber of bathroom as our input (x1,x2,x3), we  could have 2 regression heads:<ul>
<li>One predicts house price (y1)</li>
<li>One predicts the number of bedrooms (y2)</li>
</ul>
</li>
<li>This effectively predicting 2 values at once.</li>
</ul>

</div>
</div>
</div>
</div>
        </div><div cell-index="61">
          <script>
            voila_process(61, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-1:">Step 1:<a class="anchor-link" href="#Step-1:">&#182;</a></h4><p>Multiply our weights by the nodes:</p>
<p>$$
\left( \begin{array}{cccc}
w_{1,1}^{(2)} &amp; w_{1,2}^{(2)} &amp; w_{1,3}^{(2)} &amp; w_{1,4}^{(2)}\\[0.5em]
\end{array} \right) 
\left( \begin{array}{cccc}
\text{Node 1}^{(1)} \\[0.5em]
\text{Node 2}^{(1)} \\[0.5em]
\text{Node 3}^{(1)} \\[0.5em]
\text{Node 4}^{(1)} \\[0.5em]
\end{array} \right) = 
\left( \begin{array}{cccc}
w_{1,1}^{(2)}a_1^{(1)} + w_{1,2}^{(2)}a_2^{(1)} + w_{1,3}^{(2)}a_3^{(1)} + w_{1,4}^{(2)}a_4^{(1)} \\[0.5em]
\end{array} \right) =
\left( \begin{array}{cccc}
w_1^{(2)}a^{(1)} \\[0.5em]
\end{array} \right) 
$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="62">
          <script>
            voila_process(62, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Notice how we only have 1 node in the next layer, our node can be represented in a 1x1 matrix.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="63">
          <script>
            voila_process(63, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-2:">Step 2:<a class="anchor-link" href="#Step-2:">&#182;</a></h4><p>Add our bias:</p>
<p>$$
\left( \begin{array}{cccc}
w_1^{(2)}a^{(1)} \\[0.5em]
\end{array} \right) 
+ \left( \begin{array}{cccc}
b_{1}^{(2)} \\[0.5em]
\end{array} \right) = 
\left( \begin{array}{cccc}
w_1^{(2)}a_1^{(1)} + b_{1}^{(2)} \\[0.5em]
\end{array} \right) = 
\left( \begin{array}{cccc}
z_1^{(2)} \\[0.5em]
\end{array} \right)  = z^{(2)}
$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="64">
          <script>
            voila_process(64, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We are now in the following portion of out network:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="65">
          <script>
            voila_process(65, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="66">
          <script>
            voila_process(66, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-3:">Step 3:<a class="anchor-link" href="#Step-3:">&#182;</a></h4><p>We finish our forward pass using the regression activation. This means the layer is just linear, so we dont need to apply anything, we already have a numeric number! This means that the activation for this layer is just
$$
f(z^{(2)}) =
f\left( \begin{array}{cccc}
z^{(2)} \\[0.5em]
\end{array} \right)  =
\left( \begin{array}{cccc}
z^{(2)} \\[0.5em]
\end{array} \right) =
\normalsize\left( \begin{array}{cccc}
a^{(2)} \\[0.5em]
\end{array} \right) = 
a^{(2)} 
$$
This puts us here:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="67">
          <script>
            voila_process(67, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="68">
          <script>
            voila_process(68, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Since we are at our output layer, this is actually just the output itself. This concludes forward pass through the entire network. If you wish to have more practice, I recommend asking ChatGPT for small neural networks to feed forward through. Then ask if you get it right.</p>
<h3 id="Final-Forward-Pass-Algorithm">Final Forward Pass Algorithm<a class="anchor-link" href="#Final-Forward-Pass-Algorithm">&#182;</a></h3><p>In conclusion, each forward pass for the nodes in layer l can be can be summed up to the following Equation:</p>
<p>$$\Large a^{(\ell)} = f(W^{(\ell)}\cdot a^{(\ell-1)} + b^{\ell})$$</p>
<p>where</p>
<ul>
<li>$f(x)$ is the <a href="##Introduction-to-Neural-Networks">activation function</a></li>
<li>$a^{(\ell)}$ is a layer's output from its nodes</li>
<li>$a^{(\ell-1)}$ is the previous layer nodes, which I have shown to be the matrix $(\text{Node 1}^{(1)}, \text{Node 2}^{(1)},\cdots)$ when we were computing the final output, for example (Iteration 2)</li>
<li>$W^{(\ell)}$ is the weight matrix for that layer</li>
<li>$b^{\ell}$ is the bias for that layer</li>
<li>$\cdot$ is a matrix multiplication</li>
<li>$a^{(1)}$ is always one sample in your dataset (a single row of the dataset). It has d &quot;nodes&quot; (number of columns (features), without the classification labels)</li>
<li>The following are equivalent: $z^{(\ell)} = W^{(\ell)}\cdot a^{(\ell-1)} + b^{\ell}$ and $a^{(\ell)} = f(z^{(\ell)})$</li>
<li>$z^{(\ell)}$ is the node before being activated by its respective activation function</li>
</ul>
<p>Using this, you should effectively be able to calculate the &quot;predictions&quot; a neural network makes, depending on the input feature given for <strong>one sample in a dataset</strong>.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="69">
          <script>
            voila_process(69, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Making-our-Models-Learn:-Backpropagation">Making our Models Learn: Backpropagation<a class="anchor-link" href="#Making-our-Models-Learn:-Backpropagation">&#182;</a></h2><hr />
<p>We have covered how to predict a number with our neural network. However, if our model doesn't learn, how will it ever predict anything correctly? This is why we have to slowly &quot;tune&quot; our model so that it's weights will correctly predict what we want it to. We do this by using gradients to move in the direction that reduces this loss as much as possible.</p>
<h3 id="What-are-Gradients?">What are Gradients?<a class="anchor-link" href="#What-are-Gradients?">&#182;</a></h3><p>If you have every taken a calculus class before, you know what a derivative is: the change in a function at any given point f(x). The difference between derivatives and gradients lies in the fact that a gradient can be taken with respect to multiple variables, with their partial derivatives. This gradient is a <strong>vector</strong> of derivatives that not only tell you the magnitude of change, but also the <strong>direction</strong> of greatest increase. This is really useful, because most problems in machine learning are never just a single variable. Let's see an example:</p>
<p>Assume we have the function
$$f(x,y) = x^2+y^2$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="70">
          <script>
            voila_process(70, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="71">
          <script>
            voila_process(71, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Let's say we take the gradient at point (0.2,0.2):</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="72">
          <script>
            voila_process(72, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="73">
          <script>
            voila_process(73, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>and the gradient at (.5,.5):</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="74">
          <script>
            voila_process(74, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="75">
          <script>
            voila_process(75, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Notice how much bigger the arrow is. If we flip this around this can correspond to how &quot;steep&quot; the function is.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="76">
          <script>
            voila_process(76, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="77">
          <script>
            voila_process(77, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>But how does this relate to training our model?</p>
<h3 id="Minimizing-Loss-with-Gradients:-Gradient-Descent">Minimizing Loss with Gradients: Gradient Descent<a class="anchor-link" href="#Minimizing-Loss-with-Gradients:-Gradient-Descent">&#182;</a></h3><p>We had shown earlier the different loss fucntions we use for a neural network. If we were to review them again, we see that that we have the following loss functions:</p>
<p><strong>Classification:</strong>
$$\text{ categorical cross entropy loss }= -\frac{1}{n}\sum_{i=1}^n \hat{y_i}\log(p_i)\text{. }$$
<strong>Regression:</strong>
$$MSE \text{ loss }= -\frac{1}{n}\sum_{i=1}^n (\hat{y_i}-\bar{y_i})^2\text{. }$$</p>
<ul>
<li><strong>Reminder:</strong> These aren't the only loss functions, just the most common.</li>
</ul>
<p>What this means is that the loss might be like some function like we see above, where the smallest loss is at the lowest point (minima). However, since every single sample in our dataset will generate a different function, we dont ever really know what the loss functions look like or even is for that matter. We just randomly &quot;spawn&quot; on some part of this function, and try to guess where we should go. This means we don't know what our &quot;true&quot; loss function looks like. We are blind, which means we want to take a &quot;step&quot; in the direction in which we decrease loss. Since the point with the smallest loss means we get the best prediction, the intuition is that <strong>we can change the weights with respect to the change in loss in the negative direction to take a step towards this minima,</strong> until we reach the minima. This is the idea behind backpropagation for gradient descent.</p>
<p>If we take the gradient of the loss functions, we get the following values:</p>
<p><strong>Classification:</strong>
$$softmax' \text{ for a single sample } = (p_i-\hat{y_i})\text{. }$$
<strong>Regression:</strong>
$$MSE' \text{ for a single sample }= \hat{y_i}-\bar{y_i}\text{. }$$</p>
<p>the following is a key for all of the variables:</p>
<ul>
<li>$p_i$ is the softmax function</li>
<li>$\hat{y_i}$ is the true value that we are trying to predict for the label $i$ in our dataset.</li>
<li>$\bar{y_i}$ is the value we predicted with our model</li>
<li>$x_i$ are the values we get from the last layer.</li>
<li>$\frac{1}{n}\sum_{i=1}^n$ means the &quot;mean&quot; of the values.</li>
</ul>
<p>If you look closely, the gradients of the loss quantify how &quot;far&quot; our prediction was from the actual values. By changing all the weights in our matrix, subtracting the gradients of the weights with respect to the loss for every single weight, we do a single step of backpropagation. If we do this backpropagation step until we reach the loss minimum, we will successfully perform gradient descent, our model will eventually &quot;converge&quot; to the local minima, and be able to predict the values we want!</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="78">
          <script>
            voila_process(78, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Backpropagation:-The-Math-Behind-Each-Backwards-Step">Backpropagation: The Math Behind Each Backwards Step<a class="anchor-link" href="#Backpropagation:-The-Math-Behind-Each-Backwards-Step">&#182;</a></h3><p>While in theory backpropagation might make sense, this is generally one of the hardest concepts to grasp the neural network. This is because backpropagation compounds on the fact that in order to calulate the gradient of the weights, (which is a weight matrix, as we had shown in forward pass), we have to chain the derivatives of the loss with the activations and pre-activations to be able to get the corresponding weight gradient. This means we have to calculate the gradient of the weights and biases for every layer, and tune them individually. For example, let's look at the following example:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="79">
          <script>
            voila_process(79, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="80">
          <script>
            voila_process(80, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>As a disclaimer, I have no idea how this was derived. I only know the formulas. If we want to calculate the gradients of all of the weights with respect to the loss for our example network, it would look something like the following:</p>
<p>$$\nabla_{W^{(3)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(3)}}{\partial W^{(3)}}$$
$$\nabla_{b^{(3)}} = \frac{\partial L}{\partial z^{(3)}}$$
$$\nabla_{W^{(2)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(3)}}{\partial a^{(2)}}\cdot\frac{\partial a^{(2)}}{\partial z^{(2)}}\cdot\frac{\partial z^{(2)}}{\partial W^{(2)}}$$
$$\nabla_{b^{(2)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(3)}}{\partial a^{(2)}}\cdot\frac{\partial a^{(2)}}{\partial z^{(2)}}$$
$$\nabla_{W^{(1)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(3)}}{\partial a^{(2)}}\cdot\frac{\partial a^{(2)}}{\partial z^{(2)}}\cdot\frac{\partial z^{(2)}}{\partial a^{(1)}}\cdot\frac{\partial a^{(1)}}{\partial z^{(1)}}\cdot\frac{\partial z^{(1)}}{\partial W^{(1)}}$$
$$\nabla_{b^{(1)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(3)}}{\partial a^{(2)}}\cdot\frac{\partial a^{(2)}}{\partial z^{(2)}}\cdot\frac{\partial z^{(2)}}{\partial a^{(1)}}\cdot\frac{\partial a^{(1)}}{\partial z^{(1)}}$$
This essentially means you are calulating the loss of the current layer by looking at the loss from the next layer and multiplying it by the derivatives of the previous layer's activated values and unactivated values.</p>
<p><strong>Tip:</strong> If you are coding this yourself, it's a good idea to save the pre-activated ($z$) and activated ($a$) values seperately.</p>
<p>Where</p>
<ul>
<li>$\large\frac{\partial L}{\partial z^{(i)}}$ is the direction of change needed to adjust the third layer's pre-activated z for the propagation of the loss</li>
<li>$\large\frac{\partial z^{(i)}}{\partial a^{(i-1)}}$  is the change needed to adjust the previous layer's node (activated value) for the propogation of the loss</li>
<li>$\large\frac{\partial a^{(i)}}{\partial z^{(i)}}$ is the change needed to adjust the current layer's value based on the activation function. Since the activation function is a function itself, that's how the chain rule ends up growing in length the more you go through layers.</li>
</ul>
<p>It's a little hard to visualize these derivative changes, so let's try and work through the math. I know this is pretty complex, but notice this pattern here:</p>
<ul>
<li>We only need to compute the derivative with respect to the weight we care about</li>
<li>Otherwise, its just some combination of $a$ (the node output for that layer), and $z$ (the preactivated node output for that layer-think before we apply our activation function) propagated from layers previous. This means that once we compute all of the other previous values, we just have to compute the next one in succession!</li>
<li>You also already compute the gradient for the bias of each layer before finding the gradient of the weights</li>
</ul>
<p>Knowing all of this, <strong>it's possible to set up a recursive formula to simplify it, like you would a loop in a coding program!</strong> This part of the program will be pretty math-intensive, so feel free to go back and reread it. I will also have examples of me using backpropagation in examples, so you can get a grasp of it. IF you want even more practice, I personally learned backpropagation by having ChatGPT generate me small feed forward and backpropagation problems for both categorical and regression tasks that I could do by hand, with about 1-2 hidden layers each. Practice makes perfect.</p>
<h4 id="What-the-first-$%5Cdelta$-is:">What the first $\delta$ is:<a class="anchor-link" href="#What-the-first-$%5Cdelta$-is:">&#182;</a></h4><p>For our purposes, $\delta$ just means calculated loss, and likewise $\delta^{(\ell)}$ is the loss for that specific layer. The final layer, $\delta^{(\ell)}$ is just our derivative loss values, or $\large\frac{\partial L}{\partial z^{(3)}}$. Do you remember the $(p_i-\hat{y_i})$ from above? For a hypothetical example, we have 3 categorical variables. Let's say we're trying to predict a number being either 1, 2, or 3, just to give meaning to the classification head, and let's say for the particular example we give it the true value is a 2. This means our $\hat{y_i}=$
$$
\left( \begin{array}{cccc}
0\\[0.5em]
1\\[0.5em]
0\\[0.5em]
\end{array}\right)
$$
then let's assume our model predicts the following values for each category with the softmax:</p>
<p>$$
\left( \begin{array}{cccc}
.39\\[0.5em]
.33\\[0.5em]
.28\\[0.5em]
\end{array}\right)
$$</p>
<p>then the loss with respect to the last layer for our hypthetical values will be</p>
<p>$$
\delta^{(\ell)} = 
\left( \begin{array}{cccc}
.39\\[0.5em]
.33\\[0.5em]
.28\\[0.5em]
\end{array}\right) -
\left( \begin{array}{cccc}
0\\[0.5em]
1\\[0.5em]
0\\[0.5em]
\end{array}\right)  
 = 
\left( \begin{array}{cccc}
-.61\\[0.5em]
.67\\[0.5em]
-.28\\[0.5em]
\end{array}\right)
$$
You can think of it like we are trying to lower the values that are wrong and increase the values that are right! I will give an example of regression for this at the bottom of the document.</p>
<h4 id="Deriving-the-update-rule">Deriving the update rule<a class="anchor-link" href="#Deriving-the-update-rule">&#182;</a></h4><p>The most important thing to deriving the update rule is noticing patterns. We can see that any loss for the next layer is simply the loss for the previous layer, with the following values tacked on: $$\frac{\partial z^{(3)}}{\partial a^{(2)}}\cdot\frac{\partial a^{(2)}}{\partial z^{(2)}}$$
Then that means we can let $\delta^{(\ell)}$ be the loss of the function per layer. If we look at the pattern I showed you we can build a recursive rule to build a loop with: $\large\delta^{(\ell)}\cdot\frac{\partial z^{(\ell)}}{\partial a^{(\ell-1)}}\cdot\frac{\partial a^{(\ell-1)}}{\partial z^{(\ell-1)}}$.</p>
<p>To calculate the loss $\delta^{(\ell-1)}$. We know that $z^{(\ell)} = W^{(\ell)}\cdot a^{(\ell-1)} + b^{(\ell)}$, from our forward pass, before activation. We can start from here, because we already computed the derivative of the final layer doing ($p_i-y_i$). This means that taking the derivative of this function via the chain rule would therefore look something like this:
$$\frac{\partial z^{(\ell)}}{\partial a^{(\ell-1)}}[z^{(\ell)} = W^{(\ell)}\cdot a^{(\ell-1)} + b^{(\ell)}] = W^{(\ell)}$$
since the bias term just goes away. However for our second derivative $\large\frac{\partial a^{(\ell-1)}}{\partial z^{(\ell-1)}}$, we use the function $a^{(\ell-1)} = f(W^{(\ell-1)}\cdot a^{(\ell-2)} + b^{(\ell-1)})$. Notice how this is functionally equivalent to $a^{(\ell-1)} = f(z^{(\ell-1)})$. Then
$$\frac{\partial a^{(\ell-1)}}{\partial z^{(\ell-1)}}[f(z^{(\ell-1)})] = f'(z^{(\ell-1)})\cdot 1 = f'(z^{(\ell-1)})$$
where $f'(x)$ is the derivative of the activation function for the layer $\ell-1$.</p>
<p>Notice that this z vector is a vector, not a matrix corresponding to the size of the nodes. This means when we finally construct our update rule, we need to do element-wise multiplication, via the hadamard product.</p>
<h4 id="Mini-lesson-on-Hadamard-Product">Mini-lesson on Hadamard Product<a class="anchor-link" href="#Mini-lesson-on-Hadamard-Product">&#182;</a></h4><p>You can only multiply these 2 matrices/vectors if they are the same dimensions, and every element in one matrix A $a_{i\text{ , } j}$ will be multiplied by its corresponding coordinates in matrix B $b_{i\text{ , } j}$ to get a matrix AB = $a_{i\text{ , } j}*b_{i\text{ , } j}$. For example, let's say I have 2 matrices</p>
<p>$$
A = \left( \begin{array}{cccc}
1 &amp; 2 &amp; 3\\[0.5em]
4 &amp; 5 &amp; 6\\[0.5em]
7 &amp; 8 &amp; 9\\[0.5em]
\end{array}\right) \text{ and }
B = \left( \begin{array}{cccc}
1 &amp; 2 &amp; 3\\[0.5em]
4 &amp; 5 &amp; 6\\[0.5em]
7 &amp; 8 &amp; 9\\[0.5em]
\end{array}\right)
$$</p>
<p>Their hadamard product (element-wise matrix multiplication) would be</p>
<p>$$
\left( \begin{array}{cccc}
1 &amp; 2 &amp; 3\\[0.5em]
4 &amp; 5 &amp; 6\\[0.5em]
7 &amp; 8 &amp; 9\\[0.5em]
\end{array}\right)\odot
\left( \begin{array}{cccc}
1 &amp; 2 &amp; 3\\[0.5em]
4 &amp; 5 &amp; 6\\[0.5em]
7 &amp; 8 &amp; 9\\[0.5em]
\end{array}\right) = 
\left( \begin{array}{cccc}
1 &amp; 4 &amp; 9\\[0.5em]
16 &amp; 25 &amp; 36\\[0.5em]
49 &amp; 64 &amp; 81\\[0.5em]
\end{array}\right) = AB
$$</p>
<p>normal matrix multiplication on the other hand would yield
$$
\left( \begin{array}{cccc}
1 &amp; 2 &amp; 3\\[0.5em]
4 &amp; 5 &amp; 6\\[0.5em]
7 &amp; 8 &amp; 9\\[0.5em]
\end{array}\right) \text{X}
\left( \begin{array}{cccc}
1 &amp; 2 &amp; 3\\[0.5em]
4 &amp; 5 &amp; 6\\[0.5em]
7 &amp; 8 &amp; 9\\[0.5em]
\end{array}\right) = 
\left( \begin{array}{cccc}
30 &amp; 36 &amp; 42\\[0.5em]
66 &amp; 81 &amp; 96\\[0.5em]
102 &amp; 126 &amp; 150\\[0.5em]
\end{array}\right) = AB
$$</p>
<h4 id="Back-to-Propagation">Back to Propagation<a class="anchor-link" href="#Back-to-Propagation">&#182;</a></h4><p>with this now in mind, we put our derivations together to get our new update rule for each layer's loss:</p>
<p>$$\large\delta^{(\ell-1)} = \delta^{(\ell)}\cdot\frac{\partial z^{(\ell)}}{\partial a^{(\ell-1)}}\cdot\frac{\partial a^{(\ell-1)}}{\partial z^{(\ell-1)}} = W^{(\ell)T}\delta^{(\ell)} \odot f'(z^{(\ell-1)})
$$</p>
<p>or less confusingly,
$$\Large\delta^{(\ell-1)} = W^{(\ell)T}\delta^{(\ell)} \odot f'(z^{(\ell-1)}).$$
This is the loss for every layer! As a reminder:</p>
<ul>
<li>$(\ell)$ is the layer</li>
<li>$\large \delta^{(\ell-1)}$ is the loss for the layer $\ell-1$</li>
<li>$\large W^{(\ell)}$ is the weight matrix for layer $\ell$</li>
<li>$\large \delta^{(\ell)}$ is the loss for the layer $\ell$</li>
<li>$\large z^{(\ell-1)}$ is the values calculated for the node BEFORE it is sent through the activation function for that layer</li>
<li>$\large f(z^{(\ell)})$ is the activation function for the layer corresponding to the layer of it's z value</li>
<li>$\large f'(x)$ is the derivative of the activation function</li>
<li>$\large f'(z^{(\ell-1)})$ are the preactivated nodes from the $\ell-1$ layer, <strong>put through the DERIVATIVE of their activation.</strong> &lt;- This is common source of confusion.</li>
<li>$\odot$ is the hadamard product, aka element-wise multiplication (<strong>DIFFERENT FROM MATRIX MULTIPLICATION</strong>)</li>
<li>$^T$ is transpose, where we switch the rows to be the columns of a matrix, and the columns to be the rows.</li>
</ul>

</div>
</div>
</div>
</div>
        </div><div cell-index="81">
          <script>
            voila_process(81, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Getting-the-Gradient-of-the-bias-and-Weight-Matrices">Getting the Gradient of the bias and Weight Matrices<a class="anchor-link" href="#Getting-the-Gradient-of-the-bias-and-Weight-Matrices">&#182;</a></h4><p>The most important part about making update rules is looking for patterns. Look at the following: What do you notice?</p>
<h5 id="Bias">Bias<a class="anchor-link" href="#Bias">&#182;</a></h5><p>Let's look at our previous examples for bias derivations:
$$\nabla_{b^{(3)}} = \frac{\partial L}{\partial z^{(3)}}$$
$$\nabla_{b^{(2)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(3)}}{\partial a^{(2)}}\cdot\frac{\partial a^{(2)}}{\partial z^{(2)}}$$
$$\nabla_{b^{(1)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(3)}}{\partial a^{(2)}}\cdot\frac{\partial a^{(2)}}{\partial z^{(2)}}\cdot\frac{\partial z^{(2)}}{\partial a^{(1)}}\cdot\frac{\partial a^{(1)}}{\partial z^{(1)}}$$</p>
<p>Did you notice that this is exactly the loss of every single layer? This means that for our update rule, once we find our the loss we find our bias! in other words
$$\Large\delta^{(\ell)}=\nabla_{b^{(\ell)}}$$</p>
<h5 id="Weights">Weights<a class="anchor-link" href="#Weights">&#182;</a></h5><p>for $\large\nabla_{W^{(\ell)}}$, we notice that the only difference is now we tack on $\large\frac{\partial z^{(3)}}{\partial W^{(3)}}$ or
$$\nabla_{W^{(3)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(\ell)}}{\partial W^{(\ell)}}$$
$$\nabla_{W^{(2)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(3)}}{\partial a^{(2)}}\cdot\frac{\partial a^{(2)}}{\partial z^{(2)}}\cdot\frac{\partial z^{(2)}}{\partial W^{(2)}}$$
$$\nabla_{W^{(1)}} = \frac{\partial L}{\partial z^{(3)}}\cdot\frac{\partial z^{(3)}}{\partial a^{(2)}}\cdot\frac{\partial a^{(2)}}{\partial z^{(2)}}\cdot\frac{\partial z^{(2)}}{\partial a^{(1)}}\cdot\frac{\partial a^{(1)}}{\partial z^{(1)}}\cdot\frac{\partial z^{(1)}}{\partial W^{(1)}}$$</p>
<p>all of these are essentially loss multiplied by the derivative of the weight matrix (gradient). So then we can build the following rule, by looking at the patterns:
$$\large\nabla_{W^{(\ell)}} = \delta^{(\ell)}\cdot\frac{\partial z^{(\ell)}}{\partial W^{(\ell)}}$$
if we deconstruct this derivative, the top is the function $z^{(\ell)}$. We then can grab the function corresponding: $z^{(\ell)} = W^{(\ell)}\cdot a^{(\ell-1)} + b^{(\ell)}$. Then
$$\frac{\partial z^{(\ell)}}{\partial W^{(\ell)}}[W^{(\ell)}\cdot a^{(\ell-1)} + b^{(\ell)}] =  a^{(\ell-1)}$$
Notice that $\delta{(\ell)}$ and $a^{(\ell-1)}$ do not match dimension, nor will they create a matrix of the same dimensions of the weight matrix. They are both dimensions $\large n^{(\ell)}\text{x}1$ and $\large n^{(\ell-1)}\text{x}1$ respectively, where $n$ is the number of nodes in the layer. IF we want to make it the size of the weight matrix, we do $\large a^{(\ell-1)T}$ so we get dimensions $\large n^{(\ell)}\text{x}1$ and $\large 1\text{x}n^{(\ell-1)}$ for the matrix outer product, which matches the dimensions of the weight matrix. We can now find out what the update rule for the gradient of a weight matrix of any given layer!
$$\Large\nabla_{W^{(\ell)}}=\delta^{(\ell)}\cdot a^{(\ell-1)T}$$</p>
<h3 id="Final-Update-Rules">Final Update Rules<a class="anchor-link" href="#Final-Update-Rules">&#182;</a></h3><p>Based on the following update rules, we have deduced recursively the update rules for any given layer, and they are as follows:</p>
<p>$$\Large\text{The first }\delta^{(\ell)} = (p_i-\hat{y_i}) \text{ for classification, or } (\hat{y_i}-\bar{y_i}) \text{ for regression.}$$
$$\Large\nabla_{W^{(\ell)}}=\delta^{(\ell)} a^{(\ell-1)T}$$
$$\Large\nabla_{b^{(\ell)}} = \delta^{(\ell)}$$
$$\Large\delta^{(\ell-1)} = W^{(\ell)T}\delta^{(\ell)} \odot f'(z^{(\ell-1)}).$$</p>
<p>Where:</p>
<ul>
<li>$\hat{y_i}$ is the predicted value generated by your network's forward pass for any given category, for a certain sample.</li>
<li>$\bar{y_i}$ is the actual value/label given to you for any given category, for a certain sample. This will be 0/1 for classification and some real number for regression.</li>
<li>$p_i$ is the softmax function.</li>
<li>$(\ell)$ is the layer</li>
<li>$\large W^{(\ell)}$ is the weight matrix for layer $\ell$</li>
<li>$\large \delta^{(\ell)}$ is the loss for the layer $\ell$</li>
<li>Likewise, $\large \delta^{(\ell-1)}$ is the loss for the layer $\ell-1$</li>
<li>$\large z^{(\ell-1)}$ is the values calculated for the node BEFORE it is sent through the activation function for that layer</li>
<li>$\large f(z^{(\ell)})$ is the activation function for the layer corresponding to the layer of it's preactivated nodes</li>
<li>$\large f'(x)$ is the derivative of the activation function</li>
<li>$\large f'(z^{(\ell-1)})$ are the preactivated nodes from the $\ell-1$ layer, <strong>put through the DERIVATIVE of their activation.</strong> &lt;- This is common source of confusion.</li>
<li>$\odot$ is the hadamard product, aka element-wise multiplication (<strong>DIFFERENT FROM MATRIX MULTIPLICATION</strong>)</li>
<li>$^T$ is transpose, where we switch the rows to be the columns of a matrix, and the columns to be the rows.</li>
<li>$\large\delta^{(\ell)}\cdot a^{(\ell-1)T}$ is an outer product</li>
<li>$\large a^{(\ell)} = f(W^{(\ell)}\cdot a^{(\ell-1)} + b^{(\ell)}) = f(z^{(\ell)})$</li>
<li>$\large z^{(\ell)} = W^{(\ell)}\cdot a^{(\ell-1)} + b^{(\ell)}$</li>
</ul>
<p>This is definitely a lot of information, so make sure to come back to this a couple of times to be able to get it.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="82">
          <script>
            voila_process(82, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Weight-updates-(motivations-of-gradient-descent)">Weight updates (motivations of gradient descent)<a class="anchor-link" href="#Weight-updates-(motivations-of-gradient-descent)">&#182;</a></h3><p>Now that we are able to derive the gradients of the weights, we can take a step in the direction of these gradients. Naively, we can do this by subtracting the gradients from the current weights, and likewise with the biases:
$$W^{(\ell)}_{new} = W^{(\ell)} - \nabla W^{(\ell)}$$
$$b^{(\ell)}_{new} = b^{(\ell)} - \nabla b^{(\ell)}$$</p>
<p>This basic idea is how gradient descent works, however there is an important caveat: We might take too big of a step. Let's visualize how a parameter space for a loss function MIGHT look. Run the following simulation:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="83">
          <script>
            voila_process(83, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="84">
          <script>
            voila_process(84, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Do you see what happens? The gradient tells us to move quickly where it's steep, and as a result, we &quot;overshoot&quot; our minimum point of loss. We keep moving back and forth, and we will never reach the best minima.</p>
<p>Here's an analogy: Think you are holding a basketball in a dense fog, so dense that you can't see anything. This is basketball your neural network. Your goal is to throw the basketball into the hoop, but you obviously don't know where it is because you can't see You have some information telling you which direction to throw the basketball and it tells you how hard to throw the ball. Every time you throw the ball, you're automatically teleported to the ball, so you don't get to walk over the landscape to see if you can guess where to shoot. For some reason, the information telling you to throw it harder and harder when you're close to the goal. As a result, you're never able to get the ball into the hoop. You don't have all day!</p>
<p>We can see that in our simulation, the curve gets steeper the closer we get to the local minimum. As a result, the gradient tells us to throw our ball REALLY hard, because the negative gradient is steep, like we saw in the example above. So how do we fix this?</p>
<h3 id="Our-first-Hyperparameter:-$%5Ceta$-(learning-rate)-and-the-Gradient-Descent-Algorithm">Our first Hyperparameter: $\eta$ (learning rate) and the Gradient Descent Algorithm<a class="anchor-link" href="#Our-first-Hyperparameter:-$%5Ceta$-(learning-rate)-and-the-Gradient-Descent-Algorithm">&#182;</a></h3><p>We can add some small constant $\eta$ to the negative gradients to &quot;dampen&quot; their effects on the weights. This will effectively make us &quot;toss&quot; the basketball a bunch of times, so that we inch towards the goal. This kind of makes more sense, because we can't see! The classic gradient descent algorithm can therefore be written as such:</p>
<p>$$\large W^{(\ell)}_{new} = W^{(\ell)} - \eta\nabla W^{(\ell)}$$
$$\large b^{(\ell)}_{new} = b^{(\ell)} - \eta\nabla b^{(\ell)}$$</p>
<p>where</p>
<ul>
<li>$\eta$ is the learning rate. This is usually some small number, custom to every neural network problem! (You need to figure out the best one yourself)</li>
<li>$\nabla b^{(\ell)}$ is the gradient of the biases for that layer $\ell$</li>
<li>$\nabla W^{(\ell)}$ is the gradient of the Weight Matrix for that layer $\ell$</li>
<li>$W^{(\ell)}$ is the weight matrix for layer $\ell$</li>
<li>$b^{(\ell)}$ is the biases for layer $\ell$</li>
<li>$W^{(\ell)}_{new}$ is the updated weight matrix for layer $\ell$</li>
<li>$b^{(\ell)}_{new}$ is the updated biases matrix for layer $\ell$</li>
</ul>
<p>Let's see what happens when we add this constant to our simulation:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="85">
          <script>
            voila_process(85, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="86">
          <script>
            voila_process(86, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>As you can see, we converge perfectly! Conversely it's important not to make the learning rate too low, or we have the opposite problem that we saw in the first simulation:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="87">
          <script>
            voila_process(87, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="88">
          <script>
            voila_process(88, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>As you can see the ball moves so slowly that it will never reach its local minima in the time that we need it to. As a result we strain our computer, while not getting a good model.</p>
<p>Now that we know what learning rate ($\eta$) is, we now have all the tools to go through a full backpropagation example.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="89">
          <script>
            voila_process(89, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Backpropagation-Example">Backpropagation Example<a class="anchor-link" href="#Backpropagation-Example">&#182;</a></h3><p>In the meantime, let's use our new update rules to apply backpropagation to our synthetic model we had defined previously:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="90">
          <script>
            voila_process(90, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="91">
          <script>
            voila_process(91, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Layer-3">Layer 3<a class="anchor-link" href="#Layer-3">&#182;</a></h3><p>Let's recall our toolbox of update rules:
$$\text{The first }\delta^{(\ell)} = (p_i-\hat{y_i}) \text{ for classification, or } (\hat{y_i}-\bar{y_i}) \text{ for regression.}$$
$$\nabla_{W^{(\ell)}}=\delta^{(\ell)} a^{(\ell-1)T}$$
$$\nabla_{b^{(\ell)}} = \delta^{(\ell)}$$
$$\delta^{(\ell-1)} = W^{(\ell)T}\delta^{(\ell)} \odot f'(z^{(\ell-1)})$$</p>
<h4 id="Step-1:-Calculate-initial-loss">Step 1: Calculate initial loss<a class="anchor-link" href="#Step-1:-Calculate-initial-loss">&#182;</a></h4><p>Because we have specified that this is a classification problem, we know:
$$\delta^{(3)} = 
\left( \begin{array}{cccc}
p_1 - y_1 \\[0.5em]
p_2 - y_2 \\[0.5em]
p_3 - y_3\\[0.5em]
\end{array}\right)$$</p>
<h4 id="Step-2:-Calculate-the-weight-and-bias-gradients">Step 2: Calculate the weight and bias gradients<a class="anchor-link" href="#Step-2:-Calculate-the-weight-and-bias-gradients">&#182;</a></h4><p>We are now at this point on the graph:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="92">
          <script>
            voila_process(92, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="93">
          <script>
            voila_process(93, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We can calculate our weights with the update rule $\nabla_{W^{(\ell)}}=\delta^{(\ell)} a^{(\ell-1)T}$.</p>
<p>$$\Large\nabla_{W^{(3)}} = \delta^{(3)} a^{(2)T}\normalsize = 
\left( \begin{array}{cccc}
p_1 - y_1 \\[0.5em]
p_2 - y_2 \\[0.5em]
p_3 - y_3\\[0.5em]
\end{array}\right)\cdot
\left( \begin{array}{cccc}
\text{Node 1}^{(2)} \\[0.5em]
\text{Node 2}^{(2)} \\[0.5em]
\text{Node 3}^{(2)}\\[0.5em]
\text{Node 4}^{(2)}\\[0.5em]
\end{array}\right)^T =
$$
where Node $i$ is the activated value coming out of that node, or $a_i^{(2)}$.
$$\left( \begin{array}{cccc}
p_1 - y_1 \\[0.5em]
p_2 - y_2 \\[0.5em]
p_3 - y_3\\[0.5em]
\end{array}\right)\cdot
\left( \begin{array}{cccc}
\text{Node 1}^{(2)} &amp; \text{Node 2}^{(2)}&amp; \text{Node 3}^{(2)}&amp; \text{Node 4}^{(2)}\\[0.5em]
\end{array}\right) =
$$
$$\left( \begin{array}{cccc}
(p_1 - y_1)\cdot\text{Node 1}^{(2)} &amp; (p_1 - y_1)\cdot\text{Node 2}^{(2)} &amp; (p_1 - y_1)\cdot\text{Node 3}^{(2)} &amp; (p_1 - y_1)\cdot\text{Node 4}^{(2)} \\[0.5em]
(p_2 - y_2)\cdot\text{Node 1}^{(2)} &amp; (p_2 - y_2)\cdot\text{Node 2}^{(2)} &amp; (p_2 - y_2)\cdot\text{Node 3}^{(2)} &amp; (p_2 - y_2)\cdot\text{Node 4}^{(2)} \\[0.5em]
(p_3 - y_3)\cdot\text{Node 1}^{(2)} &amp; (p_3 - y_3)\cdot\text{Node 2}^{(2)} &amp; (p_3 - y_3)\cdot\text{Node 3}^{(2)} &amp; (p_3 - y_3)\cdot\text{Node 4}^{(2)}
\end{array}\right) 
$$</p>
<p>We calculate our bias to be via the update rule:</p>
<p>$$
\Large\nabla_{b^{(3)}} = \delta^{(3)} \normalsize= 
\left( \begin{array}{cccc}
p_1 - y_1 \\[0.5em]
p_2 - y_2 \\[0.5em]
p_3 - y_3\\[0.5em]
\end{array}\right)
$$</p>
<h4 id="Step-3:-Perform-Gradient-descent:">Step 3: Perform Gradient descent:<a class="anchor-link" href="#Step-3:-Perform-Gradient-descent:">&#182;</a></h4><p>$$\large W^{(3)}_{new} = W^{(3)} - \eta\nabla W^{(3)}$$
$$\Large W^{(3)} \normalsize= \left( \begin{array}{cccc}
w_{1,1}^{(3)}&amp; w_{1,2}^{(3)} &amp; w_{1,3}^{(3)} &amp; w_{1,4}^{(3)}\\[0.5em]
w_{2,1}^{(3)}&amp; w_{2,2}^{(3)} &amp; w_{2,3}^{(3)} &amp; w_{2,4}^{(3)}\\[0.5em]
w_{3,1}^{(3)}&amp; w_{3,2}^{(3)} &amp; w_{3,3}^{(3)} &amp; w_{3,4}^{(3)}\\[0.5em]
\end{array}\right) $$
$$\large\nabla_{W^{(3)}} \normalsize = 
\left( \begin{array}{cccc}
(p_1 - y_1)\cdot\text{Node 1}^{(2)} &amp; (p_1 - y_1)\cdot\text{Node 2}^{(2)} &amp; (p_1 - y_1)\cdot\text{Node 3}^{(2)} &amp; (p_1 - y_1)\cdot\text{Node 4}^{(2)} \\[0.5em]
(p_2 - y_2)\cdot\text{Node 1}^{(2)} &amp; (p_2 - y_2)\cdot\text{Node 2}^{(2)} &amp; (p_2 - y_2)\cdot\text{Node 3}^{(2)} &amp; (p_2 - y_2)\cdot\text{Node 4}^{(2)} \\[0.5em]
(p_3 - y_3)\cdot\text{Node 1}^{(2)} &amp; (p_3 - y_3)\cdot\text{Node 2}^{(2)} &amp; (p_3 - y_3)\cdot\text{Node 3}^{(2)} &amp; (p_3 - y_3)\cdot\text{Node 4}^{(2)}
\end{array}\right)
$$</p>
<p>$$\large b^{(3)}_{new} = b^{(3)} - \eta\nabla b^{(3)}$$
$$\large b^{(3)} \normalsize = 
\left( \begin{array}{cccc}
b_{1}^{(3)} \\[0.5em]
b_{2}^{(3)} \\[0.5em]
b_{3}^{(3)} \\[0.5em]
\end{array}\right) 
$$
$$\large\nabla b^{(3)}=\delta^{(3)} \normalsize
\left( \begin{array}{cccc}
p_1 - y_1 \\[0.5em]
p_2 - y_2 \\[0.5em]
p_3 - y_3\\[0.5em]
\end{array}\right)$$</p>
<p>$$\large W^{(3)}_{new} = \normalsize\left( \begin{array}{cccc}
w_{1,1}^{(3)}&amp; w_{1,2}^{(3)} &amp; w_{1,3}^{(3)} &amp; w_{1,4}^{(3)}\\[0.5em]
w_{2,1}^{(3)}&amp; w_{2,2}^{(3)} &amp; w_{2,3}^{(3)} &amp; w_{2,4}^{(3)}\\[0.5em]
w_{3,1}^{(3)}&amp; w_{3,2}^{(3)} &amp; w_{3,3}^{(3)} &amp; w_{3,4}^{(3)}\\[0.5em]
\end{array}\right) - \eta\left( \begin{array}{cccc}
(p_1 - y_1)\cdot\text{Node 1}^{(2)} &amp; (p_1 - y_1)\cdot\text{Node 2}^{(2)} &amp; (p_1 - y_1)\cdot\text{Node 3}^{(2)} &amp; (p_1 - y_1)\cdot\text{Node 4}^{(2)} \\[0.5em]
(p_2 - y_2)\cdot\text{Node 1}^{(2)} &amp; (p_2 - y_2)\cdot\text{Node 2}^{(2)} &amp; (p_2 - y_2)\cdot\text{Node 3}^{(2)} &amp; (p_2 - y_2)\cdot\text{Node 4}^{(2)} \\[0.5em]
(p_3 - y_3)\cdot\text{Node 1}^{(2)} &amp; (p_3 - y_3)\cdot\text{Node 2}^{(2)} &amp; (p_3 - y_3)\cdot\text{Node 3}^{(2)} &amp; (p_3 - y_3)\cdot\text{Node 4}^{(2)}
\end{array}\right)$$</p>
<p>$$\large b^{(3)}_{new}\normalsize = 
\left( \begin{array}{cccc}
b_{1}^{(3)} \\[0.5em]
b_{2}^{(3)} \\[0.5em]
b_{3}^{(3)} \\[0.5em]
\end{array}\right) 
-\eta
\left( \begin{array}{cccc}
p_1 - y_1 \\[0.5em]
p_2 - y_2 \\[0.5em]
p_3 - y_3\\[0.5em]
\end{array}\right)$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="94">
          <script>
            voila_process(94, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Layer-2:-Calculate-Loss-for-the-New-Layer">Layer 2: Calculate Loss for the New Layer<a class="anchor-link" href="#Layer-2:-Calculate-Loss-for-the-New-Layer">&#182;</a></h4><p>We are now here in our neural network:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="95">
          <script>
            voila_process(95, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="96">
          <script>
            voila_process(96, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-1:-Calculate-loss">Step 1: Calculate loss<a class="anchor-link" href="#Step-1:-Calculate-loss">&#182;</a></h4><p>This part is a little tricky, but just remember our update rules. You can find activation functions and their derivatives above. Remember $a^{(\ell)}$ does NOT refer to the activation function, but rather the number we get from it. I use Node i and  $a_i^{(\ell)}$ interchangeably. Our update rule is as follows: $\delta^{(2)} = W^{(3)T}\delta^{(3)} \odot f'(z^{(2)})$</p>
<p>$$\large \delta^{(2)} \normalsize=  \left( \begin{array}{cccc}
w_{1,1}^{(3)}&amp; w_{1,2}^{(3)} &amp; w_{1,3}^{(3)} &amp; w_{1,4}^{(3)}\\[0.5em]
w_{2,1}^{(3)}&amp; w_{2,2}^{(3)} &amp; w_{2,3}^{(3)} &amp; w_{2,4}^{(3)}\\[0.5em]
w_{3,1}^{(3)}&amp; w_{3,2}^{(3)} &amp; w_{3,3}^{(3)} &amp; w_{3,4}^{(3)}\\[0.5em]
\end{array}\right)^T
\left( \begin{array}{cccc}
p_1 - y_1 \\[0.5em]
p_2 - y_2 \\[0.5em]
p_3 - y_3\\[0.5em]
\end{array}\right)\odot
f'\left( \begin{array}{cccc}
z_{1}^{(2)} \\[0.5em]
z_{2}^{(2)} \\[0.5em]
z_{3}^{(2)} \\[0.5em]
z_{4}^{(2)} \\[0.5em]
\end{array}\right)
$$</p>
<p>$$\large \delta^{(2)} =  \left( \begin{array}{ccc}
w_{1,1}^{(3)} &amp; w_{2,1}^{(3)} &amp; w_{3,1}^{(3)} \\[0.5em]
w_{1,2}^{(3)} &amp; w_{2,2}^{(3)} &amp; w_{3,2}^{(3)} \\[0.5em]
w_{1,3}^{(3)} &amp; w_{2,3}^{(3)} &amp; w_{3,3}^{(3)} \\[0.5em]
w_{1,4}^{(3)} &amp; w_{2,4}^{(3)} &amp; w_{3,4}^{(3)}
\end{array}\right)
\left( \begin{array}{cccc}
p_1 - y_1 \\[0.5em]
p_2 - y_2 \\[0.5em]
p_3 - y_3\\[0.5em]
\end{array}\right)\odot
f'\left( \begin{array}{cccc}
z_{1}^{(2)}\\[0.5em]
z_{2}^{(2)}\\[0.5em]
z_{3}^{(2)}\\[0.5em]
z_{4}^{(2)}\\[0.5em]
\end{array}\right)
$$</p>
<p>to simplify this section, we will simplify each matrix knowing their constituent parts. We know for our $\delta^{(3)}, p_i - y_i$ is just some positive or negative in $-1\leq0\leq1$. This number can be represented as such: $\delta_i^{(3)}$. So we let $\delta^{(3)}$ =
$$
\left( \begin{array}{cccc}
\delta_1^{(3)}\\[0.5em]
\delta_2^{(3)} \\[0.5em]
\delta_3^{(3)}\\[0.5em]
\end{array}\right)
$$</p>
<p>then
$$W^{(3)T}\delta^{(3)} = \left( \begin{array}{ccc}
w_{1,1}^{(3)} &amp; w_{2,1}^{(3)} &amp; w_{3,1}^{(3)} \\[0.5em]
w_{1,2}^{(3)} &amp; w_{2,2}^{(3)} &amp; w_{3,2}^{(3)} \\[0.5em]
w_{1,3}^{(3)} &amp; w_{2,3}^{(3)} &amp; w_{3,3}^{(3)} \\[0.5em]
w_{1,4}^{(3)} &amp; w_{2,4}^{(3)} &amp; w_{3,4}^{(3)}
\end{array}\right)\left( \begin{array}{cccc}
\delta_1^{(3)}\\[0.5em]
\delta_2^{(3)} \\[0.5em]
\delta_3^{(3)}\\[0.5em]
\end{array}\right)\odot
f'\left( \begin{array}{cccc}
z_{1}^{(2)} \\[0.5em]
z_{2}^{(2)} \\[0.5em]
z_{3}^{(2)} \\[0.5em]
z_{4}^{(2)} \\[0.5em]
\end{array}\right) = 
$$
$$
\left(\begin{array}{cccc}
 w_{1,1}^{(3)}\delta_1^{(3)} + w_{2,1}^{(3)}\delta_2^{(3)} + w_{3,1}^{(3)}\delta_3^{(3)} \\[0.5em]
w_{1,2}^{(3)}\delta_1^{(3)} + w_{2,2}^{(3)}\delta_2^{(3)} + w_{3,2}^{(3)}\delta_3^{(3)} \\[0.5em]
w_{1,3}^{(3)}\delta_1^{(3)} + w_{2,3}^{(3)}\delta_2^{(3)} + w_{3,3}^{(3)}\delta_3^{(3)} \\[0.5em]
w_{1,4}^{(3)}\delta_1^{(3)} + w_{2,4}^{(3)}\delta_2^{(3)} + w_{3,4}^{(3)}\delta_3^{(3)} \\[0.5em]
\end{array}\right)\odot
f'\left( \begin{array}{cccc}
z_{1}^{(2)} \\[0.5em]
z_{2}^{(2)} \\[0.5em]
z_{3}^{(2)} \\[0.5em]
z_{4}^{(2)} \\[0.5em]
\end{array}\right)
$$</p>
<p>Where f'(x) is the DERIVATIVE of some <a href="#types-of-activation-functions">activation function</a>. We then get that
$$\Large \delta^{(2)} = \normalsize
\left(\begin{array}{cccc}
(w_{1,1}^{(3)}\delta_1^{(3)} + w_{2,1}^{(3)}\delta_2^{(3)} + w_{3,1}^{(3)}\delta_3^{(3)})*f'(z_{1}^{(2)})\\[0.5em]
(w_{1,2}^{(3)}\delta_1^{(3)} + w_{2,2}^{(3)}\delta_2^{(3)} + w_{3,2}^{(3)}\delta_3^{(3)})*f'(z_{2}^{(2)}) \\[0.5em]
(w_{1,3}^{(3)}\delta_1^{(3)} + w_{2,3}^{(3)}\delta_2^{(3)} + w_{3,3}^{(3)}\delta_3^{(3)})*f'(z_{3}^{(2)})\\[0.5em]
(w_{1,4}^{(3)}\delta_1^{(3)} + w_{2,4}^{(3)}\delta_2^{(3)} + w_{3,4}^{(3)}\delta_3^{(3)})*f'(z_{4}^{(2)}) \\[0.5em]
\end{array}\right)
$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="97">
          <script>
            voila_process(97, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>notice how each row is a single number. We can simplify this as
$$\Large \delta^{(2)}\normalsize =
\left(\begin{array}{cccc}
(w_{1,1}^{(3)}\delta_1^{(3)} + w_{2,1}^{(3)}\delta_2^{(3)} + w_{3,1}^{(3)}\delta_3^{(3)})*f'(z_{1}^{(2)})\\[0.5em]
(w_{1,2}^{(3)}\delta_1^{(3)} + w_{2,2}^{(3)}\delta_2^{(3)} + w_{3,2}^{(3)}\delta_3^{(3)})*f'(z_{2}^{(2)}) \\[0.5em]
(w_{1,3}^{(3)}\delta_1^{(3)} + w_{2,3}^{(3)}\delta_2^{(3)} + w_{3,3}^{(3)}\delta_3^{(3)})*f'(z_{3}^{(2)})\\[0.5em]
(w_{1,4}^{(3)}\delta_1^{(3)} + w_{2,4}^{(3)}\delta_2^{(3)} + w_{3,4}^{(3)}\delta_3^{(3)})*f'(z_{4}^{(2)}) \\[0.5em]
\end{array}\right)=
\left( \begin{array}{cccc}
\delta_1^{(2)}\\[0.5em]
\delta_2^{(2)} \\[0.5em]
\delta_3^{(2)}\\[0.5em]
\delta_4^{(2)}\\[0.5em]
\end{array}\right)
$$
We will use this matrix for our further calculations to avoid confusion.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="98">
          <script>
            voila_process(98, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We are now here in our backpropagation:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="99">
          <script>
            voila_process(99, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="100">
          <script>
            voila_process(100, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We will now go a little faster, now that we've seen the process.</p>
<h4 id="Step-2:-Calculate-$%5Cnabla_%7BW%5E%7B(2)%7D%7D$-and-$%5Cnabla_%7Bb%5E%7B(2)%7D%7D$">Step 2: Calculate $\nabla_{W^{(2)}}$ and $\nabla_{b^{(2)}}$<a class="anchor-link" href="#Step-2:-Calculate-$%5Cnabla_%7BW%5E%7B(2)%7D%7D$-and-$%5Cnabla_%7Bb%5E%7B(2)%7D%7D$">&#182;</a></h4><p>Our update rule for this layer is $\nabla_{W^{(2)}}=\delta^{(2)} a^{(1)T}$ and for bias $\nabla_{b^{(2)}}=\delta^{(2)}$.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="101">
          <script>
            voila_process(101, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Our weight gradient
$$\Large\nabla_{W^{(2)}} = \delta^{(2)} a^{(1)T}\normalsize = 
\left( \begin{array}{cccc}
\delta_1^{(2)} \\[0.5em]
\delta_2^{(2)} \\[0.5em]
\delta_3^{(2)} \\[0.5em]
\delta_4^{(2)} \\[0.5em]
\end{array}\right)\cdot
\left( \begin{array}{cccc}
\text{Node 1}^{(1)} \\[0.5em]
\text{Node 2}^{(1)} \\[0.5em]
\end{array}\right)^T =
$$
$$\left( \begin{array}{cccc}
\delta_1^{(2)} \\[0.5em]
\delta_2^{(2)} \\[0.5em]
\delta_3^{(2)} \\[0.5em]
\delta_4^{(2)} \\[0.5em]
\end{array}\right)\cdot
\left( \begin{array}{cccc}
\text{Node 1}^{(1)} &amp; \text{Node 2}^{(1)}\\[0.5em]
\end{array}\right) =
$$
$$\left( \begin{array}{cc}
\delta_1^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_1^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\delta_2^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_2^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\delta_3^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_3^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\delta_4^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_4^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\end{array}\right)
$$</p>
<p>Calculating Bias gradient:</p>
<p>$$\nabla_{b^{(2)}}=\delta^{(2)} =\left( \begin{array}{cccc}
\delta_1^{(2)} \\[0.5em]
\delta_2^{(2)} \\[0.5em]
\delta_3^{(2)} \\[0.5em]
\delta_4^{(2)} \\[0.5em]
\end{array}\right) $$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="102">
          <script>
            voila_process(102, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-3:-Update-weights-and-biases-for-layer-2-with-gradient-descent">Step 3: Update weights and biases for layer 2 with gradient descent<a class="anchor-link" href="#Step-3:-Update-weights-and-biases-for-layer-2-with-gradient-descent">&#182;</a></h4><p>for weights:
$$\Large W^{(2)} \normalsize = \left( \begin{array}{cc}
w_{1,1}^{(2)} &amp; w_{1,2}^{(2)} \\[0.5em]
w_{2,1}^{(2)} &amp; w_{2,2}^{(2)} \\[0.5em]
w_{3,1}^{(2)} &amp; w_{3,2}^{(2)} \\[0.5em]
w_{4,1}^{(2)} &amp; w_{4,2}^{(2)} \\[0.5em]
\end{array}\right)
$$
$$\Large\nabla_{W^{(2)}} \normalsize = \left( \begin{array}{cc}
\delta_1^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_1^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\delta_2^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_2^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\delta_3^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_3^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\delta_4^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_4^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\end{array}\right)
$$</p>
<p>for bias:
$$b^{(2)} = \left( \begin{array}{cccc}
b_1^{(2)} \\[0.5em]
b_2^{(2)} \\[0.5em]
b_3^{(2)} \\[0.5em]
b_4^{(2)} \\[0.5em]
\end{array}\right)$$
$$\nabla_{b^{(2)}}=\delta^{(2)} =\left( \begin{array}{cccc}
\delta_1^{(2)} \\[0.5em]
\delta_2^{(2)} \\[0.5em]
\delta_3^{(2)} \\[0.5em]
\delta_4^{(2)} \\[0.5em]
\end{array}\right) $$</p>
<p>step:
$$
\Large W_{new}^{(2)} \normalsize =
\left( \begin{array}{cc}
w_{1,1}^{(2)} &amp; w_{1,2}^{(2)} \\[0.5em]
w_{2,1}^{(2)} &amp; w_{2,2}^{(2)} \\[0.5em]
w_{3,1}^{(2)} &amp; w_{3,2}^{(2)} \\[0.5em]
w_{4,1}^{(2)} &amp; w_{4,2}^{(2)} \\[0.5em]
\end{array}\right) - \eta\left( \begin{array}{cc}
\delta_1^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_1^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\delta_2^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_2^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\delta_3^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_3^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\delta_4^{(2)}\cdot\text{Node 1}^{(1)} &amp; \delta_4^{(2)}\cdot\text{Node 2}^{(1)} \\[0.5em]
\end{array}\right)
$$</p>
<p>$$
\Large b_{new}^{(2)} \normalsize =
b^{(2)} = \left( \begin{array}{cccc}
b_1^{(2)} \\[0.5em]
b_2^{(2)} \\[0.5em]
b_3^{(2)} \\[0.5em]
b_4^{(2)} \\[0.5em]
\end{array}\right) - \eta
\left( \begin{array}{cccc}
\delta_1^{(2)} \\[0.5em]
\delta_2^{(2)} \\[0.5em]
\delta_3^{(2)} \\[0.5em]
\delta_4^{(2)} \\[0.5em]
\end{array}\right)
$$</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="103">
          <script>
            voila_process(103, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Layer-1">Layer 1<a class="anchor-link" href="#Layer-1">&#182;</a></h3><p>We are now here in our diagram:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="104">
          <script>
            voila_process(104, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="105">
          <script>
            voila_process(105, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-1:-Calculate-loss-for-the-new-layer">Step 1: Calculate loss for the new layer<a class="anchor-link" href="#Step-1:-Calculate-loss-for-the-new-layer">&#182;</a></h4><p>Our update rule for this layer's loss is $\delta^{(1)} = W^{(2)T}\delta^{(2)} \odot f'(z^{(1)})$
$$
\Large \delta^{(1)} = \normalsize\left( \begin{array}{cc}
w_{1,1}^{(2)} &amp; w_{1,2}^{(2)} \\[0.5em]
w_{2,1}^{(2)} &amp; w_{2,2}^{(2)} \\[0.5em]
w_{3,1}^{(2)} &amp; w_{3,2}^{(2)} \\[0.5em]
w_{4,1}^{(2)} &amp; w_{4,2}^{(2)} \\[0.5em]
\end{array}\right)^T\left( \begin{array}{cccc}
\delta_1^{(2)} \\[0.5em]
\delta_2^{(2)} \\[0.5em]
\delta_3^{(2)} \\[0.5em]
\delta_4^{(2)} \\[0.5em]
\end{array}\right) \odot
f'\left( \begin{array}{cccc}
z_{1}^{(1)} \\[0.5em]
z_{2}^{(1)} \\[0.5em]
\end{array}\right) = 
$$
$$
\left( \begin{array}{cccc}
w_{1,1}^{(2)} &amp; w_{2,1}^{(2)} &amp; w_{3,1}^{(2)} &amp; w_{4,1}^{(2)} \\[0.5em]
w_{1,2}^{(2)} &amp; w_{2,2}^{(2)} &amp; w_{3,2}^{(2)} &amp; w_{4,2}^{(2)} \\[0.5em]
\end{array} \right)\left( \begin{array}{cccc}
\delta_1^{(2)} \\[0.5em]
\delta_2^{(2)} \\[0.5em]
\delta_3^{(2)} \\[0.5em]
\delta_4^{(2)} \\[0.5em]
\end{array}\right) \odot
f'\left( \begin{array}{cccc}
z_{1}^{(1)} \\[0.5em]
z_{2}^{(1)} \\[0.5em]
\end{array}\right) = 
$$
$$
\left( \begin{array}{c}
w_{1,1}^{(2)}\cdot\delta_1^{(2)} + w_{2,1}^{(2)}\cdot\delta_2^{(2)} + w_{3,1}^{(2)}\cdot\delta_3^{(2)} + w_{4,1}^{(2)}\cdot\delta_4^{(2)} \\[0.5em]
w_{1,2}^{(2)}\cdot\delta_1^{(2)} + w_{2,2}^{(2)}\cdot\delta_2^{(2)} + w_{3,2}^{(2)}\cdot\delta_3^{(2)} + w_{4,2}^{(2)}\cdot\delta_4^{(2)} \\[0.5em]
\end{array} \right) \odot
f'\left( \begin{array}{cccc}
z_{1}^{(1)} \\[0.5em]
z_{2}^{(1)} \\[0.5em]
\end{array}\right) = 
$$
$$
\left( \begin{array}{c}
(w_{1,1}^{(2)}\cdot\delta_1^{(2)} + w_{2,1}^{(2)}\cdot\delta_2^{(2)} + w_{3,1}^{(2)}\cdot\delta_3^{(2)} + w_{4,1}^{(2)}\cdot\delta_4^{(2)})*f'(z_{1}^{(1)}) \\[0.5em]
(w_{1,2}^{(2)}\cdot\delta_1^{(2)} + w_{2,2}^{(2)}\cdot\delta_2^{(2)} + w_{3,2}^{(2)}\cdot\delta_3^{(2)} + w_{4,2}^{(2)}\cdot\delta_4^{(2)})*f'(z_{2}^{(1)}) \\[0.5em]
\end{array} \right) = 
\left( \begin{array}{cccc}
\delta_{1}^{(1)} \\[0.5em]
\delta_{2}^{(1)} \\[0.5em]
\end{array}\right)
$$</p>
<p>We are now here in our neural network:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="106">
          <script>
            voila_process(106, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="107">
          <script>
            voila_process(107, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>This means that these are the last set of weights we have to compute!</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="108">
          <script>
            voila_process(108, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Step-2:-Compute-$%5Cnabla_%7BW%5E%7B(1)%7D%7D$-and-$%5Cnabla_%7Bb%5E%7B(1)%7D%7D$">Step 2: Compute $\nabla_{W^{(1)}}$ and $\nabla_{b^{(1)}}$<a class="anchor-link" href="#Step-2:-Compute-$%5Cnabla_%7BW%5E%7B(1)%7D%7D$-and-$%5Cnabla_%7Bb%5E%7B(1)%7D%7D$">&#182;</a></h4><p>Our update rule for this layer is $\nabla_{W^{(1)}}=\delta^{(1)} a^{(0)T}$ and for bias $\nabla_{b^{(1)}}=\delta^{(1)}$. Notice that $a^{(0)}$ corresponds to the input layer. Because they're just the inputs, and neural networks do not apply activations on inputs before going through the network. This means that
$$a^{(0)} = x = 
\left( \begin{array}{cccc}
x_{1} \\[0.5em]
x_{2} \\[0.5em]
\end{array}\right)$$</p>
<p>then our new weight gradient</p>
<p>$$
\Large\nabla_{W^{(1)}}\normalsize = 
\left( \begin{array}{cccc}
\delta_1^{(1)}\\[0.5em]
\delta_2^{(1)}\\[0.5em]
\end{array}\right)
\left( \begin{array}{cccc}
x_{1} \\[0.5em]
x_{2} \\[0.5em]
\end{array}\right)^T = 
$$
$$
\left( \begin{array}{cccc}
\delta_1^{(1)}\\[0.5em]
\delta_2^{(1)}\\[0.5em]
\end{array}\right)
\left( \begin{array}{cccc}
x_{1} &amp; x_{2}\\[0.5em]
\end{array}\right) = 
\left( \begin{array}{cccc}
\delta_1^{(1)}x_1 &amp;\delta_1^{(1)}x_2\\[0.5em]
\delta_2^{(1)}x_1 &amp;\delta_2^{(1)}x_2 \\[0.5em]
\end{array}\right)
$$</p>
<p>our new bias gradient
$$\Large \nabla_{b^{(1)}}=\delta^{(1)}\normalsize = \left( \begin{array}{cccc}
\delta_1^{(1)}\\[0.5em]
\delta_2^{(1)}\\[0.5em]
\end{array}\right)$$</p>
<h4 id="Step-3:-We-perform-our-final-gradient-for-the-sample">Step 3: We perform our final gradient for the sample<a class="anchor-link" href="#Step-3:-We-perform-our-final-gradient-for-the-sample">&#182;</a></h4><p>for weights:
$$
\Large W^{(1)} \normalsize  =\left( \begin{array}{cc}
w_{1,1}^{(1)} &amp; w_{1,2}^{(1)} \\[0.5em]
w_{2,1}^{(1)} &amp; w_{2,2}^{(1)} \\[0.5em]
\end{array}\right)
$$
$$
\Large \nabla{W^{(1)}} \normalsize = \left( \begin{array}{cccc}
\delta_1^{(1)}x_1 &amp;\delta_1^{(1)}x_2\\[0.5em]
\delta_2^{(1)}x_1 &amp;\delta_2^{(1)}x_2 \\[0.5em]
\end{array}\right)
$$
for biases
$$
\Large b^{(1)} \normalsize  = \left( \begin{array}{cc}
b_{1}^{(1)}\\[0.5em]
b_{2}^{(1)}\\[0.5em]
\end{array}\right)
$$
$$
\Large \nabla_{b^{(1)}}=\delta^{(1)}\normalsize = \left( \begin{array}{cccc}
\delta_1^{(1)}\\[0.5em]
\delta_2^{(1)}\\[0.5em]
\end{array}\right)
$$</p>
<p>Gradient Descent:</p>
<p>$$
W_{new}^{(1)} = \left( \begin{array}{cc}
w_{1,1}^{(1)} &amp; w_{1,2}^{(1)} \\[0.5em]
w_{2,1}^{(1)} &amp; w_{2,2}^{(1)} \\[0.5em]
\end{array}\right) - \eta\left( \begin{array}{cccc}
\delta_1^{(1)}x_1 &amp;\delta_1^{(1)}x_2\\[0.5em]
\delta_2^{(1)}x_1 &amp;\delta_2^{(1)}x_2 \\[0.5em]
\end{array}\right)
$$</p>
<p>$$
b_{new}^{(1)} = 
\left( \begin{array}{cc}
b_{1}^{(1)}\\[0.5em]
b_{2}^{(1)}\\[0.5em]
\end{array}\right) - \eta\left( \begin{array}{cccc}
\delta_1^{(1)}\\[0.5em]
\delta_2^{(1)}\\[0.5em]
\end{array}\right)
$$</p>
<p>We have now finished backpropagation:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="109">
          <script>
            voila_process(109, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="110">
          <script>
            voila_process(110, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Closing-Remarks-on-Backpropagation:-Tips-and-tricks">Closing Remarks on Backpropagation: Tips and tricks<a class="anchor-link" href="#Closing-Remarks-on-Backpropagation:-Tips-and-tricks">&#182;</a></h3><h4 id="For-regression">For regression<a class="anchor-link" href="#For-regression">&#182;</a></h4><p>If you want to do the same with regression, your last weight layer will simply be a 4x1 matrix instead, with no activation. This means that your loss $\delta^{(3)}$ would simply be 1 value, $\hat{y}-\bar{y}$, which makes sense because you would only have 1 node, essentially a 1x1 matrix. Make sure you line up the matrices perfectly for this case. I highly recommend trying these problems out with concrete numbers, as I will only go over theory here. Feed problems into ChatGPT, and ask for small regression or classification networks with 1 or 2 hidden layers to feed forward and backpropagate through</p>
<h4 id="A-caveat-for-ReLU">A caveat for ReLU<a class="anchor-link" href="#A-caveat-for-ReLU">&#182;</a></h4><p>ReLU is a weird function in such that it is &quot;technically&quot; non-differentiable. While however, with coding neural networks, we defined ReLU and its derivative to be simply 0 at the cusp, so that we maintain some sort of differentiability. We can do this by exploiting if else statements, like that of <code>np.where()</code> in the numpy package.</p>
<h4 id="A-common-mistake">A common mistake<a class="anchor-link" href="#A-common-mistake">&#182;</a></h4><p>People generally tend to mix up the activation function and its derivative. Remember that for forward pass, we use the activation function, and backwards pass, when computing the loss we use the derivative of the activation function. This might be easier to understand if we visualize it. Let's come up with some synthetic values for our previous nueral network for the following layer:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="111">
          <script>
            voila_process(111, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="112">
          <script>
            voila_process(112, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Recall ReLU:
$$
\operatorname{ReLU}(x) = \begin{cases}
x, &amp; \text{if } x \geq 0, \\
0, &amp; \text{if } x &lt; 0.
\end{cases}
$$
$$
\operatorname{ReLU'}(x) = \begin{cases}
1, &amp; \text{if } x \geq 0, \\
0, &amp; \text{if } x &lt; 0.
\end{cases}
$$</p>
<p>lets assume we calculated $z^{(2)}$ to be:</p>
<p>$$
z^{(2)} = 
\left( \begin{array}{cc}
1.35\\[0.5em]
-4.30\\[0.5em]
6.78\\[0.5em]
-.32\\[0.5em]
\end{array}\right)
$$
then  $a^{(1)} =  f(z^{(2)}) = $
$$
\left( \begin{array}{cc}
1.35\\[0.5em]
0\\[0.5em]
6.78\\[0.5em]
0\\[0.5em]
\end{array}\right)
$$</p>
<p><strong>**It is extremely important to understand that $a^{(1)} \neq f'(z^{(1)})$. Remember that $a^{(1)} = f(z^{(1)})$. You can not use the derivative of $z^{(1)}$ to compute $z^{(1)}$. This is another extremely common mistake**</strong> This is how they are different:</p>
<p>$$
\text{ where } z^{(2)} = 
\left( \begin{array}{cc}
1.35\\[0.5em]
-4.30\\[0.5em]
6.78\\[0.5em]
-.32\\[0.5em]
\end{array}\right)
$$</p>
<p>our forward pass (activated values for nodes 1, 2, 3, and 4 would be as such)
$$
f(z^{(2)}) = \left( \begin{array}{cc}
1.35\\[0.5em]
0\\[0.5em]
6.78\\[0.5em]
0\\[0.5em]
\end{array}\right)
$$</p>
<p>Notice the difference:
$$
f'(z^{(2)}) = \left( \begin{array}{cc}
1\\[0.5em]
0\\[0.5em]
1\\[0.5em]
0\\[0.5em]
\end{array}\right)
$$</p>
<p>Let's look at our backpropagation example. with $z^{(2)}$ This means we would be located here in our network:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="113">
          <script>
            voila_process(113, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="114">
          <script>
            voila_process(114, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Keep this in mind. You use $f(z^{(2)})$ in forward pass and $f'(z^{(2)})$ in backpropagation. This goes for all of the other differentiable activations too. When we compute the hadamard product when doing our backpropagation example, you see how this becomes</p>
<p>$$\Large \delta^{(2)}\normalsize =
\left(\begin{array}{cccc}
(w_{1,1}^{(3)}\delta_1^{(3)} + w_{2,1}^{(3)}\delta_2^{(3)} + w_{3,1}^{(3)}\delta_3^{(3)})*f'(z_{1}^{(2)})\\[0.5em]
(w_{1,2}^{(3)}\delta_1^{(3)} + w_{2,2}^{(3)}\delta_2^{(3)} + w_{3,2}^{(3)}\delta_3^{(3)})*f'(z_{2}^{(2)}) \\[0.5em]
(w_{1,3}^{(3)}\delta_1^{(3)} + w_{2,3}^{(3)}\delta_2^{(3)} + w_{3,3}^{(3)}\delta_3^{(3)})*f'(z_{3}^{(2)})\\[0.5em]
(w_{1,4}^{(3)}\delta_1^{(3)} + w_{2,4}^{(3)}\delta_2^{(3)} + w_{3,4}^{(3)}\delta_3^{(3)})*f'(z_{4}^{(2)}) \\[0.5em]
\end{array}\right) = 
$$
$$
\left(\begin{array}{cccc}
(w_{1,1}^{(3)}\delta_1^{(3)} + w_{2,1}^{(3)}\delta_2^{(3)} + w_{3,1}^{(3)}\delta_3^{(3)})*1\\[0.5em]
(w_{1,2}^{(3)}\delta_1^{(3)} + w_{2,2}^{(3)}\delta_2^{(3)} + w_{3,2}^{(3)}\delta_3^{(3)})*0 \\[0.5em]
(w_{1,3}^{(3)}\delta_1^{(3)} + w_{2,3}^{(3)}\delta_2^{(3)} + w_{3,3}^{(3)}\delta_3^{(3)})*1\\[0.5em]
(w_{1,4}^{(3)}\delta_1^{(3)} + w_{2,4}^{(3)}\delta_2^{(3)} + w_{3,4}^{(3)}\delta_3^{(3)})*0 \\[0.5em]
\end{array}\right) $$</p>
<p>Feel free to refer back here, as you may make this mistake in the future.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="115">
          <script>
            voila_process(115, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="From-Matrices-to-Tensors:-The-Need-for-Batches:">From Matrices to Tensors: The Need for Batches:<a class="anchor-link" href="#From-Matrices-to-Tensors:-The-Need-for-Batches:">&#182;</a></h2><hr />
<p>The example that we had just performed forward pass and backpropagation on was for but one sample in our dataset. For the following, we can model our neural network again, to show you what I mean:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="116">
          <script>
            voila_process(116, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="117">
          <script>
            voila_process(117, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p><strong>Tip:</strong> For this model, drag with your cursor to see a 3d view.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="118">
          <script>
            voila_process(118, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>You can see we have 2 inputs, and then 3 possible categories to choose from. This is all for one sample in a dataset that may contain millions. We essentially performed <strong>stochastic gradient descent.</strong> The intuition is that every single sample individually, then use that to step in the direction of the local minima. This is called stochastic gradient descent. Meanwhile, normal gradient descent is when you compute the gradient with to EVERY SINGLE SAMPLE AT THE SAME TIME, then averaging them. For a dataset with a million datapoints, and neural networks with 100+ nodes per layer, you see how inefficient this becomes (you'd brick your computer!).</p>
<p>However, something to note is that every sample might deviate a little bit from its idealistic prediction. This introduces noise when performing gradient descent, and as a result we do not travel directly into the local minima. Let's look at the following example:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="119">
          <script>
            voila_process(119, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="120">
          <script>
            voila_process(120, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>You see the issue here? There is a lot of noise when we are trying to compute the gradient, since one sample, while it can roughly approximate the gradient for every batch, will add noise to every step. We then &quot;deviate a little bit&quot;, so that we can never reach the bottom of the local minima. So how do we solve this problem?</p>
<h3 id="Solution:-batched-gradient-descent-(hyperparameter-2)">Solution: batched gradient descent (hyperparameter 2)<a class="anchor-link" href="#Solution:-batched-gradient-descent-(hyperparameter-2)">&#182;</a></h3><p>Let's calculate maybe only 10 samples at the same time, average them, and then update the weights accordingly. We can build a neural network that looks like this:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="121">
          <script>
            voila_process(121, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="122">
          <script>
            voila_process(122, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>You see how now, we can input 10 samples of our 2 feature dataset at a time? We then average them, and then compute the gradients with respect to the batch. We essentially have 10 neural networks running in parallel, which we will call a &quot;slice&quot; for the purpose of this demonstration.</p>
<p>Let's let $W$ denote all of the weights and biases for every layer in a neural network &quot;slice&quot;.</p>
<p><strong>Here, the values for $W$ will be the same for every single slice</strong>. However, when we compute the gradients $\nabla W$, notice that all of the samples are different across slices, and so each &quot;slice&quot; will have a different gradient for their layers of weights and biases. So across all of the batches, <strong>we average $\nabla W$</strong>, so that we only have one set of weights for our batched neural network. We use this as our new gradient to update the weights with respect to the first layer. Then, these new weights for the first layer are all used to &quot;update&quot; the weights for all the other layers, so that the weights and biases for every neural network slice match again. Look what happens when we do this:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="123">
          <script>
            voila_process(123, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="124">
          <script>
            voila_process(124, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We still have a little noise, but the convergence is much better, and we can process our dataset much quicker. In practice, these &quot;tensors&quot; are how computers actually operate neural networks. Whenever you talk about specifying a batch size, this is generally what you are doing. Please note that each dataset might need different batch sizes based on their size. For example a dataset of 100 samples may not need batches, while a dataset of 1 million samples definitely will. Play around with it and see what gives you the best convergence for your dataset.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="125">
          <script>
            voila_process(125, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Introduction-to-Different-Optimizers-(hyperparameter-4)">Introduction to Different Optimizers (hyperparameter 4)<a class="anchor-link" href="#Introduction-to-Different-Optimizers-(hyperparameter-4)">&#182;</a></h2><hr />
<p>You have seen how we can use gradient descent in order to take steps into the local minima. Gradient Descent is a type of <strong>optimizer</strong>.</p>
<p>However, it turns out that neural networks spaces, with their multiple layers and their non-linear activation functions end up perverting the objective loss function such that it is no longer just a nice bowl, like we have seen in the previous examples. It ends up looking more like a ruggedy terrain, with lots of little dips and mountains. As a result what ends up happening is that our ball will end up getting &quot;stuck&quot; in a local minima, and never achieve better generalization. Let's look what happens in a scenario like this:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="126">
          <script>
            voila_process(126, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="127">
          <script>
            voila_process(127, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>As you can see, our ball travels down into one of the two dips, but you can see that the other one is a lot deeper. Having our ball go down there, we would get a model that can effectively generalize better. So naively we can try turning up the learning rate and see if we can &quot;jump&quot; into the better minima:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="128">
          <script>
            voila_process(128, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="129">
          <script>
            voila_process(129, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>We can see that we do jump over the first minima, but the sides of the objective function are much steeper. As a result the gradient tells us to &quot;throw the ball really hard,&quot; pushing us back out of the local minima dip. We then just jump around the minimas and never converge. So how do we combat this?</p>
<h3 id="Different-optimizers-to-combat-this-issue">Different optimizers to combat this issue<a class="anchor-link" href="#Different-optimizers-to-combat-this-issue">&#182;</a></h3><h4 id="An-important-Caveat">An important Caveat<a class="anchor-link" href="#An-important-Caveat">&#182;</a></h4><p>Before going on to cover these algorithms, all of these gradient and learning rate updates are with respect to the iterations of each individual layer's gradients. This means that each new hyperparameter we add that is not $\eta$ or the gradient $\nabla W$ will also have the superscript $(\ell)$, denoting it's respect to its gradient. This is because all layers' gradients are seperate with respect to each other (note this is different from being independent). All of these optimizers are recursive in nature, and work on there previous individual iterations. This means we will update for as many batches that we have to divide out datset. The optimizer methods we are about to cover update with respect to these batches. The gradients are averaged per batch, and we use this averaged gradient as our update rule. <strong>This is called layer-wise dependency</strong></p>
<h4 id="AdaGrad">AdaGrad<a class="anchor-link" href="#AdaGrad">&#182;</a></h4><p>There have been many algorithms to combat this issue, and help converge at a better solution. The first of these is AdaGrad. Do you remember how we had to tune the learning rate ourselves? Well what if we made an algorithm that also helped tune the learning rate by accumulating these gradients? This is what AdaGrad proposes, and it's following algorithm can be modeled as such:</p>
<p>$$W_{new} = W - \frac{\eta}{\sqrt{G_t}+\epsilon}g_t$$</p>
<p>where</p>
<ul>
<li>$g_t$ is the current gradient at the current iteration $t$, equivalent to $\nabla W$, representing the gradient for every individual layer. Note that this is not additive; it is more like a set.</li>
<li>$G_t$ is the sum of the sqare of all previous gradients, or $\large \sum_{i=1}^t g_i^2$.  For a loop, you can think of this as $G = G + (g_t)^2$</li>
<li>t is batch t out of how ever many batches it takes to subdivide the whole dataset</li>
<li>$\epsilon$ is usually some super small constant (usually $1*10^{-8}$) used to ensure no divide by 0 error</li>
<li>$\eta$ is the learning rate</li>
<li>$W_{new}$ is the new weights for all the layers of the matrix</li>
<li>$W$ is the weights for all the layers of the matrix</li>
</ul>
<p><strong>Note:</strong> This means that for our update rule, for every iteration, you &quot;save&quot; $g_t^{(\ell)}$'s weights and accumulate the square of them for $G_t^{(\ell)}$. You will therefore update $G_t^{(\ell)}$ recursively, usually via a loop.</p>
<p>What this essentially does is that for gradients (layers in our case) with high gradients and frequent big updates, we slow down in those directions, while maintaining speed for our sparser gradient updates, in the directions where the objective function isn't as steep. With this we solve the problem of the averages of the gradients not pushing us enough in some directions while pushin us too much in others, leading to poorer convergence in normal or stochastic gradient descent.
By keeping track of the square of the gradients for each layer, we can kind of &quot;adapt&quot; the learning rate depending on the slope. As an analogy, imagine you're in a car, driving on a city road with a lot of bumps and potholes. With AdaGrad, your car adjusts its speed by taking into account every pothole you’ve encountered from the start of your trip.</p>
<p>This way, we slowly start to &quot;slow down&quot; as we encounter minima. We start with a fast initial learning rate, but then it slowly diminishes, coming to zero. Going back to our analogy, over time, because you remember all the rough spots, your speed gets dialed down too much; even in areas where the road is relatively smooth—making your journey unnecessarily slow. This means you may just end up coming to a stop completely before reaching a good minimum.</p>
<h4 id="RMSProp">RMSProp<a class="anchor-link" href="#RMSProp">&#182;</a></h4><p>RMSProp aims to solve this issue where the gradients diminish too quickly by taking the moving average of the adaptive learning rates. This means that our older gradients decay, and our newer gradients will contribute more to the moving average, ensuring that the update reflects more recent information. The following algorithm looks as such:</p>
<p>$$v_t = \beta v_{t-1} + (1-\beta)g_t^2$$
$$W_{new} = W - \frac{\eta}{\sqrt{v_t}+\epsilon}g_t$$
$$v_0 = 0$$</p>
<p>where</p>
<ul>
<li>$\beta$ is the decay rate. It is usually set at .9</li>
<li>$v_t$ is the moving average of squared gradients at batch t.</li>
<li>$g_t$ is the current gradient at the current iteration $t$, equivalent to $\nabla W$, representing the gradient for every individual layer. Note that this is not additive; it is more like a set, so you would have seperate $v_t$'s for each layer, or a $v_t^{(\ell)}$</li>
<li>t is batch t out of how ever many batches it takes to subdivide the whole dataset</li>
<li>$\epsilon$ is usually some super small constant (usually $1*10^{-8}$) used to ensure no divide by 0 error</li>
<li>$\eta$ is the learning rate</li>
<li>$W_{new}$ is the new weights for all the layers of the matrix</li>
<li>$W$ is the weights for all the layers of the matrix</li>
</ul>
<p><strong>Note:</strong> This means that for our update rule, for every iteration, you &quot;save&quot; $v_t^{(\ell)}$ as it's own seperate entity per layer of weights. $v_t{(\ell)}$ will have the same dimensions as your weight matrix. You use the update rule above to get $v_t{(\ell)}$ for the next batch.</p>
<p>This solves the problem where the accumulated gradients were slowing down the optimization well before it reached it local minima. As for the continuation of our analogy:</p>
<p>With RMSProp, your car only pays attention to the potholes you’ve seen in the recent part of your drive. This means your speed is adjusted based on the current road conditions rather than being weighed down by every past bump. You’re still cautious, but not overly so, and you can keep a steadier pace overall.</p>
<p>While RMSProp is extremely powerful, it still has the potential to get stuck in local minima, because the squared gradients peter out when we reach some local minima. This means we will converge in the local minima, while there may be a better minima over the horizon. ie. We can still get stuck in potholes before we get to our destination! Also notice that for $v_0$, we essentially get that our adaptive learning rate needs to &quot;adjust&quot; from 0 in order to get the best moving average for the scenario. This is equivalent to the need to start up your car. Time is precious, and you don't want to waste it!</p>
<h4 id="Adam">Adam<a class="anchor-link" href="#Adam">&#182;</a></h4><p>Adam aims to solve RMSProp's convergence in smaller local minimums by adding first order momentum to the algorithm, and also a bias correction, so we dont need to &quot;start up our car,&quot; as I had put it previously. Adam updates can be defined with the following formulas:</p>
<p>$$m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$$
$$\text{Bias Correction: } \hat{m_t} = \frac{m_t}{(1-\beta_1^t)} \text {. The superscript t represents an exponent per iteration}$$
$$\text{Bias Correction: } \hat{v_t} = \frac{v_t}{(1-\beta_2^t)} \text {. The superscript t represents an exponent per iteration}$$
$$W_{new} = W - \eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}g_t$$
$$m_0 = 0$$
$$v_0 = 0$$</p>
<p><strong>Note:</strong> This means that for our update rule, for every iteration, you &quot;save&quot; $v_t^{(\ell)}$ and $m_t^{(\ell)}$ as it's own seperate entity per layer of weights. $v_t{(\ell)}$ and $m_t^{(\ell)}$ will have the same dimensions as your weight matrix. You use the update rule above to get $v_t{(\ell)}$ and $m_t^{(\ell)}$, apply the bias correction seperately, then use the uncorrected $v_t{(\ell)}$ and $m_t^{(\ell)}$ for the next batch. <strong>This was a common source of confusion for me when coding adam myself. When you are updating the first and second moments, you should only bias correct the value right before performing a step of gradient descent. The m and v should stay seperate from this bias correction when you recursively update them.</strong></p>
<p>where</p>
<ul>
<li>$\beta_1$ is the decay rate for momentum. It is usually set at .9</li>
<li>$\beta_2$ is the decay rate for squared gradient moving average (2nd moment). It is usually set at .999</li>
<li>$v_t$ is the moving average of squared gradients at batch t.</li>
<li>$m_t$ is the momentum of our function.</li>
<li>$g_t$ is the current gradient at the current iteration $t$, equivalent to $\nabla W$, representing the gradient for every individual layer. Note that this is not additive; it is more like a set, so you would have seperate $v_t$'s and $m_t$'s for each layer, or a $v_t^{(\ell)}$ and $m_t^{(\ell)}$</li>
<li>t is batch t out of how ever many batches it takes to subdivide the whole dataset</li>
<li>$\epsilon$ is usually some super small constant (usually $1*10^{-8}$) used to ensure no divide by 0 error</li>
<li>$\eta$ is the learning rate</li>
<li>$W_{new}$ is the new weights for all the layers of the matrix</li>
<li>$W$ is the weights for all the layers of the matrix</li>
</ul>
<p>With this newly added &quot;momentum,&quot; we essentially press our accelerator in these &quot;potholes,&quot; and this gives us enough power to drive out of them, all the way up until we reach our desired destination! While thinking of these analagies, look at the simulation below:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="130">
          <script>
            voila_process(130, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="131">
          <script>
            voila_process(131, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>You can clearly see why I opted to use Adam in my neural network implementation. However, I do not want you to discourage you from using any of these, as all optimization problems will be different, and you should choose the best optimizer that suits your specific problem!</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="132">
          <script>
            voila_process(132, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="AdamW-(hyperparameter-5)">AdamW (hyperparameter 5)<a class="anchor-link" href="#AdamW-(hyperparameter-5)">&#182;</a></h4><p>If you have seen logistic regression, you are probably familiar with the term regularization. There are many different types of regularization, such as $\ell_1$ (LASSO) regularization, which uses the absolute value of the weights to create a sparser solution, or more commonly $\ell_2$ (Ridge) Regression, which is the square of the weights used for regularization. Notice there are many other types of regularization such as ElasticNet, or even regularizations with powers $p&lt;1$. The issue with these regularization techniques, however, is that the gradients are updated adaptively, meaning the steps we take will change over time, and will not directly correlate to the size of the weights in any given layer. This means that some layers can have growing gradients, some can have shrinking gradients, and some may have very sparse gradients (matrices with a lot of 0's), and as a result, we get inconsistent regularization. This is where AdamW comes in. AdamW can be defined as
$$W_{new} = W - \eta(\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}g_t + \lambda W)$$</p>
<p>where are new term</p>
<ul>
<li>$\lambda$ is the hyperparameter specified by the client for how strong they want their decay</li>
</ul>
<p>As opposed to $\ell_2$ regularization:
$$W_{new} = W - \eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}g_t$$
$$g_t = \nabla W + \lambda W$$
You can see that $\ell_2$ regularization is encapsulated inside the gradient, which can pose the issue of this inconistent scaling, since the adaptive learning will distort the $\ell_2$ example. What adamW (the first example) does is it &quot;decouples&quot; the regularization, so that way the regularization scales independently of the adaptive learning rates and momentum calculations, and the regularization becomes consistent. <strong>This is why if you are performing any sort of norm regularization, you should use AdamW to ensure consistent regularization.</strong></p>

</div>
</div>
</div>
</div>
        </div><div cell-index="133">
          <script>
            voila_process(133, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Other-Regularization-Techniques">Other Regularization Techniques<a class="anchor-link" href="#Other-Regularization-Techniques">&#182;</a></h2><hr />
<p>We have covered 1 regularization technique. However, it is important to have multiple different types of regularization techniques, because each technique may help with certain aspects of your model overfitting. For example, we don't want any couple of nodes predicting using only one feature for its predictions, so we apply weight regularization, so that each feature and weight has more opportunity to learn, and therefore increase generalizeability. Our second types of regularization comes in the form of dropout:</p>
<h3 id="Dropout-(hyperparameter-6):">Dropout (hyperparameter 6):<a class="anchor-link" href="#Dropout-(hyperparameter-6):">&#182;</a></h3><p>The idea behind this is that we randomly drop some nodes in a layer per batch so that our neural network can not rely on any one given node, which should increase generalizability. While we are still not sure how it works, it still proves to be an extremely effective regularization technique. Here is an example picture:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="134">
          <script>
            voila_process(134, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="135">
          <script>
            voila_process(135, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h4 id="Applying-dropout">Applying dropout<a class="anchor-link" href="#Applying-dropout">&#182;</a></h4><p>Our dropouts can be defined by
$$X \sim Binomial(n,(1-p))$$</p>
<p>where</p>
<ul>
<li>p is the probability specified by the client</li>
<li>n is the number of nodes in the layer</li>
</ul>
<p>What we do is we essentially create a mask, or some random probability distribution of 0's depending on the specified probability. Let's revisit an old example, where the arrow is pointing to a layer we want to apply dropout to:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="136">
          <script>
            voila_process(136, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="137">
          <script>
            voila_process(137, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>Then n = 4, and let's assume we have a dropout of .2. We generate a list of size n with 80$ of generating a 1 and 20% chance of generating a zero. We could hypothetically generate something like this:
$$
\left( \begin{array}{ccc}
1\\[0.5em]
0 \\[0.5em]
1\\[0.5em]
1
\end{array}\right)
$$</p>
<p>Note that this could be different every single time.</p>
<p>Then, in order to balance out our lost weights, we multiply our &quot;mask&quot; by $\frac{1}{(1-p)}$:
$$
\frac{1}{(1-.2)}
\left( \begin{array}{ccc}
1\\[0.5em]
0 \\[0.5em]
1\\[0.5em]
1
\end{array}\right) = 
\left( \begin{array}{ccc}
1.25\\[0.5em]
0 \\[0.5em]
1.25\\[0.5em]
1.25
\end{array}\right)
$$
Then our layer with dropout would be:
$$\left( \begin{array}{ccc}
1.25*w_{1,1}^{(3)} &amp; 1.25*w_{2,1}^{(3)} &amp; 1.25*w_{3,1}^{(3)} \\[0.5em]
0 &amp; 0&amp; 0\\[0.5em]
1.25*w_{1,3}^{(3)} &amp; 1.25*w_{2,3}^{(3)} &amp; 1.25*w_{3,3}^{(3)} \\[0.5em]
1.25*w_{1,4}^{(3)} &amp; 1.25*w_{2,4}^{(3)} &amp;1.25* w_{3,4}^{(3)}
\end{array}\right)$$</p>
<p>This would last up until we compute everything in the batch and backpropagate correctly. Then we would do the same thing for the next batch, but notice the placement of 0's and 1's might be different, because they are picked randomly from a binomial distribution.</p>
<h4 id="Caveat:-What-if-the-dropout-drops-all-nodes?">Caveat: What if the dropout drops all nodes?<a class="anchor-link" href="#Caveat:-What-if-the-dropout-drops-all-nodes?">&#182;</a></h4><p>While you may be thinking of this, it can be a good edge case to handle. However, this is an extremely rare case, because you would eseentially have to multiply (1-dropout rate) * number of nodes, which for large layers would give next to 0 probability. However, in the rare event you get a mask with all 0's you can just flip the top one to 1, since this is so statistically improbabable it will most likely not hurt your generalization. You can pick a random 1 to flip, but note that any computational expense added in backpropagation, training, or anything with loops will blow up time complexity.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="138">
          <script>
            voila_process(138, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Input-Dropout-(Hyperparameter-7)">Input Dropout (Hyperparameter 7)<a class="anchor-link" href="#Input-Dropout-(Hyperparameter-7)">&#182;</a></h3><p>Likewise, we can apply the same thing to the input layer, that way our neural network can't ever rely on any one such feature.</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="139">
          <script>
            voila_process(139, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h3 id="Closing-Notes:-Weight-Initializations">Closing Notes: Weight Initializations<a class="anchor-link" href="#Closing-Notes:-Weight-Initializations">&#182;</a></h3><p>If you are building a neural network yourself, I will save you a HUGE headache. This took me 9 hours to figure out when coding my weights and trying to figure out why my models were never converging.</p>
<p>In theory, weights in all of the weight matrices can be initialized randomly, but in practice, this is actually horrible. If we do not standardize our weights, then our model might contain some sort of symmetry, or have some sort of pattern in certain parts of the weight matrices that can make gradients explode or vanish. For example, if we have a weight and it's the first layer, that means we chain together the derivatives of all the other layers, all of these chained derivatives can make our value explode, or vanish, if our initial values isn't set up yet.</p>
<h4 id="Common-Initialization-tecnhiques">Common Initialization tecnhiques<a class="anchor-link" href="#Common-Initialization-tecnhiques">&#182;</a></h4><p>The method I used is called He initialization which utilizes a normal distribution to initialize the weights. It's most popular for ReLU, but works fairly well for sigmoid and tanh as well. Here is the following formula:</p>
<p>$$N \sim(0, \frac{2}{\text{nodes of the previous layer}} )$$</p>
<p>The formula for for Xavier/Gloat Initialization is very similar. This works well for tanh and sigmoid:
$$N \sim(0, \frac{1}{\text{nodes of the previous layer}} )$$</p>
<p>For biases, it is common to just set these to 0. This is done because the model should &quot;adjust the height&quot; with every backpropagation step.
Given the picture below:</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="140">
          <script>
            voila_process(140, 142)
          </script>
          <div  class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs jp-mod-noInput ">

</div>
        </div><div cell-index="141">
          <script>
            voila_process(141, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<p>our nodes of the previous layer for every single node will be 2!
That means we would pick a value from that distribution that we described, and make that the weight of our node.</p>
<p><strong>WARNING!!!!!!!!!!!!!!!!! DO NOT FORGET THIS. I ALMOST WENT INSANE TRYING TO FIX THIS.</strong></p>
<p>Unless you want to spend 9 hours debugging :))))))))))))))))))))))))))))))))))))))))))</p>

</div>
</div>
</div>
</div>
        </div><div cell-index="142">
          <script>
            voila_process(142, 142)
          </script>
          
<div  class="jp-Cell jp-MarkdownCell jp-Notebook-cell">
<div class="jp-Cell-inputWrapper">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput " data-mime-type="text/markdown">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion">&#182;</a></h2><hr />
<p>Ths is the end of my document. We went over the history and reasons behind using neural networks, the forward and backwards pass, optimizers, hyperparameters, gradient descent, batches, and regularization techniques. You should be ready with all of this information to build your own neural network from scratch! Feel free to use this document as a reference for building your own neural network! I will have the listed derivaations from earlier in the document below this section for anyone who is interested in the math and derivations behind the loss, and step-by-step breakdowns on the derivatives of the activation and loss functions, as well as any other fun facts. Thank you so much for reading!</p>

</div>
</div>
</div>
</div>
        </div></div>
</div>
<script type="text/javascript">
  window.voila_finish();
</script>
<script src="../build/voici.js"></script>

</main>
</body>



</html>